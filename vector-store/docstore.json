[["0",{"pageContent":"Eduardo Casanova\nMexico City, Mexico\ned.wacc1995@gmail.com\nlinkedin.com/in/edwardcasanova\nSummary\nExperienced Full Stack Developer specializing in building elegant and scalable systems with a focus on\nmaintainability.\n \nCore values:\n- Humans over tools\n- Ideas over dogma\n- Critical thinking and general solutions\n- Balance in the pillars of life\n- Learning for life\n- Mentorship\n- Leadership comes from caring, supporting and then leading\n \nBy prioritizing humans over tools, I strive to create user-centric solutions that enhance the overall experience. My\npassion for critical thinking and general solutions allows me to tackle complex problems creatively.\n \nQuirks:\n- I am a bookworm and a fan of the O'Reilly Editorial books. Here's my read list: (https://www.litsy.com/web/stack/\nedca3899/read)\n- I am a Polyglot (Node.js, TS, Go, Rust, Solidity, Bash, SQL, Ruby)\n \nMy motto:\n \n\"I believe true engineering encompasses design, architecture, and unwavering dedication. Through meticulous\nattention to detail and continuous growth, I strive to exceed expectations and deliver exceptional results.\"\n \nAWS:\n- I've worked with tools ranging from simple EC2 deployments via Terraform, Pulumi and Crossplane to complex\nones using EKS.\n- EC2\n- Lambdas\n- K8s\n- Cognito\n- DynamoDB\n- MemCache\n- VPC\n- Amplify\n- Route53\n- IAM\n- S3\n \nOpen source projects:\nEduardo Casanova - page \n1","metadata":{"source":"./test-pdf.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Apache FOP Version 2.2","Producer":"Apache FOP Version 2.2","CreationDate":"D:20230908190906Z"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:language":"x-unknown","dc:date":"2023-09-08T19:09:06Z","pdf:producer":"Apache FOP Version 2.2","pdf:pdfversion":"1.4","xmp:creatortool":"Apache FOP Version 2.2","xmp:metadatadate":"2023-09-08T19:09:06Z","xmp:createdate":"2023-09-08T19:09:06Z"}},"totalPages":9},"loc":{"pageNumber":1}}}],["1",{"pageContent":"- Kumo: # Your quick and easy cloud development environment (For now, only AWS compatible) -> https://\ngithub.com/ed3899/kumo\n- Gimwork: Technical task for a startup I applied. Basically backend in node using aws cognito for authentication\nand authorization. MongoDB for the database and ReactNative for the frontend. Can't share because of NDA\nExperience\nCareer Break\nNone\nNov 2022 - Jun 2023 (8 months)\nI decided I needed to take my career to the next level. So I focused on developing skills mostly in the\nSRE and DevOps area.\n \nI deepened my knowledge of:\n- Ansible\n- Python\n- Go\n- Packer\n- Linux\n- Docker\n- K8s\n- Terraform\n- Pulumi\n- Bash\n- Chef and Puppet\n- AWS\n \nIt all culminated in my first open source project. Kumo\nhttps://github.com/ed3899/kumo\nFull Stack Developer\nSterling Capital Brokers Ltd.\nMay 2022 - Nov 2022 (7 months)\n- Executing user stories to support Sterling’s core technical product\n- Bug fixes and performance improvements\n- Deploy AWS resources using cross plane. (K8s)\n- Monitoring our EKS cluster\n- Observability reporting with Prometheus and Grafana\n- Tests to ensure quality releases\n- Helping to refactor our existing platform to ensure reliability and scalability\n- Improving quality assurance practices within my team\n- Collaborating with senior team members to define, document and implement the software, software\ndevelopment best practices, infrastructure as code, release/test automation, observability and\nmaintenance of SCB's Platform.\n- Expected to have a broad understanding of SBC’s technical landscape and can contribute across the\nvarious domains that comprise SCB’s technical landscape.\n- Assist with system design and implementation.\n- Make suggestions and participate in decision making regarding the team’s agile processes/practices.\nEduardo Casanova - page \n2","metadata":{"source":"./test-pdf.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Apache FOP Version 2.2","Producer":"Apache FOP Version 2.2","CreationDate":"D:20230908190906Z"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:language":"x-unknown","dc:date":"2023-09-08T19:09:06Z","pdf:producer":"Apache FOP Version 2.2","pdf:pdfversion":"1.4","xmp:creatortool":"Apache FOP Version 2.2","xmp:metadatadate":"2023-09-08T19:09:06Z","xmp:createdate":"2023-09-08T19:09:06Z"}},"totalPages":9},"loc":{"pageNumber":2}}}],["2",{"pageContent":"Junior Information Technology Consultant\nTHinK Best Practice\nNov 2020 - Mar 2022 (1 year 5 months)\nHelp businesses:\n- Analyze their current tech stack and identify gaps in their IT systems\n- Evaluate migration to clouds such as AWS or Oracle.\n- Worked with multiple AWS resources such as (EC2, VPC, Lambdas, Elastic Beanstalk, CloudWatch,\nDynamoDB)\n- Design deployment diagrams(Terraform)\n- Improve their IT security policies and controls\n- Integrate with 3rd party SASS applications\n- Data analysis, conversion and migration\nEducation\nZero To Mastery Academy\nSoftware Engineer, Computer Programming\nJun 2019 - Jun 2033\nThis academy took me from nothing to being a lifelong computer science learner. I will always be\nthankful to Andrei Neagoie for being my first sensei.\n \nIts courses may appear basic but they are the entry point for a lot of beginners who initially appear\nintimidated by the field of computer science.\nUdemy Alumni\nSoftware Engineer, Computer Science\nJun 2019 - Jan 2033\nThis site well packed with courses regarding almost everything in Computer Science.\nKode Kloud\nSoftware Engineer, Computer Science\nMar 2022 - Mar 2033\nKhan Academy\nKhan Academy Advanced Mathematics, Advanced Mathematics Course\nJun 2021 - Feb 2022\n-Algebra I,II\n-Pre-calculus\n-Trigonometry\n-Linear Algebra\n-Probability & Statistics\n-Calculus (Integral & Differential)\n-Multivariable Calculus\n-Differential equations\n \nHere's my profile\nEduardo Casanova - page \n3","metadata":{"source":"./test-pdf.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Apache FOP Version 2.2","Producer":"Apache FOP Version 2.2","CreationDate":"D:20230908190906Z"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:language":"x-unknown","dc:date":"2023-09-08T19:09:06Z","pdf:producer":"Apache FOP Version 2.2","pdf:pdfversion":"1.4","xmp:creatortool":"Apache FOP Version 2.2","xmp:metadatadate":"2023-09-08T19:09:06Z","xmp:createdate":"2023-09-08T19:09:06Z"}},"totalPages":9},"loc":{"pageNumber":3}}}],["3",{"pageContent":"http://www.khanacademy.org/profile/edca3899\nUniversidad Modelo\nGraduate, Physical Culture & Sports Training\n2015 - 2019\nThe graduate of the degree in Physical Culture and Sports Training must have the skills of leadership,\ncommunication, respect for human life, high sense of responsibility and ethics, both in their work and\ntheir community.\na) The ability to develop training cycles according to the physiological, biomechanical and specific\nneeds of an activity or sport must also be developed, respecting its development and competitive\nstages.\nb) Implement prevention and rehabilitation strategies in the treatment of injuries related to physical\nactivity and sports.\nc) Conduct research in the field of physical culture and sports training.\nIrlanda English Academy\nEnglish Language\n2007 - 2012\n- Learned English as a second language.\n- Took an exam approved by the Cambridge University called the FCE.\n- I also hold a TOEFL\nLicenses & Certifications\nAWS Certified Developer Associate 2023 NEW DVA-C02\n - Udemy\n(In Progress)\nComplete SQL and Databases Bootcamp\n - Zero To Mastery Academy\nUC-9a55fd7d-e016-4048-b525-a0f3715e473b\nMaster the Coding Interview: Data Structures and Algorithms\n - Zero To Mastery\nAcademy\nUC-2657502b-344f-43e1-821b-ce3aa6dfa610\nTerraform for absolute beginners with labs\n - KodeKloud\nUC-bad6e4dd-bdce-4fba-bd7a-dcbf2815efb8\nUnderstanding Typescript 2022 Edition\n - Academind\nUC-0508a78a-bd01-40c3-8e51-7e275305977b\nMaster the Coding Interview: Big Tech (FAANG) Interviews\n - Zero To Mastery\nAcademy\nUC-dd8eab3d-ca79-4f11-b4a2-7dabe33f0e69\nEduardo Casanova - page \n4","metadata":{"source":"./test-pdf.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Apache FOP Version 2.2","Producer":"Apache FOP Version 2.2","CreationDate":"D:20230908190906Z"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:language":"x-unknown","dc:date":"2023-09-08T19:09:06Z","pdf:producer":"Apache FOP Version 2.2","pdf:pdfversion":"1.4","xmp:creatortool":"Apache FOP Version 2.2","xmp:metadatadate":"2023-09-08T19:09:06Z","xmp:createdate":"2023-09-08T19:09:06Z"}},"totalPages":9},"loc":{"pageNumber":4}}}],["4",{"pageContent":"Complete Web & Mobile Designer in 2022: UI/UX, Figma, +more\n - Zero To Mastery\nAcademy\nUC-339c1a6b-33e5-418a-bc5e-f1abfe118006\nModern HTML & CSS From The Beginning (Including Sass)\n - Udemy\nUC-1c7b0fb4-3973-4b63-b105-be7ab51ddf91\nBootstrap 4 From Scratch With 5 Projects\n - Udemy\nUC-5f939817-8ccd-48a9-8f73-59f80b3c5f5c\nThe Complete Networking Fundamentals Course. Your CCNA start\n - Udemy\nUC-e7489c13-bd40-4ab2-9c47-fa46dce52ce0\nThe Complete Node.js Developer Course (3rd Edition)\n - Udemy\nUC-3d2a724e-8522-449f-97fa-69c12ab53093\nJust Express (with a bunch of node and http). In detail.\n - Udemy\nUC-fea38a05-0e60-4f15-8132-f52a567b9517\n20 Web Projects With Vanilla JavaScript\n - Udemy\nUC-35d12150-1ba1-419e-9e37-cfd18f16681f\nAdvanced CSS and Sass: Flexbox, Grid, Animations and More!\n - Udemy\nUC-9da1ca75-a958-4d92-8283-ede118cd8288\nBody Language for Entrepreneurs\n - Udemy\nUC-0WC9PACG\nSuccessful Negotiation: Master Your Negotiating Skills\n - Udemy\nUC-0VMF1TQ4\nSales Training: Practical Sales Techniques\n - Udemy\nUC-JCT57I00\nJavaScript: The Advanced Concepts \n - Udemy\nUC-43d7547d-608e-4300-be82-8f32bf0319e5\nMaster Ethereum & Solidity Programming From Scratch\n - Udemy\nUC-0faba1be-1f28-44c9-8d64-50ed24fd60fd\nLearning Functional Javascript with Ramda\n - Udemy\nEduardo Casanova - page \n5","metadata":{"source":"./test-pdf.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Apache FOP Version 2.2","Producer":"Apache FOP Version 2.2","CreationDate":"D:20230908190906Z"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:language":"x-unknown","dc:date":"2023-09-08T19:09:06Z","pdf:producer":"Apache FOP Version 2.2","pdf:pdfversion":"1.4","xmp:creatortool":"Apache FOP Version 2.2","xmp:metadatadate":"2023-09-08T19:09:06Z","xmp:createdate":"2023-09-08T19:09:06Z"}},"totalPages":9},"loc":{"pageNumber":5}}}],["5",{"pageContent":"UC-a8cfd27c-fc2a-4467-967f-882fcbbb4530\nThe Complete Junior to Senior Web Developer Roadmap \n - Zero To Mastery\nAcademy\nUC-b1bded46-4922-43fc-aa87-03cd1a1783c4\nLearn DevOps: Infrastructure Automation With Terraform\n - Udemy\nUC-23f52281-0521-4a9c-b5e3-ce1ed9bb4b60\nComplete React Developer (w/ Redux, Hooks, GraphQL)\n - Zero To Mastery\nAcademy\nUC-ec6832b5-0fe9-4734-9436-d01250b208d1\nComplete Next.js Developer\n - Zero To Mastery Academy\nUC-945f190e-036c-4a23-b6fb-1fed63d276bb\nComplete React Native in 2022: Zero to Mastery (with Hooks)\n - Zero To Mastery\nAcademy\nUC-986604bf-0e55-42b3-a918-86e5a5922808\nLearn to Code with Ruby\n - Udemy\nUC-642aa92d-91da-4846-8335-70ac706a9c11\nRuby on Rails 6 Complete Beginner's Course\n - Udemy\nUC-6a9a57aa-f208-459d-bc6d-f79892c898f5\nTesting Ruby with RSpec: The Complete Guide\n - Udemy\nUC-129367ff-7823-4269-be79-02f45421ce2d\nKubernetes for the Absolute Beginners - Hands-on\n - KodeKloud\nUC-f862b0f2-f47b-42e8-b661-420450d51af8\nUdemy Labs - Certified Kubernetes Application Developer\n - KodeKloud\n7C99DFF149-7C9FB775A2-7C93ACE0A1\nLabs - Kubernetes Lab for Beginners - Hands On\n - KodeKloud\n7C99DFF149-7C9FB7E9A6-7C93ACE0A1\nKubernetes Certified Application Developer (CKAD) with Tests\n - KodeKloud\nUC-e2cee94e-f53e-4c1a-80c7-9c77c47f884b\nEduardo Casanova - page \n6","metadata":{"source":"./test-pdf.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Apache FOP Version 2.2","Producer":"Apache FOP Version 2.2","CreationDate":"D:20230908190906Z"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:language":"x-unknown","dc:date":"2023-09-08T19:09:06Z","pdf:producer":"Apache FOP Version 2.2","pdf:pdfversion":"1.4","xmp:creatortool":"Apache FOP Version 2.2","xmp:metadatadate":"2023-09-08T19:09:06Z","xmp:createdate":"2023-09-08T19:09:06Z"}},"totalPages":9},"loc":{"pageNumber":6}}}],["6",{"pageContent":"Build A Weather App With Ruby On Rails\n - Udemy\nUC-6b12afa7-f508-41af-bfb3-0bf1f4a37e53\nWebpack 5 in 2022: The Complete Guide For Beginners\n - Udemy\nUC-2302f9ed-64dc-4e0b-886f-e056cf1ae300\nJSON Path Test - Free Course\n - KodeKloud\n7C99DFF149-7C93EA5E0B-7C93ACE0A1\nUdemy Labs - Certified Kubernetes Administrator with Practice Tests\n -\nKodeKloud\n7C99DFF149-7C9FB6B3E1-7C93ACE0A1\nCertified Kubernetes Administrator (CKA) with Practice Tests\n - Udemy\nUC-e1ffbca6-94ac-4b9c-b4f8-69e8a54ed54c\nF# From the Ground Up\n - Udemy\nUC-c3b412bc-e03e-442a-9765-efe8f19e19bf\nDocker Mastery: with Kubernetes +Swarm from a Docker Captain\n - Udemy\nUC-add1fa15-a28e-4923-b0b4-d8602b32d904\nGo Programming (Golang): The Complete Developer's Guide\n - Zero To Mastery\nAcademy\nUC-9d8b35b9-0082-4898-9e15-656c2bf5f016\nAWS Certified Cloud Practitioner - 2022\n - Udemy\nUC-601d7e88-258e-4805-8353-b116a0e8ba2b\nComplete Linux Training Course on CentOS 7 / RHEL 7\n - Udemy\nUC-ef81b26b-af5d-408c-8e50-e1e2170677f7\nChef for the Absolute Beginners - DevOps\n - KodeKloud\nUC-e74be459-465d-479e-87ad-11c6f7af725a\nPuppet for the Absolute Beginners - Hands-on - DevOps\n - KodeKloud\nUC-262e85ea-9399-43fd-b7f0-9434ba285403\nAnsible for the Absolute Beginner - Hands-On - DevOps\n - KodeKloud\nUC-fefed9f5-21e3-4854-8568-8df339af42f8\nEduardo Casanova - page \n7","metadata":{"source":"./test-pdf.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Apache FOP Version 2.2","Producer":"Apache FOP Version 2.2","CreationDate":"D:20230908190906Z"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:language":"x-unknown","dc:date":"2023-09-08T19:09:06Z","pdf:producer":"Apache FOP Version 2.2","pdf:pdfversion":"1.4","xmp:creatortool":"Apache FOP Version 2.2","xmp:metadatadate":"2023-09-08T19:09:06Z","xmp:createdate":"2023-09-08T19:09:06Z"}},"totalPages":9},"loc":{"pageNumber":7}}}],["7",{"pageContent":"Ansible Advanced - Hands-On - DevOps\n - KodeKloud\nUC-7d7c6992-1924-4a8f-9533-0da0112c6406\nComplete Python Developer in 2023: Zero to Mastery\n - Zero To Mastery Academy\nUC-cff54045-b1b2-4f24-a3c4-d07c1bcda732\nRocking System Design\n - Udemy\nUC-2a090618-4764-4802-910e-5db33af1266c\nDocker & Kubernetes: The Practical Guide\n - Academind\nUC-80b7388f-1eeb-4e6b-bf61-524856ef6d3a\nRust For Beginners\n - Udemy\nUC-0dfe6406-813f-4164-a8fd-201fe96236a4\nMastering the System Design Interview\n - Udemy\nUC-40a9472f-a531-479a-a754-eb431bec4ac9\nSkills\nDevOps\n   •   React Native\n   •   Full-Stack Development\n   •   SQL\n   •   Object-Oriented Programming (OOP)\n   •  \nREST APIs\n   •   Django\n   •   JSON\n   •   HTML\n   •   Amazon Web Services (AWS)\nHonors & Awards\nAqua Xtreme Challenge\n - Universidad Modelo\nMay 2016\n3rd Place!\nPeace Project Award 3\n - Escuela Preparatoria Juventus\nOct 2017\nOrganized and facilitated a \"positive conflict transformation seminar\" for high-school teenagers.\nPeace Project Award 2\n - Escuela Secundaria No. 86 \"Rita Cetina Gutierrez\"\nMay 2017\nOrganized and facilitated a \"non-violent conflict resolution seminar\" for children in low-income towns.\n1st Sport Science Congress\n - Universidad Modelo\nApr 2017\nLearned pretty interesting things.\nPeace Project Award 1\n - Universidad Modelo\nDec 2016\nEduardo Casanova - page \n8","metadata":{"source":"./test-pdf.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Apache FOP Version 2.2","Producer":"Apache FOP Version 2.2","CreationDate":"D:20230908190906Z"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:language":"x-unknown","dc:date":"2023-09-08T19:09:06Z","pdf:producer":"Apache FOP Version 2.2","pdf:pdfversion":"1.4","xmp:creatortool":"Apache FOP Version 2.2","xmp:metadatadate":"2023-09-08T19:09:06Z","xmp:createdate":"2023-09-08T19:09:06Z"}},"totalPages":9},"loc":{"pageNumber":8}}}],["8",{"pageContent":"Learned about the importance of a peace culture.\nEduardo Casanova - page \n9","metadata":{"source":"./test-pdf.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Apache FOP Version 2.2","Producer":"Apache FOP Version 2.2","CreationDate":"D:20230908190906Z"},"metadata":{"_metadata":{"dc:format":"application/pdf","dc:language":"x-unknown","dc:date":"2023-09-08T19:09:06Z","pdf:producer":"Apache FOP Version 2.2","pdf:pdfversion":"1.4","xmp:creatortool":"Apache FOP Version 2.2","xmp:metadatadate":"2023-09-08T19:09:06Z","xmp:createdate":"2023-09-08T19:09:06Z"}},"totalPages":9},"loc":{"pageNumber":9}}}],["9",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com//docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["10",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Get started [/docs/get_started] *\nIntroduction INTRODUCTION LangChain is a framework for developing applications\npowered by language models. It enables applications that: * Are context-aware:\nconnect a language model to other sources of context (prompt instructions, few\nshot examples, content to ground it's response in) * Reason: rely on a language\nmodel to reason (about how to answer based on provided","metadata":{"source":"https://js.langchain.com//docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":25,"to":52}}}}],["11",{"pageContent":"model to other sources of context (prompt instructions, few shot examples,\ncontent to ground it's response in) * Reason: rely on a language model to reason\n(about how to answer based on provided context, what actions to take, etc) The\nmain value props of LangChain are: 1. Components: abstractions for working with\nlanguage models, along with a collection of implementations for each\nabstraction. Components are modular and easy-to-use, whether you are using the\nrest of the LangChain framework or not 2. Off-the-shelf chains: a structured\nassembly of components for accomplishing specific higher-level tasks\nOff-the-shelf chains make it easy to get started. For more complex applications\nand nuanced use-cases, components make it easy to customize existing chains or\nbuild new ones. GET STARTED Here’s [/docs/get_started/installation] how to\ninstall LangChain, set up your environment, and start building. We recommend\nfollowing our Quickstart [/docs/get_started/quickstart] guide","metadata":{"source":"https://js.langchain.com//docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":52,"to":70}}}}],["12",{"pageContent":"STARTED Here’s [/docs/get_started/installation] how to install LangChain, set up\nyour environment, and start building. We recommend following our Quickstart\n[/docs/get_started/quickstart] guide to familiarize yourself with the framework\nby building your first LangChain application. Note: These docs are for the\nLangChain JS/TS package [https://github.com/hwchase17/langchainjs]. For\ndocumentation on the Python version [https://github.com/hwchase17/langchain],\nhead here [https://python.langchain.com/docs]. MODULES LangChain provides\nstandard, extendable interfaces and external integrations for the following\nmodules, listed from least to most complex: MODEL I/O [/docs/modules/model_io/]\nInterface with language models RETRIEVAL [/docs/modules/data_connection/]\nInterface with application-specific data CHAINS [/docs/modules/chains/]\nConstruct sequences of calls AGENTS [/docs/modules/agents/] Let chains choose\nwhich tools to use given high-level directives MEMORY","metadata":{"source":"https://js.langchain.com//docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":70,"to":102}}}}],["13",{"pageContent":"with application-specific data CHAINS [/docs/modules/chains/] Construct\nsequences of calls AGENTS [/docs/modules/agents/] Let chains choose which tools\nto use given high-level directives MEMORY [/docs/modules/memory/] Persist\napplication state between runs of a chain CALLBACKS [/docs/modules/callbacks/]\nLog and stream intermediate steps of any chain EXAMPLES, ECOSYSTEM, AND\nRESOURCES USE CASES [/docs/use_cases/] Walkthroughs and best-practices for\ncommon end-to-end use cases, like: * Chatbots [/docs/use_cases/chatbots/] *\nAnswering questions using sources [/docs/use_cases/question_answering/] *\nAnalyzing structured data [/docs/use_cases/tabular] * and much more... GUIDES\n[/docs/guides/] Learn best practices for developing with LangChain. ADDITIONAL\nRESOURCES [/docs/additional_resources/] Our community is full of prolific\ndevelopers, creative builders, and fantastic teachers. Check out Scrimba\n[/docs/additional_resources/scrimba] for a series of interactive","metadata":{"source":"https://js.langchain.com//docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":102,"to":142}}}}],["14",{"pageContent":"community is full of prolific developers, creative builders, and fantastic\nteachers. Check out Scrimba [/docs/additional_resources/scrimba] for a series of\ninteractive guides on how to get started with various concepts, and Gallery\n[https://github.com/kyrolabs/awesome-langchain] for a list of awesome LangChain\nprojects, compiled by the folks at KyroLabs [https://kyrolabs.com]. COMMUNITY\n[/docs/community] Head to the Community navigator [/docs/community] to find\nplaces to ask questions, share feedback, meet other developers, and dream about\nthe future of LLM’s. Previous Get started [/docs/get_started] Next Installation\n[/docs/get_started/installation] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com//docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":142,"to":171}}}}],["15",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["16",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Get started [/docs/get_started] *\nIntroduction INTRODUCTION LangChain is a framework for developing applications\npowered by language models. It enables applications that: * Are context-aware:\nconnect a language model to other sources of context (prompt instructions, few\nshot examples, content to ground it's response in) * Reason: rely on a language\nmodel to reason (about how to answer based on provided","metadata":{"source":"https://js.langchain.com/docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":25,"to":52}}}}],["17",{"pageContent":"model to other sources of context (prompt instructions, few shot examples,\ncontent to ground it's response in) * Reason: rely on a language model to reason\n(about how to answer based on provided context, what actions to take, etc) The\nmain value props of LangChain are: 1. Components: abstractions for working with\nlanguage models, along with a collection of implementations for each\nabstraction. Components are modular and easy-to-use, whether you are using the\nrest of the LangChain framework or not 2. Off-the-shelf chains: a structured\nassembly of components for accomplishing specific higher-level tasks\nOff-the-shelf chains make it easy to get started. For more complex applications\nand nuanced use-cases, components make it easy to customize existing chains or\nbuild new ones. GET STARTED Here’s [/docs/get_started/installation] how to\ninstall LangChain, set up your environment, and start building. We recommend\nfollowing our Quickstart [/docs/get_started/quickstart] guide","metadata":{"source":"https://js.langchain.com/docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":52,"to":70}}}}],["18",{"pageContent":"STARTED Here’s [/docs/get_started/installation] how to install LangChain, set up\nyour environment, and start building. We recommend following our Quickstart\n[/docs/get_started/quickstart] guide to familiarize yourself with the framework\nby building your first LangChain application. Note: These docs are for the\nLangChain JS/TS package [https://github.com/hwchase17/langchainjs]. For\ndocumentation on the Python version [https://github.com/hwchase17/langchain],\nhead here [https://python.langchain.com/docs]. MODULES LangChain provides\nstandard, extendable interfaces and external integrations for the following\nmodules, listed from least to most complex: MODEL I/O [/docs/modules/model_io/]\nInterface with language models RETRIEVAL [/docs/modules/data_connection/]\nInterface with application-specific data CHAINS [/docs/modules/chains/]\nConstruct sequences of calls AGENTS [/docs/modules/agents/] Let chains choose\nwhich tools to use given high-level directives MEMORY","metadata":{"source":"https://js.langchain.com/docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":70,"to":102}}}}],["19",{"pageContent":"with application-specific data CHAINS [/docs/modules/chains/] Construct\nsequences of calls AGENTS [/docs/modules/agents/] Let chains choose which tools\nto use given high-level directives MEMORY [/docs/modules/memory/] Persist\napplication state between runs of a chain CALLBACKS [/docs/modules/callbacks/]\nLog and stream intermediate steps of any chain EXAMPLES, ECOSYSTEM, AND\nRESOURCES USE CASES [/docs/use_cases/] Walkthroughs and best-practices for\ncommon end-to-end use cases, like: * Chatbots [/docs/use_cases/chatbots/] *\nAnswering questions using sources [/docs/use_cases/question_answering/] *\nAnalyzing structured data [/docs/use_cases/tabular] * and much more... GUIDES\n[/docs/guides/] Learn best practices for developing with LangChain. ADDITIONAL\nRESOURCES [/docs/additional_resources/] Our community is full of prolific\ndevelopers, creative builders, and fantastic teachers. Check out Scrimba\n[/docs/additional_resources/scrimba] for a series of interactive","metadata":{"source":"https://js.langchain.com/docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":102,"to":142}}}}],["20",{"pageContent":"community is full of prolific developers, creative builders, and fantastic\nteachers. Check out Scrimba [/docs/additional_resources/scrimba] for a series of\ninteractive guides on how to get started with various concepts, and Gallery\n[https://github.com/kyrolabs/awesome-langchain] for a list of awesome LangChain\nprojects, compiled by the folks at KyroLabs [https://kyrolabs.com]. COMMUNITY\n[/docs/community] Head to the Community navigator [/docs/community] to find\nplaces to ask questions, share feedback, meet other developers, and dream about\nthe future of LLM’s. Previous Get started [/docs/get_started] Next Installation\n[/docs/get_started/installation] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/get_started/introduction","title":"Introduction | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":142,"to":171}}}}],["21",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases USE CASES Walkthroughs\nof common end-to-end use cases 🗃️ QA AND CHAT OVER DOCUMENTS 2 items\n[/docs/use_cases/question_answering/] 📄️ TABULAR QUESTION ANSWERING Lots of\ndata and information is stored in tabular data, whether it be","metadata":{"source":"https://js.langchain.com/docs/use_cases","title":"Use cases | 🦜️🔗 Langchain","description":"Walkthroughs of common end-to-end use cases","language":"en","loc":{"lines":{"from":1,"to":38}}}}],["22",{"pageContent":"use cases 🗃️ QA AND CHAT OVER DOCUMENTS 2 items\n[/docs/use_cases/question_answering/] 📄️ TABULAR QUESTION ANSWERING Lots of\ndata and information is stored in tabular data, whether it be csvs, excel\nsheets, or SQL tables. [/docs/use_cases/tabular] 📄️ INTERACTING WITH APIS Lots\nof data and information is stored behind APIs. [/docs/use_cases/api] 📄️\nSUMMARIZATION A common use case is wanting to summarize long documents.\n[/docs/use_cases/summarization] 🗃️ AGENT SIMULATIONS 1 items\n[/docs/use_cases/agent_simulations/] 🗃️ AUTONOMOUS AGENTS 3 items\n[/docs/use_cases/autonomous_agents/] 📄️ CHATBOTS Language models are good at\nproducing text, which makes them ideal for creating chatbots.\n[/docs/use_cases/chatbots] 📄️ EXTRACTION Most APIs and databases still deal\nwith structured information. Therefore, in order to better work with those, it\ncan be useful to extract structured information from text. Examples of this","metadata":{"source":"https://js.langchain.com/docs/use_cases","title":"Use cases | 🦜️🔗 Langchain","description":"Walkthroughs of common end-to-end use cases","language":"en","loc":{"lines":{"from":38,"to":93}}}}],["23",{"pageContent":"APIs and databases still deal with structured information. Therefore, in order\nto better work with those, it can be useful to extract structured information\nfrom text. Examples of this include: [/docs/use_cases/extraction] Next QA and\nChat over Documents [/docs/use_cases/question_answering/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases","title":"Use cases | 🦜️🔗 Langchain","description":"Walkthroughs of common end-to-end use cases","language":"en","loc":{"lines":{"from":93,"to":112}}}}],["24",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/get_started","title":"Get started | 🦜️🔗 Langchain","description":"Get started with LangChain","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["25",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Get started GET STARTED Get started with\nLangChain 📄️ INTRODUCTION [/docs/get_started/introduction] 📄️ INSTALLATION\n[/docs/get_started/installation] 📄️ QUICKSTART Installation\n[/docs/get_started/quickstart] Next Introduction\n[/docs/get_started/introduction] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python","metadata":{"source":"https://js.langchain.com/docs/get_started","title":"Get started | 🦜️🔗 Langchain","description":"Get started with LangChain","language":"en","loc":{"lines":{"from":25,"to":73}}}}],["26",{"pageContent":"* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/get_started","title":"Get started | 🦜️🔗 Langchain","description":"Get started with LangChain","language":"en","loc":{"lines":{"from":73,"to":84}}}}],["27",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/get_started/installation","title":"Installation | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["28",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Get started [/docs/get_started] *\nInstallation INSTALLATION info Updating from <0.0.52? See this section for\ninstructions. SUPPORTED ENVIRONMENTS LangChain is written in TypeScript and can\nbe used in: * Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x * Cloudflare Workers\n* Vercel / Next.js (Browser, Serverless and Edge functions) * Supabase Edge\nFunctions * Browser * Deno * Bun INSTALLATION To get","metadata":{"source":"https://js.langchain.com/docs/get_started/installation","title":"Installation | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":25,"to":68}}}}],["29",{"pageContent":"(ESM and CommonJS) - 18.x, 19.x, 20.x * Cloudflare Workers * Vercel / Next.js\n(Browser, Serverless and Edge functions) * Supabase Edge Functions * Browser *\nDeno * Bun INSTALLATION To get started, install LangChain with the following\ncommand: * npm * Yarn * pnpm npm install -S langchain yarn add langchain pnpm\nadd langchain TYPESCRIPT LangChain is written in TypeScript and provides type\ndefinitions for all of its public APIs. LOADING THE LIBRARY ESM LangChain\nprovides an ESM build targeting Node.js environments. You can import it using\nthe following syntax: import { OpenAI } from \"langchain/llms/openai\"; If you are\nusing TypeScript in an ESM project we suggest updating your tsconfig.json to\ninclude the following: tsconfig.json { \"compilerOptions\": { ... \"target\":\n\"ES2020\", // or higher \"module\": \"nodenext\", } } COMMONJS LangChain provides a\nCommonJS build targeting Node.js environments. You can import it using the\nfollowing","metadata":{"source":"https://js.langchain.com/docs/get_started/installation","title":"Installation | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":68,"to":136}}}}],["30",{"pageContent":"{ ... \"target\": \"ES2020\", // or higher \"module\": \"nodenext\", } } COMMONJS\nLangChain provides a CommonJS build targeting Node.js environments. You can\nimport it using the following syntax: const { OpenAI } =\nrequire(\"langchain/llms/openai\"); CLOUDFLARE WORKERS LangChain can be used in\nCloudflare Workers. You can import it using the following syntax: import {\nOpenAI } from \"langchain/llms/openai\"; VERCEL / NEXT.JS LangChain can be used in\nVercel / Next.js. We support using LangChain in frontend components, in\nServerless functions and in Edge functions. You can import it using the\nfollowing syntax: import { OpenAI } from \"langchain/llms/openai\"; DENO /\nSUPABASE EDGE FUNCTIONS LangChain can be used in Deno / Supabase Edge Functions.\nYou can import it using the following syntax: import { OpenAI } from\n\"https://esm.sh/langchain/llms/openai\"; We recommend looking at our Supabase\nTemplate","metadata":{"source":"https://js.langchain.com/docs/get_started/installation","title":"Installation | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":136,"to":187}}}}],["31",{"pageContent":"used in Deno / Supabase Edge Functions. You can import it using the following\nsyntax: import { OpenAI } from \"https://esm.sh/langchain/llms/openai\"; We\nrecommend looking at our Supabase Template\n[https://github.com/langchain-ai/langchain-template-supabase] for an example of\nhow to use LangChain in Supabase Edge Functions. BROWSER LangChain can be used\nin the browser. In our CI we test bundling LangChain with Webpack and Vite, but\nother bundlers should work too. You can import it using the following syntax:\nimport { OpenAI } from \"langchain/llms/openai\"; UPDATING FROM <0.0.52 If you are\nupdating from a version of LangChain prior to 0.0.52, you will need to update\nyour imports to use the new path structure. For example, if you were previously\ndoing import { OpenAI } from \"langchain/llms\"; you will now need to do import {\nOpenAI } from \"langchain/llms/openai\"; This applies to all imports from the\nfollowing 6 modules, which have been split into submodules for","metadata":{"source":"https://js.langchain.com/docs/get_started/installation","title":"Installation | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":187,"to":228}}}}],["32",{"pageContent":"\"langchain/llms\"; you will now need to do import { OpenAI } from\n\"langchain/llms/openai\"; This applies to all imports from the following 6\nmodules, which have been split into submodules for each integration. The\ncombined modules are deprecated, do not work outside of Node.js, and will be\nremoved in a future version. * If you were using langchain/llms, see LLMs\n[/docs/modules/model_io/models/llms] for updated import paths. * If you were\nusing langchain/chat_models, see Chat Models\n[/docs/modules/model_io/models/chat] for updated import paths. * If you were\nusing langchain/embeddings, see Embeddings\n[/docs/modules/data_connection/text_embedding] for updated import paths. * If\nyou were using langchain/vectorstores, see Vector Stores\n[/docs/modules/data_connection/vectorstores] for updated import paths. * If you\nwere using langchain/document_loaders, see Document Loaders\n[/docs/modules/data_connection/document_loaders] for updated import paths. * If\nyou were using","metadata":{"source":"https://js.langchain.com/docs/get_started/installation","title":"Installation | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":228,"to":250}}}}],["33",{"pageContent":"for updated import paths. * If you were using langchain/document_loaders, see\nDocument Loaders [/docs/modules/data_connection/document_loaders] for updated\nimport paths. * If you were using langchain/retrievers, see Retrievers\n[/docs/modules/data_connection/retrievers] for updated import paths. Other\nmodules are not affected by this change, and you can continue to import them\nfrom the same path. Additionally, there are some breaking changes that were\nneeded to support new environments: * import { Calculator } from\n\"langchain/tools\"; now moved to * import { Calculator } from\n\"langchain/tools/calculator\"; * import { loadLLM } from \"langchain/llms\"; now\nmoved to * import { loadLLM } from \"langchain/llms/load\"; * import { loadAgent }\nfrom \"langchain/agents\"; now moved to * import { loadAgent } from\n\"langchain/agents/load\"; * import { loadPrompt } from \"langchain/prompts\"; now\nmoved to * import { loadPrompt } from \"langchain/prompts/load\"; * import {\nloadChain }","metadata":{"source":"https://js.langchain.com/docs/get_started/installation","title":"Installation | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":250,"to":268}}}}],["34",{"pageContent":"* import { loadAgent } from \"langchain/agents/load\"; * import { loadPrompt }\nfrom \"langchain/prompts\"; now moved to * import { loadPrompt } from\n\"langchain/prompts/load\"; * import { loadChain } from \"langchain/chains\"; now\nmoved to * import { loadChain } from \"langchain/chains/load\"; UNSUPPORTED:\nNODE.JS 16 We do not support Node.js 16, but if you still want to run LangChain\non Node.js 16, you will need to follow the instructions in this section. We do\nnot guarantee that these instructions will continue to work in the future. You\nwill have to make fetch available globally, either: * run your application with\nNODE_OPTIONS='--experimental-fetch' node ..., or * install node-fetch and follow\nthe instructions here\n[https://github.com/node-fetch/node-fetch#providing-global-access] You'll also\nneed to polyfill ReadableStream\n[https://www.npmjs.com/package/web-streams-polyfill] by installing: npm i\nweb-streams-polyfill And then adding it to the global namespace in your","metadata":{"source":"https://js.langchain.com/docs/get_started/installation","title":"Installation | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":268,"to":292}}}}],["35",{"pageContent":"also need to polyfill ReadableStream\n[https://www.npmjs.com/package/web-streams-polyfill] by installing: npm i\nweb-streams-polyfill And then adding it to the global namespace in your main\nentrypoint: import \"web-streams-polyfill/es6\" Additionally you'll have to\npolyfill structuredClone, eg. by installing core-js and following the\ninstructions here [https://github.com/zloirock/core-js]. If you are running\nNode.js 18+, you do not need to do anything. Previous Introduction\n[/docs/get_started/introduction] Next Quickstart [/docs/get_started/quickstart]\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/get_started/installation","title":"Installation | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":292,"to":329}}}}],["36",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["37",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Get started [/docs/get_started] * Quickstart\nOn this page QUICKSTART INSTALLATION To install LangChain run: * npm * Yarn *\npnpm npm install -S langchain yarn add langchain pnpm add langchain For more\ndetails, see our Installation guide [/docs/get_started/installation].\nENVIRONMENT SETUP Using LangChain will usually require integrations with one or\nmore model providers, data stores, APIs,","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":25,"to":79}}}}],["38",{"pageContent":"more details, see our Installation guide [/docs/get_started/installation].\nENVIRONMENT SETUP Using LangChain will usually require integrations with one or\nmore model providers, data stores, APIs, etc. For this example, we'll use\nOpenAI's model APIs. Accessing their API requires an API key, which you can get\nby creating an account and heading here\n[https://platform.openai.com/account/api-keys]. Once we have a key we'll want to\nset it as an environment variable by running: export OPENAI_API_KEY=\"...\" If\nyou'd prefer not to set an environment variable you can pass the key in directly\nvia the openAIApiKey parameter when initializing the OpenAI LLM class: import {\nOpenAI } from \"langchain/llms/openai\"; const llm = new OpenAI({ openAIApiKey:\n\"YOUR_KEY_HERE\", }); BUILDING AN APPLICATION Now we can start building our\nlanguage model application. LangChain provides many modules that can be used to\nbuild language model applications. Modules can be used as stand-alones in","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":79,"to":111}}}}],["39",{"pageContent":"AN APPLICATION Now we can start building our language model application.\nLangChain provides many modules that can be used to build language model\napplications. Modules can be used as stand-alones in simple applications and\nthey can be combined for more complex use cases. The most common and most\nimportant chain that LangChain helps create contains three things: * LLM: The\nlanguage model is the core reasoning engine here. In order to work with\nLangChain, you need to understand the different types of language models and how\nto work with them. * Prompt Templates: This provides instructions to the\nlanguage model. This controls what the language model outputs, so understanding\nhow to construct prompts and different prompting strategies is crucial. * Output\nParsers: These translate the raw response from the LLM to a more workable\nformat, making it easy to use the output downstream. In this getting started\nguide we will cover those three components by themselves, and then go","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":111,"to":125}}}}],["40",{"pageContent":"the raw response from the LLM to a more workable format, making it easy to use\nthe output downstream. In this getting started guide we will cover those three\ncomponents by themselves, and then go over how to combine all of them.\nUnderstanding these concepts will set you up well for being able to use and\ncustomize LangChain applications. Most LangChain applications allow you to\nconfigure the LLM and/or the prompt used, so knowing how to take advantage of\nthis will be a big enabler. LLMS There are two types of language models, which\nin LangChain are called: * LLMs: this is a language model which takes a string\nas input and returns a string * ChatModels: this is a language model which takes\na list of messages as input and returns a message The input/output for LLMs is\nsimple and easy to understand - a string. But what about ChatModels? The input\nthere is a list of ChatMessages, and the output is a single ChatMessage. A\nChatMessage has two required components: * content: This","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":125,"to":144}}}}],["41",{"pageContent":"to understand - a string. But what about ChatModels? The input there is a list\nof ChatMessages, and the output is a single ChatMessage. A ChatMessage has two\nrequired components: * content: This is the content of the message. * role: This\nis the role of the entity from which the ChatMessage is coming from. LangChain\nprovides several objects to easily distinguish between different roles: *\nHumanMessage: A ChatMessage coming from a human/user. * AIMessage: A ChatMessage\ncoming from an AI/assistant. * SystemMessage: A ChatMessage coming from the\nsystem. * FunctionMessage: A ChatMessage coming from a function call. If none of\nthose roles sound right, there is also a ChatMessage class where you can specify\nthe role manually. For more information on how to use these different messages\nmost effectively, see our prompting guide. LangChain provides a standard\ninterface for both, but it's useful to understand this difference in order to\nconstruct prompts for a given language model.","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":144,"to":161}}}}],["42",{"pageContent":"most effectively, see our prompting guide. LangChain provides a standard\ninterface for both, but it's useful to understand this difference in order to\nconstruct prompts for a given language model. The standard interface that\nLangChain provides has two methods: * predict: Takes in a string, returns a\nstring * predictMessages: Takes in a list of messages, returns a message. Let's\nsee how to work with these different types of models and these different types\nof inputs. First, let's import an LLM and a ChatModel and call predict. import {\nOpenAI } from \"langchain/llms/openai\"; import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; const llm = new OpenAI({ temperature: 0.9, });\nconst chatModel = new ChatOpenAI(); const text = \"What would be a good company\nname for a company that makes colorful socks?\"; const llmResult = await\nllm.predict(text); /* \"Feetful of Fun\" */ const chatModelResult = await\nchatModel.predict(text); /* \"Socks O'Color\" */ The OpenAI and","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":161,"to":196}}}}],["43",{"pageContent":"that makes colorful socks?\"; const llmResult = await llm.predict(text); /*\n\"Feetful of Fun\" */ const chatModelResult = await chatModel.predict(text); /*\n\"Socks O'Color\" */ The OpenAI and ChatOpenAI objects are basically just\nconfiguration objects. You can initialize them with parameters like temperature\nand others, and pass them around. Next, let's use the predictMessages method to\nrun over a list of messages. import { HumanMessage } from \"langchain/schema\";\nconst text = \"What would be a good company name for a company that makes\ncolorful socks?\"; const messages = [new HumanMessage({ content: text })]; const\nllmResult = await llm.predictMessages(messages); /* AIMessage { content:\n\"Feetful of Fun\" } */ const chatModelResult = await\nchatModel.predictMessages(messages); /* AIMessage { content: \"Socks O'Color\" }\n*/ For both these methods, you can also pass in parameters as keyword arguments.\nFor example, you could pass in temperature: 0 to adjust the","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":196,"to":240}}}}],["44",{"pageContent":"AIMessage { content: \"Socks O'Color\" } */ For both these methods, you can also\npass in parameters as keyword arguments. For example, you could pass in\ntemperature: 0 to adjust the temperature that is used from what the object was\nconfigured with. Whatever values are passed in during run time will always\noverride what the object was configured with. PROMPT TEMPLATES Most LLM\napplications do not pass user input directly into an LLM. Usually they will add\nthe user input to a larger piece of text, called a prompt template, that\nprovides additional context on the specific task at hand. In the previous\nexample, the text we passed to the model contained instructions to generate a\ncompany name. For our application, it'd be great if the user only had to provide\nthe description of a company/product, without having to worry about giving the\nmodel instructions. PromptTemplates help with exactly this! They bundle up all\nthe logic for going from user input into a fully formatted","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":240,"to":262}}}}],["45",{"pageContent":"a company/product, without having to worry about giving the model instructions.\nPromptTemplates help with exactly this! They bundle up all the logic for going\nfrom user input into a fully formatted prompt. This can start off very simple -\nfor example, a prompt to produce the above string would just be: import {\nPromptTemplate } from \"langchain/prompts\"; const prompt =\nPromptTemplate.fromTemplate(\"What is a good name for a company that makes\n{product}?\"); const formattedPrompt = await prompt.format({ product: \"colorful\nsocks\", }); /* \"What is a good name for a company that makes colorful socks?\" */\nThere are several advantages to using these over raw string formatting. You can\n\"partial\" out variables - e.g. you can format only some of the variables at a\ntime. You can compose them together, easily combining different templates into a\nsingle prompt. For explanations of these functionalities, see the section on\nprompts [/docs/modules/model_io/prompts] for more","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":262,"to":284}}}}],["46",{"pageContent":"can compose them together, easily combining different templates into a single\nprompt. For explanations of these functionalities, see the section on prompts\n[/docs/modules/model_io/prompts] for more detail. PromptTemplates can also be\nused to produce a list of messages. In this case, the prompt not only contains\ninformation about the content, but also each message (its role, its position in\nthe list, etc). Here, what happens most often is a ChatPromptTemplate is a list\nof ChatMessageTemplates. Each ChatMessageTemplate contains instructions for how\nto format that ChatMessage - its role, and then also its content. Let's take a\nlook at this below: import { ChatPromptTemplate } from \"langchain/prompts\";\nconst template = \"You are a helpful assistant that translates {input_language}\ninto {output_language}.\"; const humanTemplate = \"{text}\"; const chatPrompt =\nChatPromptTemplate.fromMessages([ [\"system\", template], [\"human\",\nhumanTemplate], ]); const formattedChatPrompt = await","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":284,"to":302}}}}],["47",{"pageContent":"{output_language}.\"; const humanTemplate = \"{text}\"; const chatPrompt =\nChatPromptTemplate.fromMessages([ [\"system\", template], [\"human\",\nhumanTemplate], ]); const formattedChatPrompt = await\nchatPrompt.formatMessages({ input_language: \"English\", output_language:\n\"French\", text: \"I love programming.\", }); /* [ SystemMessage { content: 'You\nare a helpful assistant that translates English into French.' }, HumanMessage {\ncontent: 'I love programming.' } ] */ ChatPromptTemplates can also be\nconstructed in other ways - see the section on prompts\n[/docs/modules/model_io/prompts] for more detail. OUTPUT PARSERS OutputParsers\nconvert the raw output of an LLM into a format that can be used downstream.\nThere are few main type of OutputParsers, including: * Convert text from LLM ->\nstructured information (e.g. JSON) * Convert a ChatMessage into just a string *\nConvert the extra information returned from a call besides the message (like\nOpenAI","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":302,"to":339}}}}],["48",{"pageContent":"* Convert text from LLM -> structured information (e.g. JSON) * Convert a\nChatMessage into just a string * Convert the extra information returned from a\ncall besides the message (like OpenAI function invocation) into a string. For\nmore information, see the section on output parsers\n[/docs/modules/model_io/output_parsers]. In this getting started guide, we will\nwrite our own output parser - one that converts a comma separated list into a\nlist. import { BaseOutputParser } from \"langchain/schema/output_parser\"; /** *\nParse the output of an LLM call to a comma-separated list. */ class\nCommaSeparatedListOutputParser extends BaseOutputParser { async parse(text:\nstring): Promise { return text.split(\",\").map((item) => item.trim()); } } const\nparser = new CommaSeparatedListOutputParser(); const result = await\nparser.parse(\"hi, bye\"); /* ['hi', 'bye'] */ PROMPTTEMPLATE + LLM + OUTPUTPARSER\nWe can now combine all these into one chain. This chain","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":339,"to":371}}}}],["49",{"pageContent":"result = await parser.parse(\"hi, bye\"); /* ['hi', 'bye'] */ PROMPTTEMPLATE + LLM\n+ OUTPUTPARSER We can now combine all these into one chain. This chain will take\ninput variables, pass those to a prompt template to create a prompt, pass the\nprompt to a language model, and then pass the output through an (optional)\noutput parser. This is a convenient way to bundle up a modular piece of logic.\nLet's see it in action! import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; import { ChatPromptTemplate } from\n\"langchain/prompts\"; import { BaseOutputParser } from\n\"langchain/schema/output_parser\"; /** * Parse the output of an LLM call to a\ncomma-separated list. */ class CommaSeparatedListOutputParser extends\nBaseOutputParser { async parse(text: string): Promise { return\ntext.split(\",\").map((item) => item.trim()); } } const template = `You are a\nhelpful assistant who generates comma separated lists. A user will pass in a\ncategory, and you should","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":371,"to":400}}}}],["50",{"pageContent":"{ return text.split(\",\").map((item) => item.trim()); } } const template = `You\nare a helpful assistant who generates comma separated lists. A user will pass in\na category, and you should generate 5 objects in that category in a comma\nseparated list. ONLY return a comma separated list, and nothing more.`; const\nhumanTemplate = \"{text}\"; /** * Chat prompt for generating comma-separated\nlists. It combines the system * template and the human template. */ const\nchatPrompt = ChatPromptTemplate.fromMessages( [ [\"system\", template], [\"human\",\nhumanTemplate], ] ); const model = new ChatOpenAI({}); const parser = new\nCommaSeparatedListOutputParser(); const chain =\nchatPrompt.pipe(model).pipe(parser); const result = await chain.invoke({ text:\n\"colors\", }); /* [\"red\", \"blue\", \"green\", \"yellow\", \"orange\"] */ Note that we\nare using the .pipe() method to join these components together. This .pipe()\nmethod is part of the LangChain Expression Language. To learn more","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":400,"to":439}}}}],["51",{"pageContent":"\"green\", \"yellow\", \"orange\"] */ Note that we are using the .pipe() method to\njoin these components together. This .pipe() method is part of the LangChain\nExpression Language. To learn more about this syntax, read the documentation\nhere [/docs/expression_language]. NEXT STEPS And that's it for the quickstart!\nWe've now gone over how to create the core building block of LangChain\napplications. There is a lot more nuance in all these components (LLMs, prompts,\noutput parsers) and a lot more different components to learn about as well. To\ncontinue on your journey: * Dive deeper [/docs/modules/model_io] into LLMs,\nprompts, and output parsers * Learn the other key components [/docs/modules] *\nRead up on LangChain Expression Language [/docs/expression_language] to learn\nhow to chain these components together * Check out our helpful guides\n[/docs/guides] for detailed walkthroughs on particular topics * Explore\nend-to-end use cases","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":439,"to":459}}}}],["52",{"pageContent":"to learn how to chain these components together * Check out our helpful guides\n[/docs/guides] for detailed walkthroughs on particular topics * Explore\nend-to-end use cases [/docs/use_cases/] Previous Installation\n[/docs/get_started/installation] Next LangChain Expression Language (LCEL)\n[/docs/expression_language/] * Installation * Environment setup * Building an\napplication * LLMs * Prompt templates * Output parsers * PromptTemplate + LLM +\nOutputParser * Next steps Community * Discord [https://discord.gg/cU2adEyC7w] *\nTwitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/get_started/quickstart","title":"Quickstart | 🦜️🔗 Langchain","description":"Installation","language":"en","loc":{"lines":{"from":459,"to":490}}}}],["53",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/expression_language/","title":"LangChain Expression Language (LCEL) | 🦜️🔗 Langchain","description":"LangChain Expression Language or LCEL is a declarative way to easily compose chains together.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["54",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language LANGCHAIN\nEXPRESSION LANGUAGE (LCEL) LangChain Expression Language or LCEL is a\ndeclarative way to easily compose chains together. Any chain constructed this\nway will automatically have full sync, async, and streaming support. INTERFACE\n[/docs/expression_language/interface] The base interface shared by all LCEL\nobjects COOKBOOK [/docs/expression_language/cookbook] Examples of","metadata":{"source":"https://js.langchain.com/docs/expression_language/","title":"LangChain Expression Language (LCEL) | 🦜️🔗 Langchain","description":"LangChain Expression Language or LCEL is a declarative way to easily compose chains together.","language":"en","loc":{"lines":{"from":25,"to":56}}}}],["55",{"pageContent":"full sync, async, and streaming support. INTERFACE\n[/docs/expression_language/interface] The base interface shared by all LCEL\nobjects COOKBOOK [/docs/expression_language/cookbook] Examples of common LCEL\nusage patterns Previous Quickstart [/docs/get_started/quickstart] Next Route\nbetween multiple runnables [/docs/expression_language/how_to/routing] Community\n* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/","title":"LangChain Expression Language (LCEL) | 🦜️🔗 Langchain","description":"LangChain Expression Language or LCEL is a declarative way to easily compose chains together.","language":"en","loc":{"lines":{"from":56,"to":84}}}}],["56",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] * Retrieval augmented\ngeneration (RAG) [/docs/expression_language/cookbook/retrieval] * Multiple\nchains [/docs/expression_language/cookbook/multiple_chains] * Querying a SQL DB\n[/docs/expression_language/cookbook/sql_db] * Adding memory","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook","title":"Cookbook | 🦜️🔗 Langchain","description":"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.","language":"en","loc":{"lines":{"from":1,"to":21}}}}],["57",{"pageContent":"* Multiple chains [/docs/expression_language/cookbook/multiple_chains] *\nQuerying a SQL DB [/docs/expression_language/cookbook/sql_db] * Adding memory\n[/docs/expression_language/cookbook/adding_memory] * Using tools\n[/docs/expression_language/cookbook/tools] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook","title":"Cookbook | 🦜️🔗 Langchain","description":"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.","language":"en","loc":{"lines":{"from":21,"to":42}}}}],["58",{"pageContent":"* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * Cookbook COOKBOOK Example code for accomplishing\ncommon tasks with the LangChain Expression Language (LCEL). These examples show\nhow to compose different Runnable (the core LCEL interface) components to\nachieve various tasks. If you're just getting acquainted with LCEL, the Prompt +\nLLM [/docs/expression_language/cookbook/prompt_llm_parser] page is a good place\nto start. 📄️ PROMPT + LLM One of the most foundational Expression Language\ncompositions is taking: [/docs/expression_language/cookbook/prompt_llm_parser]\n📄️ RETRIEVAL AUGMENTED GENERATION (RAG) Let's now look at adding in a retrieval\nstep to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\"","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook","title":"Cookbook | 🦜️🔗 Langchain","description":"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.","language":"en","loc":{"lines":{"from":42,"to":69}}}}],["59",{"pageContent":"RETRIEVAL AUGMENTED GENERATION (RAG) Let's now look at adding in a retrieval\nstep to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\"\nchain: [/docs/expression_language/cookbook/retrieval] 📄️ MULTIPLE CHAINS\nRunnables can easily be used to combine multiple Chains:\n[/docs/expression_language/cookbook/multiple_chains] 📄️ QUERYING A SQL DB We\ncan replicate our SQLDatabaseChain with Runnables.\n[/docs/expression_language/cookbook/sql_db] 📄️ ADDING MEMORY This shows how to\nadd memory to an arbitrary chain. Right now, you can use the memory classes but\nneed to hook them up manually.\n[/docs/expression_language/cookbook/adding_memory] 📄️ USING TOOLS Tools are\nalso runnables, and can therefore be used within a chain:\n[/docs/expression_language/cookbook/tools] Previous Use RunnableMaps\n[/docs/expression_language/how_to/map] Next Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] Community * Discord\n[https://discord.gg/cU2adEyC7w] *","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook","title":"Cookbook | 🦜️🔗 Langchain","description":"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.","language":"en","loc":{"lines":{"from":69,"to":110}}}}],["60",{"pageContent":"RunnableMaps [/docs/expression_language/how_to/map] Next Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook","title":"Cookbook | 🦜️🔗 Langchain","description":"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.","language":"en","loc":{"lines":{"from":110,"to":127}}}}],["61",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Route between multiple runnables [/docs/expression_language/how_to/routing] *\nUse RunnableMaps [/docs/expression_language/how_to/map] * Cookbook\n[/docs/expression_language/cookbook/] * LangChain Expression Language (LCEL)\n[/docs/expression_language/] * Interface [/docs/expression_language/interface] *\nModules [/docs/modules/] * Model I/ O [/docs/modules/model_io/] * Retrieval","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["62",{"pageContent":"Expression Language (LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * How to * Route between multiple runnables On this\npage ROUTE BETWEEN MULTIPLE RUNNABLES This notebook covers how to do routing in\nthe LangChain Expression Language. Routing allows you to create","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":23,"to":54}}}}],["63",{"pageContent":"How to * Route between multiple runnables On this page ROUTE BETWEEN MULTIPLE\nRUNNABLES This notebook covers how to do routing in the LangChain Expression\nLanguage. Routing allows you to create non-deterministic chains where the output\nof a previous step defines the next step. Routing helps provide structure and\nconsistency around interactions with LLMs. There are two ways to perform\nrouting: 1. Using a RunnableBranch. 2. Writing custom factory function that\ntakes the input of a previous step and returns a runnable. Importantly, this\nshould return a runnable and NOT actually execute. We'll illustrate both methods\nusing a two step sequence where the first step classifies an input question as\nbeing about LangChain, Anthropic, or Other, then routes to a corresponding\nprompt chain. USING A RUNNABLEBRANCH A RunnableBranch is initialized with a list\nof (condition, runnable) pairs and a default runnable. It selects which branch\nby passing each condition the input it's","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":54,"to":80}}}}],["64",{"pageContent":"chain. USING A RUNNABLEBRANCH A RunnableBranch is initialized with a list of\n(condition, runnable) pairs and a default runnable. It selects which branch by\npassing each condition the input it's invoked with. It selects the first\ncondition to evaluate to True, and runs the corresponding runnable to that\ncondition with the input. If no provided conditions match, it runs the default\nrunnable. Here's an example of what it looks like in action: import {\nChatAnthropic } from \"langchain/chat_models/anthropic\"; import { PromptTemplate\n} from \"langchain/prompts\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; import { RunnableBranch, RunnableSequence }\nfrom \"langchain/schema/runnable\"; const promptTemplate =\nPromptTemplate.fromTemplate(`Given the user question below, classify it as\neither being about \\`LangChain\\`, \\`Anthropic\\`, or \\`Other\\`. Do not respond\nwith more than one","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":80,"to":101}}}}],["65",{"pageContent":"the user question below, classify it as either being about \\`LangChain\\`,\n\\`Anthropic\\`, or \\`Other\\`. Do not respond with more than one word. {question}\nClassification:`); const model = new ChatAnthropic({ modelName: \"claude-2\", });\nconst classificationChain = RunnableSequence.from([ promptTemplate, model, new\nStringOutputParser(), ]); const classificationChainResult = await\nclassificationChain.invoke({ question: \"how do I call Anthropic?\", });\nconsole.log(classificationChainResult); /* Anthropic */ const langChainChain =\nPromptTemplate.fromTemplate( `You are an expert in langchain. Always answer\nquestions starting with \"As Harrison Chase told me\". Respond to the following\nquestion: Question: {question} Answer:` ).pipe(model); const anthropicChain =\nPromptTemplate.fromTemplate( `You are an expert in anthropic. \\ Always answer\nquestions starting with \"As Dario Amodei told me\". \\ Respond to the","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":101,"to":142}}}}],["66",{"pageContent":"anthropicChain = PromptTemplate.fromTemplate( `You are an expert in anthropic. \\\nAlways answer questions starting with \"As Dario Amodei told me\". \\ Respond to\nthe following question: Question: {question} Answer:` ).pipe(model); const\ngeneralChain = PromptTemplate.fromTemplate( `Respond to the following question:\nQuestion: {question} Answer:` ).pipe(model); const branch =\nRunnableBranch.from([ [ (x: { topic: string; question: string }) =>\nx.topic.toLowerCase().includes(\"anthropic\"), anthropicChain, ], [ (x: { topic:\nstring; question: string }) => x.topic.toLowerCase().includes(\"langchain\"),\nlangChainChain, ], generalChain, ]); const fullChain = RunnableSequence.from([ {\ntopic: classificationChain, question: (input: { question: string }) =>\ninput.question, }, branch, ]); const result1 = await fullChain.invoke({\nquestion: \"how do I use Anthropic?\", }); console.log(result1); /* AIMessage {\ncontent: ' As Dario Amodei","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":142,"to":188}}}}],["67",{"pageContent":"}) => input.question, }, branch, ]); const result1 = await fullChain.invoke({\nquestion: \"how do I use Anthropic?\", }); console.log(result1); /* AIMessage {\ncontent: ' As Dario Amodei told me, here are some tips for how to use\nAnthropic:\\n' + '\\n' + \"First, sign up for an account on Anthropic's website.\nThis will give you access to their conversational AI assistant named Claude. \\n\"\n+ '\\n' + \"Once you've created an account, you can have conversations with Claude\nthrough their web interface. Talk to Claude like you would talk to a person,\nasking questions, giving instructions, etc. Claude is trained to have natural\nconversations and be helpful.\\n\" + '\\n' + \"You can also integrate Claude into\nyour own applications using Anthropic's API. This allows you to build Claude's\nconversational abilities into chatbots, virtual assistants, and other AI systems\nyou develop.\\n\" + '\\n' + 'Anthropic is constantly working on improving","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":188,"to":209}}}}],["68",{"pageContent":"This allows you to build Claude's conversational abilities into chatbots,\nvirtual assistants, and other AI systems you develop.\\n\" + '\\n' + 'Anthropic is\nconstantly working on improving Claude, so its capabilities are always\nexpanding. Make sure to check their blog and documentation to stay up to date on\nthe latest features.\\n' + '\\n' + 'The key is to interact with Claude regularly\nso it can learn from you. The more you chat with it, the better it will become\nat understanding you and having personalized conversations. Over time, Claude\nwill feel more human-like as it accumulates more conversational experience.',\nadditional_kwargs: {} } */ const result2 = await fullChain.invoke({ question:\n\"how do I use LangChain?\", }); console.log(result2); /* AIMessage { content: '\nAs Harrison Chase told me, here is how you use LangChain:\\n' + '\\n' + 'First,\nthink carefully about what you want to ask or have the AI do. Frame your request\nclearly","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":209,"to":228}}}}],["69",{"pageContent":"{ content: ' As Harrison Chase told me, here is how you use LangChain:\\n' + '\\n'\n+ 'First, think carefully about what you want to ask or have the AI do. Frame\nyour request clearly and specifically. Avoid vague or overly broad prompts that\ncould lead to unhelpful or concerning responses. \\n' + '\\n' + 'Next, type your\nquestion or request into the chat window and send it. Be patient as the AI\nprocesses your input and generates a response. The AI will do its best to\nprovide a helpful answer or follow your instructions, but its capabilities are\nlimited.\\n' + '\\n' + 'Keep your requests simple at first. Ask basic questions or\nhave the AI summarize content or generate basic text. As you get more\ncomfortable, you can try having the AI perform more complex tasks like answering\ntricky questions, generating stories, or having a conversation.\\n' + '\\n' + \"Pay\nattention to the AI's responses. If they seem off topic, nonsensical, or\nconcerning,","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":228,"to":237}}}}],["70",{"pageContent":"tasks like answering tricky questions, generating stories, or having a\nconversation.\\n' + '\\n' + \"Pay attention to the AI's responses. If they seem off\ntopic, nonsensical, or concerning, rephrase your prompt to steer the AI in a\nbetter direction. You may need to provide additional clarification or context to\nget useful results.\\n\" + '\\n' + 'Be polite and respectful towards the AI system.\nRemember, it is a tool designed to be helpful, harmless, and honest. Do not try\nto trick, confuse, or exploit it. \\n' + '\\n' + 'I hope these tips help you have\na safe, fun and productive experience using LangChain! Let me know if you have\nany other questions.', additional_kwargs: {} } */ const result3 = await\nfullChain.invoke({ question: \"what is 2 + 2?\", }); console.log(result3); /*\nAIMessage { content: ' 4', additional_kwargs: {} } */ API REFERENCE: *\nChatAnthropic [/docs/api/chat_models_anthropic/classes/ChatAnthropic] from","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":237,"to":266}}}}],["71",{"pageContent":"2?\", }); console.log(result3); /* AIMessage { content: ' 4', additional_kwargs:\n{} } */ API REFERENCE: * ChatAnthropic\n[/docs/api/chat_models_anthropic/classes/ChatAnthropic] from\nlangchain/chat_models/anthropic * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nStringOutputParser [/docs/api/schema_output_parser/classes/StringOutputParser]\nfrom langchain/schema/output_parser * RunnableBranch\n[/docs/api/schema_runnable/classes/RunnableBranch] from\nlangchain/schema/runnable * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable USING A CUSTOM FUNCTION You can also use a custom\nfunction to route between different outputs. Here's an example: import {\nChatAnthropic } from \"langchain/chat_models/anthropic\"; import { PromptTemplate\n} from \"langchain/prompts\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; import { RunnableLambda, RunnableSequence }\nfrom","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":266,"to":297}}}}],["72",{"pageContent":"{ PromptTemplate } from \"langchain/prompts\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; import { RunnableLambda, RunnableSequence }\nfrom \"langchain/schema/runnable\"; const promptTemplate =\nPromptTemplate.fromTemplate(`Given the user question below, classify it as\neither being about \\`LangChain\\`, \\`Anthropic\\`, or \\`Other\\`. Do not respond\nwith more than one word. {question} Classification:`); const model = new\nChatAnthropic({ modelName: \"claude-2\", }); const classificationChain =\nRunnableSequence.from([ promptTemplate, model, new StringOutputParser(), ]);\nconst classificationChainResult = await classificationChain.invoke({ question:\n\"how do I call Anthropic?\", }); console.log(classificationChainResult); /*\nAnthropic */ const langChainChain = PromptTemplate.fromTemplate( `You are an\nexpert in langchain. Always answer questions starting with \"As Harrison Chase\ntold","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":297,"to":333}}}}],["73",{"pageContent":"Anthropic */ const langChainChain = PromptTemplate.fromTemplate( `You are an\nexpert in langchain. Always answer questions starting with \"As Harrison Chase\ntold me\". Respond to the following question: Question: {question} Answer:`\n).pipe(model); const anthropicChain = PromptTemplate.fromTemplate( `You are an\nexpert in anthropic. \\ Always answer questions starting with \"As Dario Amodei\ntold me\". \\ Respond to the following question: Question: {question} Answer:`\n).pipe(model); const generalChain = PromptTemplate.fromTemplate( `Respond to the\nfollowing question: Question: {question} Answer:` ).pipe(model); const route =\n({ topic }: { input: string; topic: string }) => { if\n(topic.toLowerCase().includes(\"anthropic\")) { return anthropicChain; } else if\n(topic.toLowerCase().includes(\"langchain\")) { return langChainChain; } else {\nreturn generalChain; } }; const fullChain = RunnableSequence.from([ { topic:\nclassificationChain, question: (input: {","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":333,"to":374}}}}],["74",{"pageContent":"{ return langChainChain; } else { return generalChain; } }; const fullChain =\nRunnableSequence.from([ { topic: classificationChain, question: (input: {\nquestion: string }) => input.question, }, route, ]); const result1 = await\nfullChain.invoke({ question: \"how do I use Anthropic?\", });\nconsole.log(result1); /* AIMessage { content: ' As Dario Amodei told me, here\nare some tips for how to use Anthropic:\\n' + '\\n' + \"First, sign up for an\naccount on Anthropic's website. This will give you access to their\nconversational AI assistant named Claude. \\n\" + '\\n' + \"Once you've created an\naccount, you can have conversations with Claude through their web interface.\nTalk to Claude like you would talk to a person, asking questions, giving\ninstructions, etc. Claude is trained to have natural conversations and be\nhelpful.\\n\" + '\\n' + \"You can also integrate Claude into your own applications\nusing Anthropic's API. This allows","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":374,"to":403}}}}],["75",{"pageContent":"instructions, etc. Claude is trained to have natural conversations and be\nhelpful.\\n\" + '\\n' + \"You can also integrate Claude into your own applications\nusing Anthropic's API. This allows you to build Claude's conversational\nabilities into chatbots, virtual assistants, and other AI systems you\ndevelop.\\n\" + '\\n' + 'Anthropic is constantly working on improving Claude, so\nits capabilities are always expanding. Make sure to check their blog and\ndocumentation to stay up to date on the latest features.\\n' + '\\n' + 'The key is\nto interact with Claude regularly so it can learn from you. The more you chat\nwith it, the better it will become at understanding you and having personalized\nconversations. Over time, Claude will feel more human-like as it accumulates\nmore conversational experience.', additional_kwargs: {} } */ const result2 =\nawait fullChain.invoke({ question: \"how do I use LangChain?\", });\nconsole.log(result2); /* AIMessage {","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":403,"to":421}}}}],["76",{"pageContent":"more conversational experience.', additional_kwargs: {} } */ const result2 =\nawait fullChain.invoke({ question: \"how do I use LangChain?\", });\nconsole.log(result2); /* AIMessage { content: ' As Harrison Chase told me, here\nis how you use LangChain:\\n' + '\\n' + 'First, think carefully about what you\nwant to ask or have the AI do. Frame your request clearly and specifically.\nAvoid vague or overly broad prompts that could lead to unhelpful or concerning\nresponses. \\n' + '\\n' + 'Next, type your question or request into the chat\nwindow and send it. Be patient as the AI processes your input and generates a\nresponse. The AI will do its best to provide a helpful answer or follow your\ninstructions, but its capabilities are limited.\\n' + '\\n' + 'Keep your requests\nsimple at first. Ask basic questions or have the AI summarize content or\ngenerate basic text. As you get more comfortable, you can try having the AI\nperform more complex tasks like","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":421,"to":440}}}}],["77",{"pageContent":"your requests simple at first. Ask basic questions or have the AI summarize\ncontent or generate basic text. As you get more comfortable, you can try having\nthe AI perform more complex tasks like answering tricky questions, generating\nstories, or having a conversation.\\n' + '\\n' + \"Pay attention to the AI's\nresponses. If they seem off topic, nonsensical, or concerning, rephrase your\nprompt to steer the AI in a better direction. You may need to provide additional\nclarification or context to get useful results.\\n\" + '\\n' + 'Be polite and\nrespectful towards the AI system. Remember, it is a tool designed to be helpful,\nharmless, and honest. Do not try to trick, confuse, or exploit it. \\n' + '\\n' +\n'I hope these tips help you have a safe, fun and productive experience using\nLangChain! Let me know if you have any other questions.', additional_kwargs: {}\n} */ const result3 = await fullChain.invoke({ question: \"what is 2 +","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":440,"to":452}}}}],["78",{"pageContent":"safe, fun and productive experience using LangChain! Let me know if you have any\nother questions.', additional_kwargs: {} } */ const result3 = await\nfullChain.invoke({ question: \"what is 2 + 2?\", }); console.log(result3); /*\nAIMessage { content: ' 4', additional_kwargs: {} } */ API REFERENCE: *\nChatAnthropic [/docs/api/chat_models_anthropic/classes/ChatAnthropic] from\nlangchain/chat_models/anthropic * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nStringOutputParser [/docs/api/schema_output_parser/classes/StringOutputParser]\nfrom langchain/schema/output_parser * RunnableLambda\n[/docs/api/schema_runnable/classes/RunnableLambda] from\nlangchain/schema/runnable * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable Previous LangChain Expression Language (LCEL)\n[/docs/expression_language/] Next Use RunnableMaps\n[/docs/expression_language/how_to/map] * Using a","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":601,"to":636}}}}],["79",{"pageContent":"from langchain/schema/runnable Previous LangChain Expression Language (LCEL)\n[/docs/expression_language/] Next Use RunnableMaps\n[/docs/expression_language/how_to/map] * Using a RunnableBranch * Using a custom\nfunction Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/routing","title":"Route between multiple runnables | 🦜️🔗 Langchain","description":"This notebook covers how to do routing in the LangChain Expression Language.","language":"en","loc":{"lines":{"from":636,"to":659}}}}],["80",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] * Retrieval augmented\ngeneration (RAG) [/docs/expression_language/cookbook/retrieval] * Multiple\nchains [/docs/expression_language/cookbook/multiple_chains] * Querying a SQL DB\n[/docs/expression_language/cookbook/sql_db] * Adding memory","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/","title":"Cookbook | 🦜️🔗 Langchain","description":"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.","language":"en","loc":{"lines":{"from":1,"to":21}}}}],["81",{"pageContent":"* Multiple chains [/docs/expression_language/cookbook/multiple_chains] *\nQuerying a SQL DB [/docs/expression_language/cookbook/sql_db] * Adding memory\n[/docs/expression_language/cookbook/adding_memory] * Using tools\n[/docs/expression_language/cookbook/tools] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/","title":"Cookbook | 🦜️🔗 Langchain","description":"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.","language":"en","loc":{"lines":{"from":21,"to":42}}}}],["82",{"pageContent":"* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * Cookbook COOKBOOK Example code for accomplishing\ncommon tasks with the LangChain Expression Language (LCEL). These examples show\nhow to compose different Runnable (the core LCEL interface) components to\nachieve various tasks. If you're just getting acquainted with LCEL, the Prompt +\nLLM [/docs/expression_language/cookbook/prompt_llm_parser] page is a good place\nto start. 📄️ PROMPT + LLM One of the most foundational Expression Language\ncompositions is taking: [/docs/expression_language/cookbook/prompt_llm_parser]\n📄️ RETRIEVAL AUGMENTED GENERATION (RAG) Let's now look at adding in a retrieval\nstep to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\"","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/","title":"Cookbook | 🦜️🔗 Langchain","description":"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.","language":"en","loc":{"lines":{"from":42,"to":69}}}}],["83",{"pageContent":"RETRIEVAL AUGMENTED GENERATION (RAG) Let's now look at adding in a retrieval\nstep to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\"\nchain: [/docs/expression_language/cookbook/retrieval] 📄️ MULTIPLE CHAINS\nRunnables can easily be used to combine multiple Chains:\n[/docs/expression_language/cookbook/multiple_chains] 📄️ QUERYING A SQL DB We\ncan replicate our SQLDatabaseChain with Runnables.\n[/docs/expression_language/cookbook/sql_db] 📄️ ADDING MEMORY This shows how to\nadd memory to an arbitrary chain. Right now, you can use the memory classes but\nneed to hook them up manually.\n[/docs/expression_language/cookbook/adding_memory] 📄️ USING TOOLS Tools are\nalso runnables, and can therefore be used within a chain:\n[/docs/expression_language/cookbook/tools] Previous Use RunnableMaps\n[/docs/expression_language/how_to/map] Next Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] Community * Discord\n[https://discord.gg/cU2adEyC7w] *","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/","title":"Cookbook | 🦜️🔗 Langchain","description":"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.","language":"en","loc":{"lines":{"from":69,"to":110}}}}],["84",{"pageContent":"RunnableMaps [/docs/expression_language/how_to/map] Next Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/","title":"Cookbook | 🦜️🔗 Langchain","description":"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.","language":"en","loc":{"lines":{"from":110,"to":127}}}}],["85",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] * Retrieval augmented\ngeneration (RAG) [/docs/expression_language/cookbook/retrieval] * Multiple\nchains [/docs/expression_language/cookbook/multiple_chains] * Querying a SQL DB\n[/docs/expression_language/cookbook/sql_db] * Adding memory","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/prompt_llm_parser","title":"Prompt + LLM | 🦜️🔗 Langchain","description":"One of the most foundational Expression Language compositions is taking:","language":"en","loc":{"lines":{"from":1,"to":21}}}}],["86",{"pageContent":"* Multiple chains [/docs/expression_language/cookbook/multiple_chains] *\nQuerying a SQL DB [/docs/expression_language/cookbook/sql_db] * Adding memory\n[/docs/expression_language/cookbook/adding_memory] * Using tools\n[/docs/expression_language/cookbook/tools] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/prompt_llm_parser","title":"Prompt + LLM | 🦜️🔗 Langchain","description":"One of the most foundational Expression Language compositions is taking:","language":"en","loc":{"lines":{"from":21,"to":42}}}}],["87",{"pageContent":"* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * Cookbook [/docs/expression_language/cookbook/] *\nPrompt + LLM PROMPT + LLM One of the most foundational Expression Language\ncompositions is taking: PromptTemplate / ChatPromptTemplate -> LLM / ChatModel\n-> OutputParser Almost all other chains you build will use this building block.\nINTERACTIVE WALKTHROUGH The below scrim from Scrimba\n[https://scrimba.com/scrim/c6rD6Nt9] interactively walks through a simple prompt\ntemplate + LLM chain. You can update and run the code as it's being written in\nthe video: PROMPTTEMPLATE + LLM A PromptTemplate -> LLM is a core chain that is\nused in most other larger chains/systems. import { PromptTemplate } from\n\"langchain/prompts\"; import { ChatOpenAI } from","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/prompt_llm_parser","title":"Prompt + LLM | 🦜️🔗 Langchain","description":"One of the most foundational Expression Language compositions is taking:","language":"en","loc":{"lines":{"from":42,"to":74}}}}],["88",{"pageContent":"+ LLM A PromptTemplate -> LLM is a core chain that is used in most other larger\nchains/systems. import { PromptTemplate } from \"langchain/prompts\"; import {\nChatOpenAI } from \"langchain/chat_models/openai\"; const model = new\nChatOpenAI({}); const promptTemplate = PromptTemplate.fromTemplate( \"Tell me a\njoke about {topic}\" ); const chain = promptTemplate.pipe(model); const result =\nawait chain.invoke({ topic: \"bears\" }); console.log(result); /* AIMessage {\ncontent: \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", } */ API\nREFERENCE: * PromptTemplate [/docs/api/prompts/classes/PromptTemplate] from\nlangchain/prompts * ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI]\nfrom langchain/chat_models/openai Often times we want to attach kwargs to the\nmodel that's passed in. To do this, runnables contain a .bind method. Here's how\nyou can use it: ATTACHING STOP SEQUENCES Interactive tutorial import {\nPromptTemplate } from","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/prompt_llm_parser","title":"Prompt + LLM | 🦜️🔗 Langchain","description":"One of the most foundational Expression Language compositions is taking:","language":"en","loc":{"lines":{"from":74,"to":115}}}}],["89",{"pageContent":"attach kwargs to the model that's passed in. To do this, runnables contain a\n.bind method. Here's how you can use it: ATTACHING STOP SEQUENCES Interactive\ntutorial import { PromptTemplate } from \"langchain/prompts\"; import { ChatOpenAI\n} from \"langchain/chat_models/openai\"; const prompt =\nPromptTemplate.fromTemplate(`Tell me a joke about {subject}`); const model = new\nChatOpenAI({}); const chain = prompt.pipe(model.bind({ stop: [\"\\n\"] })); const\nresult = await chain.invoke({ subject: \"bears\" }); console.log(result); /*\nAIMessage { contents: \"Why don't bears use cell phones?\" } */ API REFERENCE: *\nPromptTemplate [/docs/api/prompts/classes/PromptTemplate] from langchain/prompts\n* ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai ATTACHING FUNCTION CALL INFORMATION Interactive\ntutorial import { PromptTemplate } from \"langchain/prompts\"; import { ChatOpenAI\n} from \"langchain/chat_models/openai\"; const prompt =","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/prompt_llm_parser","title":"Prompt + LLM | 🦜️🔗 Langchain","description":"One of the most foundational Expression Language compositions is taking:","language":"en","loc":{"lines":{"from":115,"to":160}}}}],["90",{"pageContent":"FUNCTION CALL INFORMATION Interactive tutorial import { PromptTemplate } from\n\"langchain/prompts\"; import { ChatOpenAI } from \"langchain/chat_models/openai\";\nconst prompt = PromptTemplate.fromTemplate(`Tell me a joke about {subject}`);\nconst model = new ChatOpenAI({}); const functionSchema = [ { name: \"joke\",\ndescription: \"A joke\", parameters: { type: \"object\", properties: { setup: {\ntype: \"string\", description: \"The setup for the joke\", }, punchline: { type:\n\"string\", description: \"The punchline for the joke\", }, }, required: [\"setup\",\n\"punchline\"], }, }, ]; const chain = prompt.pipe( model.bind({ functions:\nfunctionSchema, function_call: { name: \"joke\" }, }) ); const result = await\nchain.invoke({ subject: \"bears\" }); console.log(result); /* AIMessage { content:\n\"\", additional_kwargs: { function_call: { name: \"joke\",","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/prompt_llm_parser","title":"Prompt + LLM | 🦜️🔗 Langchain","description":"One of the most foundational Expression Language compositions is taking:","language":"en","loc":{"lines":{"from":160,"to":209}}}}],["91",{"pageContent":"}) ); const result = await chain.invoke({ subject: \"bears\" });\nconsole.log(result); /* AIMessage { content: \"\", additional_kwargs: {\nfunction_call: { name: \"joke\", arguments: '{\\n \"setup\": \"Why don\\'t bears wear\nshoes?\",\\n \"punchline\": \"Because they have bear feet!\"\\n}' } } } */ API\nREFERENCE: * PromptTemplate [/docs/api/prompts/classes/PromptTemplate] from\nlangchain/prompts * ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI]\nfrom langchain/chat_models/openai PROMPTTEMPLATE + LLM + OUTPUTPARSER\nInteractive tutorial We can also add in an output parser to conveniently\ntransform the raw LLM/ChatModel output into a consistent string format: import {\nPromptTemplate } from \"langchain/prompts\"; import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; import { RunnableSequence } from\n\"langchain/schema/runnable\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; const model = new","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/prompt_llm_parser","title":"Prompt + LLM | 🦜️🔗 Langchain","description":"One of the most foundational Expression Language compositions is taking:","language":"en","loc":{"lines":{"from":209,"to":249}}}}],["92",{"pageContent":"ChatOpenAI } from \"langchain/chat_models/openai\"; import { RunnableSequence }\nfrom \"langchain/schema/runnable\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; const model = new ChatOpenAI({}); const\npromptTemplate = PromptTemplate.fromTemplate( \"Tell me a joke about {topic}\" );\nconst outputParser = new StringOutputParser(); const chain =\nRunnableSequence.from([promptTemplate, model, outputParser]); const result =\nawait chain.invoke({ topic: \"bears\" }); console.log(result); /* \"Why don't bears\nwear shoes?\\n\\nBecause they have bear feet!\" */ API REFERENCE: * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable * StringOutputParser\n[/docs/api/schema_output_parser/classes/StringOutputParser] from","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/prompt_llm_parser","title":"Prompt + LLM | 🦜️🔗 Langchain","description":"One of the most foundational Expression Language compositions is taking:","language":"en","loc":{"lines":{"from":249,"to":277}}}}],["93",{"pageContent":"* RunnableSequence [/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable * StringOutputParser\n[/docs/api/schema_output_parser/classes/StringOutputParser] from\nlangchain/schema/output_parser Previous Cookbook\n[/docs/expression_language/cookbook/] Next Retrieval augmented generation (RAG)\n[/docs/expression_language/cookbook/retrieval] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/prompt_llm_parser","title":"Prompt + LLM | 🦜️🔗 Langchain","description":"One of the most foundational Expression Language compositions is taking:","language":"en","loc":{"lines":{"from":277,"to":298}}}}],["94",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] * Retrieval augmented\ngeneration (RAG) [/docs/expression_language/cookbook/retrieval] * Multiple\nchains [/docs/expression_language/cookbook/multiple_chains] * Querying a SQL DB\n[/docs/expression_language/cookbook/sql_db] * Adding memory","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":1,"to":21}}}}],["95",{"pageContent":"* Multiple chains [/docs/expression_language/cookbook/multiple_chains] *\nQuerying a SQL DB [/docs/expression_language/cookbook/sql_db] * Adding memory\n[/docs/expression_language/cookbook/adding_memory] * Using tools\n[/docs/expression_language/cookbook/tools] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":21,"to":42}}}}],["96",{"pageContent":"* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * Cookbook [/docs/expression_language/cookbook/] *\nRetrieval augmented generation (RAG) On this page RAG Let's now look at adding\nin a retrieval step to a prompt and an LLM, which adds up to a\n\"retrieval-augmented generation\" chain: import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; import { HNSWLib } from\n\"langchain/vectorstores/hnswlib\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { PromptTemplate } from\n\"langchain/prompts\"; import { RunnableSequence, RunnablePassthrough, } from\n\"langchain/schema/runnable\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; import { Document } from \"langchain/document\";\nconst model = new ChatOpenAI({}); const","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":42,"to":73}}}}],["97",{"pageContent":"from \"langchain/schema/runnable\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; import { Document } from \"langchain/document\";\nconst model = new ChatOpenAI({}); const vectorStore = await HNSWLib.fromTexts(\n[\"mitochondria is the powerhouse of the cell\"], [{ id: 1 }], new\nOpenAIEmbeddings() ); const retriever = vectorStore.asRetriever(); const prompt\n= PromptTemplate.fromTemplate(`Answer the question based only on the following\ncontext: {context} Question: {question}`); const serializeDocs = (docs:\nDocument[]) => docs.map((doc) => doc.pageContent).join(\"\\n\"); const chain =\nRunnableSequence.from([ { context: retriever.pipe(serializeDocs), question: new\nRunnablePassthrough(), }, prompt, model, new StringOutputParser(), ]); const\nresult = await chain.invoke(\"What is the powerhouse of the cell?\");\nconsole.log(result); /* \"The powerhouse of the cell is the mitochondria.\" */ API\nREFERENCE: * ChatOpenAI","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":73,"to":118}}}}],["98",{"pageContent":"result = await chain.invoke(\"What is the powerhouse of the cell?\");\nconsole.log(result); /* \"The powerhouse of the cell is the mitochondria.\" */ API\nREFERENCE: * ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * HNSWLib\n[/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nRunnableSequence [/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable * RunnablePassthrough\n[/docs/api/schema_runnable/classes/RunnablePassthrough] from\nlangchain/schema/runnable * StringOutputParser\n[/docs/api/schema_output_parser/classes/StringOutputParser] from\nlangchain/schema/output_parser * Document [/docs/api/document/classes/Document]\nfrom langchain/document CONVERSATIONAL RETRIEVAL","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":118,"to":141}}}}],["99",{"pageContent":"[/docs/api/schema_output_parser/classes/StringOutputParser] from\nlangchain/schema/output_parser * Document [/docs/api/document/classes/Document]\nfrom langchain/document CONVERSATIONAL RETRIEVAL CHAIN Because\nRunnableSequence.from and runnable.pipe both accept runnable-like objects,\nincluding single-argument functions, we can add in conversation history via a\nformatting function. This allows us to recreate the popular\nConversationalRetrievalQAChain to \"chat with data\": import { PromptTemplate }\nfrom \"langchain/prompts\"; import { RunnableSequence, RunnablePassthrough, } from\n\"langchain/schema/runnable\"; import { Document } from \"langchain/document\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\"; import { HNSWLib }\nfrom \"langchain/vectorstores/hnswlib\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; const model = new ChatOpenAI({}); const\ncondenseQuestionTemplate = `Given the","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":141,"to":164}}}}],["100",{"pageContent":"} from \"langchain/embeddings/openai\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; const model = new ChatOpenAI({}); const\ncondenseQuestionTemplate = `Given the following conversation and a follow up\nquestion, rephrase the follow up question to be a standalone question, in its\noriginal language. Chat History: {chat_history} Follow Up Input: {question}\nStandalone question:`; const CONDENSE_QUESTION_PROMPT =\nPromptTemplate.fromTemplate( condenseQuestionTemplate ); const answerTemplate =\n`Answer the question based only on the following context: {context} Question:\n{question} `; const ANSWER_PROMPT = PromptTemplate.fromTemplate(answerTemplate);\nconst combineDocumentsFn = (docs: Document[], separator = \"\\n\\n\") => { const\nserializedDocs = docs.map((doc) => doc.pageContent); return\nserializedDocs.join(separator); }; const formatChatHistory = (chatHistory:\n[string, string][]) => { const formattedDialogueTurns = chatHistory.map(\n(dialogueTurn) =>","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":164,"to":193}}}}],["101",{"pageContent":"doc.pageContent); return serializedDocs.join(separator); }; const\nformatChatHistory = (chatHistory: [string, string][]) => { const\nformattedDialogueTurns = chatHistory.map( (dialogueTurn) => `Human:\n${dialogueTurn[0]}\\nAssistant: ${dialogueTurn[1]}` ); return\nformattedDialogueTurns.join(\"\\n\"); }; const vectorStore = await\nHNSWLib.fromTexts( [ \"mitochondria is the powerhouse of the cell\", \"mitochondria\nis made of lipids\", ], [{ id: 1 }, { id: 2 }], new OpenAIEmbeddings() ); const\nretriever = vectorStore.asRetriever(); type ConversationalRetrievalQAChainInput\n= { question: string; chat_history: [string, string][]; }; const\nstandaloneQuestionChain = RunnableSequence.from([ { question: (input:\nConversationalRetrievalQAChainInput) => input.question, chat_history: (input:\nConversationalRetrievalQAChainInput) => formatChatHistory(input.chat_history),\n}, CONDENSE_QUESTION_PROMPT, model, new StringOutputParser(), ]); const","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":193,"to":230}}}}],["102",{"pageContent":"chat_history: (input: ConversationalRetrievalQAChainInput) =>\nformatChatHistory(input.chat_history), }, CONDENSE_QUESTION_PROMPT, model, new\nStringOutputParser(), ]); const answerChain = RunnableSequence.from([ { context:\nretriever.pipe(combineDocumentsFn), question: new RunnablePassthrough(), },\nANSWER_PROMPT, model, ]); const conversationalRetrievalQAChain =\nstandaloneQuestionChain.pipe(answerChain); const result1 = await\nconversationalRetrievalQAChain.invoke({ question: \"What is the powerhouse of the\ncell?\", chat_history: [], }); console.log(result1); /* AIMessage { content: \"The\npowerhouse of the cell is the mitochondria.\" } */ const result2 = await\nconversationalRetrievalQAChain.invoke({ question: \"What are they made out of?\",\nchat_history: [ [ \"What is the powerhouse of the cell?\", \"The powerhouse of the\ncell is the mitochondria.\", ], ], }); console.log(result2); /* AIMessage {\ncontent: \"Mitochondria are","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":230,"to":270}}}}],["103",{"pageContent":"[ [ \"What is the powerhouse of the cell?\", \"The powerhouse of the cell is the\nmitochondria.\", ], ], }); console.log(result2); /* AIMessage { content:\n\"Mitochondria are made out of lipids.\" } */ API REFERENCE: * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nRunnableSequence [/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable * RunnablePassthrough\n[/docs/api/schema_runnable/classes/RunnablePassthrough] from\nlangchain/schema/runnable * Document [/docs/api/document/classes/Document] from\nlangchain/document * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * HNSWLib\n[/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * StringOutputParser\n[/docs/api/schema_output_parser/classes/StringOutputParser] from","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":270,"to":294}}}}],["104",{"pageContent":"* OpenAIEmbeddings [/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * StringOutputParser\n[/docs/api/schema_output_parser/classes/StringOutputParser] from\nlangchain/schema/output_parser Note that the individual chains we created are\nthemselves Runnables and can therefore be piped into each other. Previous Prompt\n+ LLM [/docs/expression_language/cookbook/prompt_llm_parser] Next Multiple\nchains [/docs/expression_language/cookbook/multiple_chains] * Conversational\nRetrieval Chain Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/retrieval","title":"Retrieval augmented generation (RAG) | 🦜️🔗 Langchain","description":"Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:","language":"en","loc":{"lines":{"from":294,"to":319}}}}],["105",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] * Retrieval augmented\ngeneration (RAG) [/docs/expression_language/cookbook/retrieval] * Multiple\nchains [/docs/expression_language/cookbook/multiple_chains] * Querying a SQL DB\n[/docs/expression_language/cookbook/sql_db] * Adding memory","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/multiple_chains","title":"Multiple chains | 🦜️🔗 Langchain","description":"Runnables can easily be used to combine multiple Chains:","language":"en","loc":{"lines":{"from":1,"to":21}}}}],["106",{"pageContent":"* Multiple chains [/docs/expression_language/cookbook/multiple_chains] *\nQuerying a SQL DB [/docs/expression_language/cookbook/sql_db] * Adding memory\n[/docs/expression_language/cookbook/adding_memory] * Using tools\n[/docs/expression_language/cookbook/tools] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/multiple_chains","title":"Multiple chains | 🦜️🔗 Langchain","description":"Runnables can easily be used to combine multiple Chains:","language":"en","loc":{"lines":{"from":21,"to":42}}}}],["107",{"pageContent":"* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * Cookbook [/docs/expression_language/cookbook/] *\nMultiple chains On this page MULTIPLE CHAINS Runnables can easily be used to\ncombine multiple Chains: import { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\"; import {\nStringOutputParser } from \"langchain/schema/output_parser\"; import {\nChatAnthropic } from \"langchain/chat_models/anthropic\"; const prompt1 =\nPromptTemplate.fromTemplate( `What is the city {person} is from? Only respond\nwith the name of the city.` ); const prompt2 = PromptTemplate.fromTemplate(\n`What country is the city {city} in? Respond in {language}.` ); const model =\nnew ChatAnthropic({}); const chain =","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/multiple_chains","title":"Multiple chains | 🦜️🔗 Langchain","description":"Runnables can easily be used to combine multiple Chains:","language":"en","loc":{"lines":{"from":42,"to":74}}}}],["108",{"pageContent":"with the name of the city.` ); const prompt2 = PromptTemplate.fromTemplate(\n`What country is the city {city} in? Respond in {language}.` ); const model =\nnew ChatAnthropic({}); const chain = prompt1.pipe(model).pipe(new\nStringOutputParser()); const combinedChain = RunnableSequence.from([ { city:\nchain, language: (input) => input.language, }, prompt2, model, new\nStringOutputParser(), ]); const result = await combinedChain.invoke({ person:\n\"Obama\", language: \"German\", }); console.log(result); /* Chicago befindet sich\nin den Vereinigten Staaten. */ API REFERENCE: * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nRunnableSequence [/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable * StringOutputParser\n[/docs/api/schema_output_parser/classes/StringOutputParser] from\nlangchain/schema/output_parser * ChatAnthropic\n[/docs/api/chat_models_anthropic/classes/ChatAnthropic] from","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/multiple_chains","title":"Multiple chains | 🦜️🔗 Langchain","description":"Runnables can easily be used to combine multiple Chains:","language":"en","loc":{"lines":{"from":74,"to":113}}}}],["109",{"pageContent":"* StringOutputParser [/docs/api/schema_output_parser/classes/StringOutputParser]\nfrom langchain/schema/output_parser * ChatAnthropic\n[/docs/api/chat_models_anthropic/classes/ChatAnthropic] from\nlangchain/chat_models/anthropic The RunnableSequence above coerces the object\ninto a RunnableMap. Each property in the map receives the same parameters. The\nrunnable or function set as the value of that property is invoked with those\nparameters, and the return value populates an object which is then passed onto\nthe next runnable in the sequence. PASSTHROUGHS In the example above, we use a\npassthrough in a runnable map to pass along original input variables to future\nsteps in the chain. In general, how exactly you do this depends on what exactly\nthe input is: * If the original input was a string, then you likely just want to\npass along the string. This can be done with RunnablePassthrough. For an example\nof this, see the retrieval chain in the RAG section","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/multiple_chains","title":"Multiple chains | 🦜️🔗 Langchain","description":"Runnables can easily be used to combine multiple Chains:","language":"en","loc":{"lines":{"from":113,"to":128}}}}],["110",{"pageContent":"original input was a string, then you likely just want to pass along the string.\nThis can be done with RunnablePassthrough. For an example of this, see the\nretrieval chain in the RAG section\n[/docs/expression_language/cookbook/retrieval] of this cookbook. * If the\noriginal input was an object, then you likely want to pass along specific keys.\nFor this, you can use an arrow function that takes the object as input and\nextracts the desired key, as shown above. Previous Retrieval augmented\ngeneration (RAG) [/docs/expression_language/cookbook/retrieval] Next Querying a\nSQL DB [/docs/expression_language/cookbook/sql_db] * Passthroughs Community *\nDiscord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/multiple_chains","title":"Multiple chains | 🦜️🔗 Langchain","description":"Runnables can easily be used to combine multiple Chains:","language":"en","loc":{"lines":{"from":128,"to":154}}}}],["111",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] * Retrieval augmented\ngeneration (RAG) [/docs/expression_language/cookbook/retrieval] * Multiple\nchains [/docs/expression_language/cookbook/multiple_chains] * Querying a SQL DB\n[/docs/expression_language/cookbook/sql_db] * Adding memory","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/sql_db","title":"Querying a SQL DB | 🦜️🔗 Langchain","description":"We can replicate our SQLDatabaseChain with Runnables.","language":"en","loc":{"lines":{"from":1,"to":21}}}}],["112",{"pageContent":"* Multiple chains [/docs/expression_language/cookbook/multiple_chains] *\nQuerying a SQL DB [/docs/expression_language/cookbook/sql_db] * Adding memory\n[/docs/expression_language/cookbook/adding_memory] * Using tools\n[/docs/expression_language/cookbook/tools] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/sql_db","title":"Querying a SQL DB | 🦜️🔗 Langchain","description":"We can replicate our SQLDatabaseChain with Runnables.","language":"en","loc":{"lines":{"from":21,"to":42}}}}],["113",{"pageContent":"* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * Cookbook [/docs/expression_language/cookbook/] *\nQuerying a SQL DB On this page QUERYING A SQL DB We can replicate our\nSQLDatabaseChain with Runnables. SETUP We'll need the Chinook sample DB for this\nexample. First install typeorm: * npm * Yarn * pnpm npm install typeorm yarn add\ntypeorm pnpm add typeorm Then install the dependencies needed for your database.\nFor example, for SQLite: * npm * Yarn * pnpm npm install sqlite3 yarn add\nsqlite3 pnpm add sqlite3 For other databases see\nhttps://typeorm.io/#installation [https://typeorm.io/#installation]. Finally\nfollow the instructions on","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/sql_db","title":"Querying a SQL DB | 🦜️🔗 Langchain","description":"We can replicate our SQLDatabaseChain with Runnables.","language":"en","loc":{"lines":{"from":42,"to":109}}}}],["114",{"pageContent":"install sqlite3 yarn add sqlite3 pnpm add sqlite3 For other databases see\nhttps://typeorm.io/#installation [https://typeorm.io/#installation]. Finally\nfollow the instructions on https://database.guide/2-sample-databases-sqlite/\n[https://database.guide/2-sample-databases-sqlite/] to get the sample database\nfor this example. COMPOSITION import { DataSource } from \"typeorm\"; import {\nSqlDatabase } from \"langchain/sql_db\"; import { RunnableSequence } from\n\"langchain/schema/runnable\"; import { PromptTemplate } from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\"; import {\nChatOpenAI } from \"langchain/chat_models/openai\"; const datasource = new\nDataSource({ type: \"sqlite\", database: \"Chinook.db\", }); const db = await\nSqlDatabase.fromDataSourceParams({ appDataSource: datasource, }); const prompt =\nPromptTemplate.fromTemplate(`Based on the table schema below, write a SQL query\nthat would answer the user's","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/sql_db","title":"Querying a SQL DB | 🦜️🔗 Langchain","description":"We can replicate our SQLDatabaseChain with Runnables.","language":"en","loc":{"lines":{"from":109,"to":149}}}}],["115",{"pageContent":"SqlDatabase.fromDataSourceParams({ appDataSource: datasource, }); const prompt =\nPromptTemplate.fromTemplate(`Based on the table schema below, write a SQL query\nthat would answer the user's question: {schema} Question: {question} SQL\nQuery:`); const model = new ChatOpenAI(); const sqlQueryGeneratorChain =\nRunnableSequence.from([ { schema: async () => db.getTableInfo(), question:\n(input: { question: string }) => input.question, }, prompt, model.bind({ stop:\n[\"\\nSQLResult:\"] }), new StringOutputParser(), ]); const result = await\nsqlQueryGeneratorChain.invoke({ question: \"How many employees are there?\", });\nconsole.log(result); /* SELECT COUNT(EmployeeId) AS TotalEmployees FROM Employee\n*/ const finalResponsePrompt = PromptTemplate.fromTemplate(`Based on the table\nschema below, question, sql query, and sql response, write a natural language\nresponse: {schema} Question: {question} SQL Query: {query} SQL Response:\n{response}`); const fullChain =","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/sql_db","title":"Querying a SQL DB | 🦜️🔗 Langchain","description":"We can replicate our SQLDatabaseChain with Runnables.","language":"en","loc":{"lines":{"from":149,"to":190}}}}],["116",{"pageContent":"on the table schema below, question, sql query, and sql response, write a\nnatural language response: {schema} Question: {question} SQL Query: {query} SQL\nResponse: {response}`); const fullChain = RunnableSequence.from([ { question:\n(input) => input.question, query: sqlQueryGeneratorChain, }, { schema: async ()\n=> db.getTableInfo(), question: (input) => input.question, query: (input) =>\ninput.query, response: (input) => db.run(input.query), }, finalResponsePrompt,\nmodel, ]); const finalResponse = await fullChain.invoke({ question: \"How many\nemployees are there?\", }); console.log(finalResponse); /* AIMessage { content:\n'There are 8 employees.', additional_kwargs: { function_call: undefined } } */\nAPI REFERENCE: * SqlDatabase [/docs/api/sql_db/classes/SqlDatabase] from\nlangchain/sql_db * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable * PromptTemplate","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/sql_db","title":"Querying a SQL DB | 🦜️🔗 Langchain","description":"We can replicate our SQLDatabaseChain with Runnables.","language":"en","loc":{"lines":{"from":190,"to":232}}}}],["117",{"pageContent":"* SqlDatabase [/docs/api/sql_db/classes/SqlDatabase] from langchain/sql_db *\nRunnableSequence [/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nStringOutputParser [/docs/api/schema_output_parser/classes/StringOutputParser]\nfrom langchain/schema/output_parser * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai Previous Multiple chains\n[/docs/expression_language/cookbook/multiple_chains] Next Adding memory\n[/docs/expression_language/cookbook/adding_memory] * Setup * Composition\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/sql_db","title":"Querying a SQL DB | 🦜️🔗 Langchain","description":"We can replicate our SQLDatabaseChain with Runnables.","language":"en","loc":{"lines":{"from":232,"to":259}}}}],["118",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] * Retrieval augmented\ngeneration (RAG) [/docs/expression_language/cookbook/retrieval] * Multiple\nchains [/docs/expression_language/cookbook/multiple_chains] * Querying a SQL DB\n[/docs/expression_language/cookbook/sql_db] * Adding memory","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/adding_memory","title":"Adding memory | 🦜️🔗 Langchain","description":"This shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook them up manually.","language":"en","loc":{"lines":{"from":1,"to":21}}}}],["119",{"pageContent":"* Multiple chains [/docs/expression_language/cookbook/multiple_chains] *\nQuerying a SQL DB [/docs/expression_language/cookbook/sql_db] * Adding memory\n[/docs/expression_language/cookbook/adding_memory] * Using tools\n[/docs/expression_language/cookbook/tools] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/adding_memory","title":"Adding memory | 🦜️🔗 Langchain","description":"This shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook them up manually.","language":"en","loc":{"lines":{"from":21,"to":42}}}}],["120",{"pageContent":"* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * Cookbook [/docs/expression_language/cookbook/] *\nAdding memory ADDING MEMORY This shows how to add memory to an arbitrary chain.\nRight now, you can use the memory classes but need to hook them up manually.\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\"; import {\nChatAnthropic } from \"langchain/chat_models/anthropic\"; import { BufferMemory }\nfrom \"langchain/memory\"; const model = new ChatAnthropic(); const prompt =\nChatPromptTemplate.fromMessages([ [\"system\", \"You are a helpful chatbot\"], new\nMessagesPlaceholder(\"history\"), [\"human\", \"{input}\"], ]); // Default \"inputKey\",\n\"outputKey\", and \"memoryKey values would","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/adding_memory","title":"Adding memory | 🦜️🔗 Langchain","description":"This shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook them up manually.","language":"en","loc":{"lines":{"from":42,"to":70}}}}],["121",{"pageContent":"[\"system\", \"You are a helpful chatbot\"], new MessagesPlaceholder(\"history\"),\n[\"human\", \"{input}\"], ]); // Default \"inputKey\", \"outputKey\", and \"memoryKey\nvalues would work here // but we specify them for clarity. const memory = new\nBufferMemory({ returnMessages: true, inputKey: \"input\", outputKey: \"output\",\nmemoryKey: \"history\", }); console.log(await memory.loadMemoryVariables({})); /*\n{ history: [] } */ const chain = RunnableSequence.from([ { input: (initialInput)\n=> initialInput.input, memory: () => memory.loadMemoryVariables({}), }, { input:\n(previousOutput) => previousOutput.input, history: (previousOutput) =>\npreviousOutput.memory.history, }, prompt, model, ]); const inputs = { input:\n\"Hey, I'm Bob!\", }; const response = await chain.invoke(inputs);\nconsole.log(response); /* AIMessage { content: \" Hi Bob, nice to meet you! I'm\nClaude, an AI assistant created by Anthropic to be helpful, harmless, and\nhonest.\",","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/adding_memory","title":"Adding memory | 🦜️🔗 Langchain","description":"This shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook them up manually.","language":"en","loc":{"lines":{"from":70,"to":113}}}}],["122",{"pageContent":"chain.invoke(inputs); console.log(response); /* AIMessage { content: \" Hi Bob,\nnice to meet you! I'm Claude, an AI assistant created by Anthropic to be\nhelpful, harmless, and honest.\", additional_kwargs: {} } */ await\nmemory.saveContext(inputs, { output: response.content, }); console.log(await\nmemory.loadMemoryVariables({})); /* { history: [ HumanMessage { content: \"Hey,\nI'm Bob!\", additional_kwargs: {} }, AIMessage { content: \" Hi Bob, nice to meet\nyou! I'm Claude, an AI assistant created by Anthropic to be helpful, harmless,\nand honest.\", additional_kwargs: {} } ] } */ const inputs2 = { input: \"What's my\nname?\", }; const response2 = await chain.invoke(inputs2);\nconsole.log(response2); /* AIMessage { content: ' You told me your name is\nBob.', additional_kwargs: {} } */ API REFERENCE: * ChatPromptTemplate\n[/docs/api/prompts/classes/ChatPromptTemplate] from langchain/prompts","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/adding_memory","title":"Adding memory | 🦜️🔗 Langchain","description":"This shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook them up manually.","language":"en","loc":{"lines":{"from":113,"to":165}}}}],["123",{"pageContent":"{ content: ' You told me your name is Bob.', additional_kwargs: {} } */ API\nREFERENCE: * ChatPromptTemplate [/docs/api/prompts/classes/ChatPromptTemplate]\nfrom langchain/prompts * MessagesPlaceholder\n[/docs/api/prompts/classes/MessagesPlaceholder] from langchain/prompts *\nRunnableSequence [/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable * ChatAnthropic\n[/docs/api/chat_models_anthropic/classes/ChatAnthropic] from\nlangchain/chat_models/anthropic * BufferMemory\n[/docs/api/memory/classes/BufferMemory] from langchain/memory Previous Querying\na SQL DB [/docs/expression_language/cookbook/sql_db] Next Using tools\n[/docs/expression_language/cookbook/tools] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/adding_memory","title":"Adding memory | 🦜️🔗 Langchain","description":"This shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook them up manually.","language":"en","loc":{"lines":{"from":165,"to":198}}}}],["124",{"pageContent":"[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/adding_memory","title":"Adding memory | 🦜️🔗 Langchain","description":"This shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook them up manually.","language":"en","loc":{"lines":{"from":198,"to":208}}}}],["125",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * Prompt + LLM\n[/docs/expression_language/cookbook/prompt_llm_parser] * Retrieval augmented\ngeneration (RAG) [/docs/expression_language/cookbook/retrieval] * Multiple\nchains [/docs/expression_language/cookbook/multiple_chains] * Querying a SQL DB\n[/docs/expression_language/cookbook/sql_db] * Adding memory","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/tools","title":"Using tools | 🦜️🔗 Langchain","description":"Tools are also runnables, and can therefore be used within a chain:","language":"en","loc":{"lines":{"from":1,"to":21}}}}],["126",{"pageContent":"* Multiple chains [/docs/expression_language/cookbook/multiple_chains] *\nQuerying a SQL DB [/docs/expression_language/cookbook/sql_db] * Adding memory\n[/docs/expression_language/cookbook/adding_memory] * Using tools\n[/docs/expression_language/cookbook/tools] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/tools","title":"Using tools | 🦜️🔗 Langchain","description":"Tools are also runnables, and can therefore be used within a chain:","language":"en","loc":{"lines":{"from":21,"to":42}}}}],["127",{"pageContent":"* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * Cookbook [/docs/expression_language/cookbook/] *\nUsing tools USING TOOLS Tools are also runnables, and can therefore be used\nwithin a chain: import { SerpAPI } from \"langchain/tools\"; import {\nChatAnthropic } from \"langchain/chat_models/anthropic\"; import { PromptTemplate\n} from \"langchain/prompts\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; const search = new SerpAPI(); const prompt =\nPromptTemplate.fromTemplate(`Turn the following user input into a search query\nfor a search engine: {input}`); const model = new ChatAnthropic({}); const chain\n= prompt.pipe(model).pipe(new StringOutputParser()).pipe(search); const result =\nawait chain.invoke({ input: \"Who is the current","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/tools","title":"Using tools | 🦜️🔗 Langchain","description":"Tools are also runnables, and can therefore be used within a chain:","language":"en","loc":{"lines":{"from":42,"to":75}}}}],["128",{"pageContent":"model = new ChatAnthropic({}); const chain = prompt.pipe(model).pipe(new\nStringOutputParser()).pipe(search); const result = await chain.invoke({ input:\n\"Who is the current prime minister of Malaysia?\", }); console.log(result); /*\nAnwar Ibrahim */ API REFERENCE: * SerpAPI [/docs/api/tools/classes/SerpAPI] from\nlangchain/tools * ChatAnthropic\n[/docs/api/chat_models_anthropic/classes/ChatAnthropic] from\nlangchain/chat_models/anthropic * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nStringOutputParser [/docs/api/schema_output_parser/classes/StringOutputParser]\nfrom langchain/schema/output_parser Previous Adding memory\n[/docs/expression_language/cookbook/adding_memory] Next LangChain Expression\nLanguage (LCEL) [/docs/expression_language/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/tools","title":"Using tools | 🦜️🔗 Langchain","description":"Tools are also runnables, and can therefore be used within a chain:","language":"en","loc":{"lines":{"from":75,"to":110}}}}],["129",{"pageContent":"* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/cookbook/tools","title":"Using tools | 🦜️🔗 Langchain","description":"Tools are also runnables, and can therefore be used within a chain:","language":"en","loc":{"lines":{"from":110,"to":121}}}}],["130",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Route between multiple runnables [/docs/expression_language/how_to/routing] *\nUse RunnableMaps [/docs/expression_language/how_to/map] * Cookbook\n[/docs/expression_language/cookbook/] * LangChain Expression Language (LCEL)\n[/docs/expression_language/] * Interface [/docs/expression_language/interface] *\nModules [/docs/modules/] * Model I/ O [/docs/modules/model_io/] * Retrieval","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/map","title":"Use RunnableMaps | 🦜️🔗 Langchain","description":"RunnableMaps allow you to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["131",{"pageContent":"Expression Language (LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * How to * Use RunnableMaps On this page USE\nRUNNABLEMAPS RunnableMaps allow you to execute multiple Runnables in parallel,\nand to return the output of these Runnables as a map. import { ChatAnthropic","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/map","title":"Use RunnableMaps | 🦜️🔗 Langchain","description":"RunnableMaps allow you to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.","language":"en","loc":{"lines":{"from":23,"to":54}}}}],["132",{"pageContent":"* Use RunnableMaps On this page USE RUNNABLEMAPS RunnableMaps allow you to\nexecute multiple Runnables in parallel, and to return the output of these\nRunnables as a map. import { ChatAnthropic } from\n\"langchain/chat_models/anthropic\"; import { PromptTemplate } from\n\"langchain/prompts\"; import { RunnableMap } from \"langchain/schema/runnable\";\nconst model = new ChatAnthropic({}); const jokeChain =\nPromptTemplate.fromTemplate( \"Tell me a joke about {topic}\" ).pipe(model); const\npoemChain = PromptTemplate.fromTemplate( \"write a 2-line poem about {topic}\"\n).pipe(model); const mapChain = RunnableMap.from({ joke: jokeChain, poem:\npoemChain, }); const result = await mapChain.invoke({ topic: \"bear\" });\nconsole.log(result); /* { joke: AIMessage { content: \" Here's a silly joke about\na bear:\\n\" + '\\n' + 'What do you call a bear with no teeth?\\n' + 'A gummy\nbear!', additional_kwargs: {} }, poem: AIMessage { content: ' Here is","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/map","title":"Use RunnableMaps | 🦜️🔗 Langchain","description":"RunnableMaps allow you to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.","language":"en","loc":{"lines":{"from":54,"to":92}}}}],["133",{"pageContent":"joke about a bear:\\n\" + '\\n' + 'What do you call a bear with no teeth?\\n' + 'A\ngummy bear!', additional_kwargs: {} }, poem: AIMessage { content: ' Here is a\n2-line poem about a bear:\\n' + '\\n' + 'Furry and wild, the bear roams free \\n' +\n'Foraging the forest, strong as can be', additional_kwargs: {} } } */ API\nREFERENCE: * ChatAnthropic\n[/docs/api/chat_models_anthropic/classes/ChatAnthropic] from\nlangchain/chat_models/anthropic * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts * RunnableMap\n[/docs/api/schema_runnable/classes/RunnableMap] from langchain/schema/runnable\nMANIPULATING OUTPUTS/INPUTS Maps can be useful for manipulating the output of\none Runnable to match the input format of the next Runnable in a sequence. Note\nbelow that the object within the RunnableSequence.from() call is automatically\ncoerced into a runnable map. All keys of the object must have","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/map","title":"Use RunnableMaps | 🦜️🔗 Langchain","description":"RunnableMaps allow you to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.","language":"en","loc":{"lines":{"from":92,"to":123}}}}],["134",{"pageContent":"the input format of the next Runnable in a sequence. Note below that the object\nwithin the RunnableSequence.from() call is automatically coerced into a runnable\nmap. All keys of the object must have values that are runnables or can be\nthemselves coerced to runnables (functions to RunnableLambdas or objects to\nRunnableMaps). This coercion will also occur when composing chains via the\n.pipe() method. import { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\"; import {\nPromptTemplate } from \"langchain/prompts\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; import { RunnablePassthrough,\nRunnableSequence, } from \"langchain/schema/runnable\"; import { HNSWLib } from\n\"langchain/vectorstores/hnswlib\"; import type { Document } from\n\"langchain/document\"; const model = new ChatAnthropic(); const vectorstore =\nawait HNSWLib.fromDocuments( [{ pageContent: \"mitochondria is the powerhouse of\nthe","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/map","title":"Use RunnableMaps | 🦜️🔗 Langchain","description":"RunnableMaps allow you to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.","language":"en","loc":{"lines":{"from":123,"to":142}}}}],["135",{"pageContent":"type { Document } from \"langchain/document\"; const model = new ChatAnthropic();\nconst vectorstore = await HNSWLib.fromDocuments( [{ pageContent: \"mitochondria\nis the powerhouse of the cell\", metadata: {} }], new CohereEmbeddings() ); const\nretriever = vectorstore.asRetriever(); const template = `Answer the question\nbased only on the following context: {context} Question: {question}`; const\nprompt = PromptTemplate.fromTemplate(template); const formatDocs = (docs:\nDocument[]) => docs.map((doc) => doc.pageContent); const retrievalChain =\nRunnableSequence.from([ { context: retriever.pipe(formatDocs), question: new\nRunnablePassthrough() }, prompt, model, new StringOutputParser(), ]); const\nresult = await retrievalChain.invoke( \"what is the powerhouse of the cell?\" );\nconsole.log(result); /* Based on the given context, the powerhouse of the cell\nis mitochondria. */ API REFERENCE: * ChatAnthropic\n[/docs/api/chat_models_anthropic/classes/ChatAnthropic] from","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/map","title":"Use RunnableMaps | 🦜️🔗 Langchain","description":"RunnableMaps allow you to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.","language":"en","loc":{"lines":{"from":142,"to":180}}}}],["136",{"pageContent":"Based on the given context, the powerhouse of the cell is mitochondria. */ API\nREFERENCE: * ChatAnthropic\n[/docs/api/chat_models_anthropic/classes/ChatAnthropic] from\nlangchain/chat_models/anthropic * CohereEmbeddings\n[/docs/api/embeddings_cohere/classes/CohereEmbeddings] from\nlangchain/embeddings/cohere * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nStringOutputParser [/docs/api/schema_output_parser/classes/StringOutputParser]\nfrom langchain/schema/output_parser * RunnablePassthrough\n[/docs/api/schema_runnable/classes/RunnablePassthrough] from\nlangchain/schema/runnable * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable * HNSWLib\n[/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * Document [/docs/api/document/classes/Document]\nfrom langchain/document Here the input to prompt is expected to be a map with\nkeys \"context\" and \"question\". The user input","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/map","title":"Use RunnableMaps | 🦜️🔗 Langchain","description":"RunnableMaps allow you to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.","language":"en","loc":{"lines":{"from":180,"to":197}}}}],["137",{"pageContent":"* Document [/docs/api/document/classes/Document] from langchain/document Here\nthe input to prompt is expected to be a map with keys \"context\" and \"question\".\nThe user input is just the question. So we need to get the context using our\nretriever and passthrough the user input under the \"question\" key. Previous\nRoute between multiple runnables [/docs/expression_language/how_to/routing] Next\nCookbook [/docs/expression_language/cookbook/] * Manipulating outputs/inputs\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/how_to/map","title":"Use RunnableMaps | 🦜️🔗 Langchain","description":"RunnableMaps allow you to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.","language":"en","loc":{"lines":{"from":197,"to":222}}}}],["138",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/expression_language/interface","title":"Interface | 🦜️🔗 Langchain","description":"In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["139",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * LangChain Expression Language\n[/docs/expression_language/] * Interface On this page INTERFACE In an effort to\nmake it as easy as possible to create custom chains, we've implemented a\n\"Runnable\" [/docs/api/schema_runnable/classes/Runnable] protocol that most\ncomponents implement. This is a standard interface with a few different methods,\nwhich make it easy to define custom chains as well as making it possible","metadata":{"source":"https://js.langchain.com/docs/expression_language/interface","title":"Interface | 🦜️🔗 Langchain","description":"In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement.","language":"en","loc":{"lines":{"from":25,"to":52}}}}],["140",{"pageContent":"protocol that most components implement. This is a standard interface with a few\ndifferent methods, which make it easy to define custom chains as well as making\nit possible to invoke them in a standard way. The standard interface exposed\nincludes: * stream: stream back chunks of the response * invoke: call the chain\non an input * batch: call the chain on a list of inputs The type of the input\nvaries by component. For a prompt it is an object, for a retriever it is a\nsingle string, for a model either a single string, a list of chat messages, or a\nPromptValue. The output type also varies by component. For an LLM it is a\nstring, for a ChatModel it's a ChatMessage, for a prompt it's a PromptValue, and\nfor a retriever it's a list of documents. You can combine runnables (and\nrunnable-like objects such as functions and objects whose values are all\nfunctions) into sequences in two ways: * Call the .pipe instance method, which\ntakes another runnable-like as an argument * Use the","metadata":{"source":"https://js.langchain.com/docs/expression_language/interface","title":"Interface | 🦜️🔗 Langchain","description":"In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement.","language":"en","loc":{"lines":{"from":52,"to":70}}}}],["141",{"pageContent":"objects such as functions and objects whose values are all functions) into\nsequences in two ways: * Call the .pipe instance method, which takes another\nrunnable-like as an argument * Use the RunnableSequence.from([]) static method\nwith an array of runnable-likes, which will run in sequence when invoked See\nbelow for examples of how this looks. STREAM import { PromptTemplate } from\n\"langchain/prompts\"; import { ChatOpenAI } from \"langchain/chat_models/openai\";\nconst model = new ChatOpenAI({}); const promptTemplate =\nPromptTemplate.fromTemplate( \"Tell me a joke about {topic}\" ); const chain =\npromptTemplate.pipe(model); const stream = await chain.stream({ topic: \"bears\"\n}); // Each chunk has the same interface as a chat message for await (const\nchunk of stream) { console.log(chunk?.content); } /* Why don't bears wear shoes?\nBecause they have bear feet! */ API REFERENCE: * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *","metadata":{"source":"https://js.langchain.com/docs/expression_language/interface","title":"Interface | 🦜️🔗 Langchain","description":"In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement.","language":"en","loc":{"lines":{"from":70,"to":110}}}}],["142",{"pageContent":"don't bears wear shoes? Because they have bear feet! */ API REFERENCE: *\nPromptTemplate [/docs/api/prompts/classes/PromptTemplate] from langchain/prompts\n* ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai INVOKE import { PromptTemplate } from\n\"langchain/prompts\"; import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { RunnableSequence } from \"langchain/schema/runnable\"; const model = new\nChatOpenAI({}); const promptTemplate = PromptTemplate.fromTemplate( \"Tell me a\njoke about {topic}\" ); // You can also create a chain using an array of\nrunnables const chain = RunnableSequence.from([promptTemplate, model]); const\nresult = await chain.invoke({ topic: \"bears\" }); console.log(result); /*\nAIMessage { content: \"Why don't bears wear shoes?\\n\\nBecause they have bear\nfeet!\", } */ API REFERENCE: * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts * ChatOpenAI","metadata":{"source":"https://js.langchain.com/docs/expression_language/interface","title":"Interface | 🦜️🔗 Langchain","description":"In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement.","language":"en","loc":{"lines":{"from":110,"to":153}}}}],["143",{"pageContent":"content: \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", } */ API\nREFERENCE: * PromptTemplate [/docs/api/prompts/classes/PromptTemplate] from\nlangchain/prompts * ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI]\nfrom langchain/chat_models/openai * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable BATCH import { PromptTemplate } from\n\"langchain/prompts\"; import { ChatOpenAI } from \"langchain/chat_models/openai\";\nconst model = new ChatOpenAI({}); const promptTemplate =\nPromptTemplate.fromTemplate( \"Tell me a joke about {topic}\" ); const chain =\npromptTemplate.pipe(model); const result = await chain.batch([{ topic: \"bears\"\n}, { topic: \"cats\" }]); console.log(result); /* [ AIMessage { content: \"Why\ndon't bears wear shoes?\\n\\nBecause they have bear feet!\", }, AIMessage {\ncontent: \"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\" } ] */\nAPI","metadata":{"source":"https://js.langchain.com/docs/expression_language/interface","title":"Interface | 🦜️🔗 Langchain","description":"In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement.","language":"en","loc":{"lines":{"from":153,"to":196}}}}],["144",{"pageContent":"content: \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", },\nAIMessage { content: \"Why don't cats play poker in the wild?\\n\\nToo many\ncheetahs!\" } ] */ API REFERENCE: * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai You can also pass a batchOptions argument to the\ncall. There are options to set maximum concurrency and whether or not to return\nexceptions instead of throwing them (useful for gracefully handling failures!):\nimport { PromptTemplate } from \"langchain/prompts\"; import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; const model = new ChatOpenAI({ modelName:\n\"badmodel\", }); const promptTemplate = PromptTemplate.fromTemplate( \"Tell me a\njoke about {topic}\" ); const chain = promptTemplate.pipe(model); const result =\nawait chain.batch( [{ topic: \"bears\" }, { topic: \"cats\" }], {}, {","metadata":{"source":"https://js.langchain.com/docs/expression_language/interface","title":"Interface | 🦜️🔗 Langchain","description":"In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement.","language":"en","loc":{"lines":{"from":196,"to":230}}}}],["145",{"pageContent":"= PromptTemplate.fromTemplate( \"Tell me a joke about {topic}\" ); const chain =\npromptTemplate.pipe(model); const result = await chain.batch( [{ topic: \"bears\"\n}, { topic: \"cats\" }], {}, { returnExceptions: true, maxConcurrency: 1 } );\nconsole.log(result); /* [ NotFoundError: The model `badmodel` does not exist at\nFunction.generate\n(/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/error.ts:71:6) at\nOpenAI.makeStatusError\n(/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:381:13)\nat OpenAI.makeRequest\n(/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:442:15)\nat process.processTicksAndRejections (node:internal/process/task_queues:95:5) at\nasync\nfile:///Users/jacoblee/langchain/langchainjs/langchain/dist/chat_models/openai.js:514:29\nat RetryOperation._fn\n(/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {\nstatus: 404, NotFoundError: The model","metadata":{"source":"https://js.langchain.com/docs/expression_language/interface","title":"Interface | 🦜️🔗 Langchain","description":"In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement.","language":"en","loc":{"lines":{"from":230,"to":253}}}}],["146",{"pageContent":"at RetryOperation._fn\n(/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {\nstatus: 404, NotFoundError: The model `badmodel` does not exist at\nFunction.generate\n(/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/error.ts:71:6) at\nOpenAI.makeStatusError\n(/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:381:13)\nat OpenAI.makeRequest\n(/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:442:15)\nat process.processTicksAndRejections (node:internal/process/task_queues:95:5) at\nasync\nfile:///Users/jacoblee/langchain/langchainjs/langchain/dist/chat_models/openai.js:514:29\nat RetryOperation._fn\n(/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {\nstatus: 404, ] */ API REFERENCE: * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI]","metadata":{"source":"https://js.langchain.com/docs/expression_language/interface","title":"Interface | 🦜️🔗 Langchain","description":"In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement.","language":"en","loc":{"lines":{"from":253,"to":272}}}}],["147",{"pageContent":"{ status: 404, ] */ API REFERENCE: * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai Previous LangChain Expression Language (LCEL)\n[/docs/expression_language/] Next Modules [/docs/modules/] * Stream * Invoke *\nBatch Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/expression_language/interface","title":"Interface | 🦜️🔗 Langchain","description":"In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement.","language":"en","loc":{"lines":{"from":272,"to":307}}}}],["148",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/modules/","title":"Modules | 🦜️🔗 Langchain","description":"LangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["149",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules On this page MODULES LangChain\nprovides standard, extendable interfaces and external integrations for the\nfollowing modules, listed from least to most complex: MODEL I/O\n[/docs/modules/model_io/] Interface with language models DATA CONNECTION\n[/docs/modules/data_connection/] Interface with application-specific data CHAINS\n[/docs/modules/chains/] Construct sequences of calls AGENTS","metadata":{"source":"https://js.langchain.com/docs/modules/","title":"Modules | 🦜️🔗 Langchain","description":"LangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:","language":"en","loc":{"lines":{"from":25,"to":64}}}}],["150",{"pageContent":"with language models DATA CONNECTION [/docs/modules/data_connection/] Interface\nwith application-specific data CHAINS [/docs/modules/chains/] Construct\nsequences of calls AGENTS [/docs/modules/agents/] Let chains choose which tools\nto use given high-level directives MEMORY [/docs/modules/memory/] Persist\napplication state between runs of a chain CALLBACKS [/docs/modules/callbacks/]\nLog and stream intermediate steps of any chain Previous Interface\n[/docs/expression_language/interface] Next Model I/O [/docs/modules/model_io/]\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/","title":"Modules | 🦜️🔗 Langchain","description":"LangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:","language":"en","loc":{"lines":{"from":64,"to":105}}}}],["151",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Prompts [/docs/modules/model_io/prompts/] * Language\nmodels [/docs/modules/model_io/models/] * Output parsers\n[/docs/modules/model_io/output_parsers/] * Retrieval","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/","title":"Model I/O | 🦜️🔗 Langchain","description":"The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["152",{"pageContent":"* Prompts [/docs/modules/model_io/prompts/] * Language models\n[/docs/modules/model_io/models/] * Output parsers\n[/docs/modules/model_io/output_parsers/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * Memory\n[/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Model I/ O MODEL\nI/O The core element of any language model application is...the model. LangChain\ngives you the building blocks to interface with any language model. * Prompts\n[/docs/modules/model_io/prompts/]: Templatize, dynamically select, and","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/","title":"Model I/O | 🦜️🔗 Langchain","description":"The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.","language":"en","loc":{"lines":{"from":24,"to":52}}}}],["153",{"pageContent":"model application is...the model. LangChain gives you the building blocks to\ninterface with any language model. * Prompts [/docs/modules/model_io/prompts/]:\nTemplatize, dynamically select, and manage model inputs * Language models\n[/docs/modules/model_io/models/]: Make calls to language models through common\ninterfaces * Output parsers [/docs/modules/model_io/output_parsers/]: Extract\ninformation from model outputs model_io_diagram\n[/assets/images/model_io-1f23a36233d7731e93576d6885da2750.jpg] Previous Modules\n[/docs/modules/] Next Prompts [/docs/modules/model_io/prompts/] Community *\nDiscord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/","title":"Model I/O | 🦜️🔗 Langchain","description":"The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.","language":"en","loc":{"lines":{"from":52,"to":79}}}}],["154",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Prompts [/docs/modules/model_io/prompts/] * Prompt\ntemplates [/docs/modules/model_io/prompts/prompt_templates/] * Example selectors","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/prompts/","title":"Prompts | 🦜️🔗 Langchain","description":"The new way of programming models is through prompts.","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["155",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Prompts\n[/docs/modules/model_io/prompts/] * Prompt templates\n[/docs/modules/model_io/prompts/prompt_templates/] * Example selectors\n[/docs/modules/model_io/prompts/example_selectors/] * Prompt selectors\n[/docs/modules/model_io/prompts/prompt_selectors/] * Language models\n[/docs/modules/model_io/models/] * Output parsers\n[/docs/modules/model_io/output_parsers/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * Memory\n[/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / *","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/prompts/","title":"Prompts | 🦜️🔗 Langchain","description":"The new way of programming models is through prompts.","language":"en","loc":{"lines":{"from":23,"to":46}}}}],["156",{"pageContent":"[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Prompts PROMPTS The new way of programming models is\nthrough prompts. A prompt refers to the input to the model. This input is often\nconstructed from multiple components. LangChain provides several classes and\nfunctions to make constructing and working with prompts easy. * Prompt templates\n[/docs/modules/model_io/prompts/prompt_templates/]: Parametrize model inputs *\nExample selectors [/docs/modules/model_io/prompts/example_selectors/]:\nDynamically select examples to include in prompts Previous Model I/O\n[/docs/modules/model_io/] Next Prompt templates\n[/docs/modules/model_io/prompts/prompt_templates/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/prompts/","title":"Prompts | 🦜️🔗 Langchain","description":"The new way of programming models is through prompts.","language":"en","loc":{"lines":{"from":46,"to":77}}}}],["157",{"pageContent":"templates [/docs/modules/model_io/prompts/prompt_templates/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/prompts/","title":"Prompts | 🦜️🔗 Langchain","description":"The new way of programming models is through prompts.","language":"en","loc":{"lines":{"from":77,"to":91}}}}],["158",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Prompts [/docs/modules/model_io/prompts/] * Language\nmodels [/docs/modules/model_io/models/] * LLMs\n[/docs/modules/model_io/models/llms/] * Chat models","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/","title":"Language models | 🦜️🔗 Langchain","description":"LangChain provides interfaces and integrations for two types of models:","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["159",{"pageContent":"* Prompts [/docs/modules/model_io/prompts/] * Language models\n[/docs/modules/model_io/models/] * LLMs [/docs/modules/model_io/models/llms/] *\nChat models [/docs/modules/model_io/models/chat/] * Output parsers\n[/docs/modules/model_io/output_parsers/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * Memory\n[/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Language models On this page LANGUAGE MODELS\nLangChain provides interfaces and integrations for","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/","title":"Language models | 🦜️🔗 Langchain","description":"LangChain provides interfaces and integrations for two types of models:","language":"en","loc":{"lines":{"from":24,"to":54}}}}],["160",{"pageContent":"reference [/docs/api/] * / * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Language models On this page LANGUAGE MODELS\nLangChain provides interfaces and integrations for two types of models: * LLMs\n[/docs/modules/model_io/models/llms/]: Models that take a text string as input\nand return a text string * Chat models [/docs/modules/model_io/models/chat/]:\nModels that are backed by a language model but take a list of Chat Messages as\ninput and return a Chat Message LLMS VS CHAT MODELS LLMs and Chat Models are\nsubtly but importantly different. LLMs in LangChain refer to pure text\ncompletion models. The APIs they wrap take a string prompt as input and output a\nstring completion. OpenAI's GPT-3 is implemented as an LLM. Chat models are\noften backed by LLMs but tuned specifically for having conversations. And,\ncrucially, their provider APIs expose a different interface than pure text\ncompletion models. Instead of a single string, they take a list of chat","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/","title":"Language models | 🦜️🔗 Langchain","description":"LangChain provides interfaces and integrations for two types of models:","language":"en","loc":{"lines":{"from":54,"to":78}}}}],["161",{"pageContent":"tuned specifically for having conversations. And, crucially, their provider APIs\nexpose a different interface than pure text completion models. Instead of a\nsingle string, they take a list of chat messages as input. Usually these\nmessages are labeled with the speaker (usually one of \"System\", \"AI\", and\n\"Human\"). And they return a (\"AI\") chat message as output. GPT-4 and Anthropic's\nClaude are both implemented as Chat Models. To make it possible to swap LLMs and\nChat Models, both implement the Base Language Model interface. This exposes\ncommon methods \"predict\", which takes a string and returns a string, and\n\"predict messages\", which takes messages and returns a message. If you are using\na specific model it's recommended you use the methods specific to that model\nclass (i.e., \"predict\" for LLMs and \"predict messages\" for Chat Models), but if\nyou're creating an application that should work with different types of models\nthe shared interface can be helpful. Previous Prompt","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/","title":"Language models | 🦜️🔗 Langchain","description":"LangChain provides interfaces and integrations for two types of models:","language":"en","loc":{"lines":{"from":78,"to":90}}}}],["162",{"pageContent":"\"predict\" for LLMs and \"predict messages\" for Chat Models), but if you're\ncreating an application that should work with different types of models the\nshared interface can be helpful. Previous Prompt selectors\n[/docs/modules/model_io/prompts/prompt_selectors/] Next LLMs\n[/docs/modules/model_io/models/llms/] * LLMs vs Chat Models Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/","title":"Language models | 🦜️🔗 Langchain","description":"LangChain provides interfaces and integrations for two types of models:","language":"en","loc":{"lines":{"from":90,"to":114}}}}],["163",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Prompts [/docs/modules/model_io/prompts/] * Language\nmodels [/docs/modules/model_io/models/] * Output parsers\n[/docs/modules/model_io/output_parsers/] *","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["164",{"pageContent":"* Prompts [/docs/modules/model_io/prompts/] * Language models\n[/docs/modules/model_io/models/] * Output parsers\n[/docs/modules/model_io/output_parsers/] * How-to\n[/docs/modules/model_io/output_parsers/how_to/use_with_llm_chain] * Bytes output\nparser [/docs/modules/model_io/output_parsers/bytes] * Combining output parsers\n[/docs/modules/model_io/output_parsers/combining_output_parser] * List parser\n[/docs/modules/model_io/output_parsers/comma_separated] * Custom list parser\n[/docs/modules/model_io/output_parsers/custom_list_parser] * Auto-fixing parser\n[/docs/modules/model_io/output_parsers/output_fixing_parser] * String output\nparser [/docs/modules/model_io/output_parsers/string] * Structured output parser\n[/docs/modules/model_io/output_parsers/structured] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * Memory\n[/docs/modules/memory/] * Agents [/docs/modules/agents/] *","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":24,"to":39}}}}],["165",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Output parsers On this page OUTPUT PARSERS Language\nmodels output text. But many times you may want to get more structured\ninformation than just text back. This is where output parsers come in. Output\nparsers are classes that help structure language model responses. There are two\nmain methods an output parser must implement: * \"Get format instructions\": A\nmethod which","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":39,"to":70}}}}],["166",{"pageContent":"parsers come in. Output parsers are classes that help structure language model\nresponses. There are two main methods an output parser must implement: * \"Get\nformat instructions\": A method which returns a string containing instructions\nfor how the output of a language model should be formatted. * \"Parse\": A method\nwhich takes in a string (assumed to be the response from a language model) and\nparses it into some structure. And then one optional one: * \"Parse with prompt\":\nA method which takes in a string (assumed to be the response from a language\nmodel) and a prompt (assumed to the prompt that generated such a response) and\nparses it into some structure. The prompt is largely provided in the event the\nOutputParser wants to retry or fix the output in some way, and needs information\nfrom the prompt to do so. GET STARTED Below we go over one useful type of output\nparser, the StructuredOutputParser. STRUCTURED OUTPUT PARSER This output parser\ncan be used when you want","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":70,"to":93}}}}],["167",{"pageContent":"from the prompt to do so. GET STARTED Below we go over one useful type of output\nparser, the StructuredOutputParser. STRUCTURED OUTPUT PARSER This output parser\ncan be used when you want to return multiple fields. If you want complex schema\nreturned (i.e. a JSON object with arrays of strings), use the Zod Schema\ndetailed below. import { OpenAI } from \"langchain/llms/openai\"; import {\nPromptTemplate } from \"langchain/prompts\"; import { StructuredOutputParser }\nfrom \"langchain/output_parsers\"; import { RunnableSequence } from\n\"langchain/schema/runnable\"; const parser =\nStructuredOutputParser.fromNamesAndDescriptions({ answer: \"answer to the user's\nquestion\", source: \"source used to answer the user's question, should be a\nwebsite.\", }); const chain = RunnableSequence.from([\nPromptTemplate.fromTemplate( \"Answer the users question as best as\npossible.\\n{format_instructions}\\n{question}\" ), new OpenAI({ temperature: 0 }),","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":93,"to":120}}}}],["168",{"pageContent":"chain = RunnableSequence.from([ PromptTemplate.fromTemplate( \"Answer the users\nquestion as best as possible.\\n{format_instructions}\\n{question}\" ), new\nOpenAI({ temperature: 0 }), parser, ]);\nconsole.log(parser.getFormatInstructions()); /* Answer the users question as\nbest as possible. The output should be formatted as a JSON instance that\nconforms to the JSON schema below. As an example, for the schema {{\"properties\":\n{{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\",\n\"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}} the object {{\"foo\":\n[\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object\n{{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted. Here is the\noutput schema: ```\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer\nto the user's\nquestion\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources\nused to answer the question, should be","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":120,"to":139}}}}],["169",{"pageContent":"to the user's\nquestion\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources\nused to answer the question, should be\nwebsites.\"}},\"required\":[\"answer\",\"sources\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n``` What is the capital of France? */ const response = await chain.invoke({\nquestion: \"What is the capital of France?\", format_instructions:\nparser.getFormatInstructions(), }); console.log(response); // { answer: 'Paris',\nsource: 'https://en.wikipedia.org/wiki/Paris' } API REFERENCE: * OpenAI\n[/docs/api/llms_openai/classes/OpenAI] from langchain/llms/openai *\nPromptTemplate [/docs/api/prompts/classes/PromptTemplate] from langchain/prompts\n* StructuredOutputParser\n[/docs/api/output_parsers/classes/StructuredOutputParser] from\nlangchain/output_parsers * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable STRUCTURED OUTPUT PARSER WITH ZOD SCHEMA This output","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":139,"to":166}}}}],["170",{"pageContent":"from langchain/output_parsers * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable STRUCTURED OUTPUT PARSER WITH ZOD SCHEMA This output\nparser can be also be used when you want to define the output schema using Zod,\na TypeScript validation library. The Zod schema passed in needs be parseable\nfrom a JSON string, so eg. z.date() is not allowed. import { z } from \"zod\";\nimport { OpenAI } from \"langchain/llms/openai\"; import { PromptTemplate } from\n\"langchain/prompts\"; import { StructuredOutputParser } from\n\"langchain/output_parsers\"; import { RunnableSequence } from\n\"langchain/schema/runnable\"; // We can use zod to define a schema for the output\nusing the `fromZodSchema` method of `StructuredOutputParser`. const parser =\nStructuredOutputParser.fromZodSchema( z.object({ answer:\nz.string().describe(\"answer to the user's question\"), sources: z\n.array(z.string()) .describe(\"sources used to answer the question,","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":166,"to":187}}}}],["171",{"pageContent":"z.object({ answer: z.string().describe(\"answer to the user's question\"),\nsources: z .array(z.string()) .describe(\"sources used to answer the question,\nshould be websites.\"), }) ); const chain = RunnableSequence.from([\nPromptTemplate.fromTemplate( \"Answer the users question as best as\npossible.\\n{format_instructions}\\n{question}\" ), new OpenAI({ temperature: 0 }),\nparser, ]); console.log(parser.getFormatInstructions()); /* Answer the users\nquestion as best as possible. The output should be formatted as a JSON instance\nthat conforms to the JSON schema below. As an example, for the schema\n{{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\",\n\"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}} the\nobject {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The\nobject {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted. Here\nis the output","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":187,"to":212}}}}],["172",{"pageContent":"[\"foo\"]}}}} the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of\nthe schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not\nwell-formatted. Here is the output schema: ```\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer\nto the user's\nquestion\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources\nused to answer the question, should be\nwebsites.\"}},\"required\":[\"answer\",\"sources\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n``` What is the capital of France? */ const response = await chain.invoke({\nquestion: \"What is the capital of France?\", format_instructions:\nparser.getFormatInstructions(), }); console.log(response); /* { answer: 'Paris',\nsources: [ 'https://en.wikipedia.org/wiki/Paris' ] } */ API REFERENCE: * OpenAI\n[/docs/api/llms_openai/classes/OpenAI] from langchain/llms/openai *\nPromptTemplate [/docs/api/prompts/classes/PromptTemplate] from","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":212,"to":239}}}}],["173",{"pageContent":"] } */ API REFERENCE: * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nStructuredOutputParser [/docs/api/output_parsers/classes/StructuredOutputParser]\nfrom langchain/output_parsers * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable Previous PromptLayer OpenAI\n[/docs/modules/model_io/models/chat/integrations/prompt_layer_openai] Next Use\nwith LLMChains [/docs/modules/model_io/output_parsers/how_to/use_with_llm_chain]\n* Get started Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/","title":"Output parsers | 🦜️🔗 Langchain","description":"Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.","language":"en","loc":{"lines":{"from":239,"to":272}}}}],["174",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * Document\ntransformers","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/","title":"Retrieval | 🦜️🔗 Langchain","description":"Many LLM applications require user-specific data that is not part of the model's training set.","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["175",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * Document transformers\n[/docs/modules/data_connection/document_transformers/] * Text embedding models\n[/docs/modules/data_connection/text_embedding/] * Vector stores\n[/docs/modules/data_connection/vectorstores/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * Experimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Chains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/","title":"Retrieval | 🦜️🔗 Langchain","description":"Many LLM applications require user-specific data that is not part of the model's training set.","language":"en","loc":{"lines":{"from":23,"to":41}}}}],["176",{"pageContent":"* Modules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem]\n* Additional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\nRETRIEVAL Many LLM applications require user-specific data that is not part of\nthe model's training set. The primary way of accomplishing this is through\nRetrieval Augmented Generation (RAG). In this process, external data is\nretrieved and then passed to the LLM when doing the generation step. LangChain\nprovides all the building blocks for RAG applications - from simple to complex.\nThis section of the documentation covers everything related to the retrieval\nstep - e.g. the fetching of the data. Although this sounds simple, it can be\nsubtly complex. This encompasses several key","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/","title":"Retrieval | 🦜️🔗 Langchain","description":"Many LLM applications require user-specific data that is not part of the model's training set.","language":"en","loc":{"lines":{"from":41,"to":64}}}}],["177",{"pageContent":"This section of the documentation covers everything related to the retrieval\nstep - e.g. the fetching of the data. Although this sounds simple, it can be\nsubtly complex. This encompasses several key modules. data_connection_diagram\n[/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg] Document\nloaders [/docs/modules/data_connection/document_loaders/] Load documents from\nmany different sources. LangChain provides many different document loaders as\nwell as integrations with other major providers in the space, such as\nUnstructured. We provide integrations to load all types of documents (html, PDF,\ncode) from all types of locations (private s3 buckets, public websites).\nDocument transformers [/docs/modules/data_connection/document_transformers/] A\nkey part of retrieval is fetching only the relevant parts of documents. This\ninvolves several transformation steps in order to best prepare the documents for\nretrieval. One of the primary ones here is splitting (or chunking)","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/","title":"Retrieval | 🦜️🔗 Langchain","description":"Many LLM applications require user-specific data that is not part of the model's training set.","language":"en","loc":{"lines":{"from":64,"to":79}}}}],["178",{"pageContent":"fetching only the relevant parts of documents. This involves several\ntransformation steps in order to best prepare the documents for retrieval. One\nof the primary ones here is splitting (or chunking) a large document into\nsmaller chunks. LangChain provides several different algorithms for doing this,\nas well as logic optimized for specific document types (code, markdown, etc).\nText embedding models [/docs/modules/data_connection/text_embedding/] Another\nkey part of retrieval has become creating embeddings for documents. Embeddings\ncapture the semantic meaning of text, allowing you to quickly and efficiently\nfind other pieces of text that are similar. LangChain provides integrations with\ndifferent embedding providers and methods, from open-source to proprietary API,\nallowing you to choose the one best suited for your needs. LangChain exposes a\nstandard interface, allowing you to easily swap between models. Vector stores\n[/docs/modules/data_connection/vectorstores/] With the rise of","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/","title":"Retrieval | 🦜️🔗 Langchain","description":"Many LLM applications require user-specific data that is not part of the model's training set.","language":"en","loc":{"lines":{"from":79,"to":93}}}}],["179",{"pageContent":"the one best suited for your needs. LangChain exposes a standard interface,\nallowing you to easily swap between models. Vector stores\n[/docs/modules/data_connection/vectorstores/] With the rise of embeddings, there\nhas emerged a need for databases to support efficient storage and searching of\nthese embeddings. LangChain provides integrations with many different\nvectorstores, from open-source local ones to cloud-hosted proprietary ones,\nallowing you choose the one best suited for your needs. LangChain exposes a\nstandard interface, allowing you to easily swap between vector stores.\nRetrievers [/docs/modules/data_connection/retrievers/] Once the data is in the\ndatabase, you still need to retrieve it. LangChain supports many different\nretrieval algorithms and is one of the places where we add the most value. We\nsupport basic methods that are easy to get started - namely simple semantic\nsearch. However, we have also added a collection of algorithms on top of this to\nincrease","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/","title":"Retrieval | 🦜️🔗 Langchain","description":"Many LLM applications require user-specific data that is not part of the model's training set.","language":"en","loc":{"lines":{"from":93,"to":107}}}}],["180",{"pageContent":"where we add the most value. We support basic methods that are easy to get\nstarted - namely simple semantic search. However, we have also added a\ncollection of algorithms on top of this to increase performance. These include:\n* Parent Document Retriever\n[/docs/modules/data_connection/retrievers/how_to/parent-document-retriever]:\nThis allows you to create multiple embeddings per parent document, allowing you\nto look up smaller chunks but return larger context. * Self Query Retriever\n[/docs/modules/data_connection/retrievers/how_to/self_query]: User questions\noften contain reference to something that isn't just semantic, but rather\nexpresses some logic that can best be represented as a metadata filter.\nSelf-query allows you to parse out the semantic part of a query from other\nmetadata filters present in the query * And more! Previous Structured output\nparser [/docs/modules/model_io/output_parsers/structured] Next Document","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/","title":"Retrieval | 🦜️🔗 Langchain","description":"Many LLM applications require user-specific data that is not part of the model's training set.","language":"en","loc":{"lines":{"from":107,"to":121}}}}],["181",{"pageContent":"out the semantic part of a query from other metadata filters present in the\nquery * And more! Previous Structured output parser\n[/docs/modules/model_io/output_parsers/structured] Next Document loaders\n[/docs/modules/data_connection/document_loaders/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/","title":"Retrieval | 🦜️🔗 Langchain","description":"Many LLM applications require user-specific data that is not part of the model's training set.","language":"en","loc":{"lines":{"from":121,"to":142}}}}],["182",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * How-to","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_loaders/","title":"Document loaders | 🦜️🔗 Langchain","description":"Use document loaders to load data from a source as Document's. A Document is a piece of text","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["183",{"pageContent":"[/docs/modules/] * Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * How-to\n[/docs/modules/data_connection/document_loaders/how_to/creating_documents] *\nIntegrations\n[/docs/modules/data_connection/document_loaders/integrations/file_loaders/] *\nDocument transformers [/docs/modules/data_connection/document_transformers/] *\nText embedding models [/docs/modules/data_connection/text_embedding/] * Vector\nstores [/docs/modules/data_connection/vectorstores/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * Experimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Chains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_loaders/","title":"Document loaders | 🦜️🔗 Langchain","description":"Use document loaders to load data from a source as Document's. A Document is a piece of text","language":"en","loc":{"lines":{"from":23,"to":40}}}}],["184",{"pageContent":"* Memory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders On this page DOCUMENT\nLOADERS Use document loaders to load data from a source as Document's. A\nDocument is a piece of text and associated metadata. For example, there are\ndocument loaders for loading a simple .txt file, for loading the text contents\nof any web page, or even for loading a transcript of a YouTube video. Document\nloaders expose a \"load\" method for loading data as documents from a configured\nsource. They optionally","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_loaders/","title":"Document loaders | 🦜️🔗 Langchain","description":"Use document loaders to load data from a source as Document's. A Document is a piece of text","language":"en","loc":{"lines":{"from":40,"to":67}}}}],["185",{"pageContent":"text contents of any web page, or even for loading a transcript of a YouTube\nvideo. Document loaders expose a \"load\" method for loading data as documents\nfrom a configured source. They optionally implement a \"lazy load\" as well for\nlazily loading data into memory. GET STARTED The simplest loader reads in a file\nas text and places it all into one Document. import { TextLoader } from\n\"langchain/document_loaders/fs/text\"; const loader = new\nTextLoader(\"src/document_loaders/example_data/example.txt\"); const docs = await\nloader.load(); API REFERENCE: * TextLoader\n[/docs/api/document_loaders_fs_text/classes/TextLoader] from\nlangchain/document_loaders/fs/text Previous Retrieval\n[/docs/modules/data_connection/] Next Creating documents\n[/docs/modules/data_connection/document_loaders/how_to/creating_documents] * Get\nstarted Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain]","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_loaders/","title":"Document loaders | 🦜️🔗 Langchain","description":"Use document loaders to load data from a source as Document's. A Document is a piece of text","language":"en","loc":{"lines":{"from":67,"to":103}}}}],["186",{"pageContent":"* Get started Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_loaders/","title":"Document loaders | 🦜️🔗 Langchain","description":"Use document loaders to load data from a source as Document's. A Document is a piece of text","language":"en","loc":{"lines":{"from":103,"to":117}}}}],["187",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * Document\ntransformers","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_transformers/","title":"Document transformers | 🦜️🔗 Langchain","description":"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["188",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * Document transformers\n[/docs/modules/data_connection/document_transformers/] * Integrations\n[/docs/modules/data_connection/document_transformers/integrations/html-to-text]\n* Text splitters\n[/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter]\n* Text embedding models [/docs/modules/data_connection/text_embedding/] * Vector\nstores [/docs/modules/data_connection/vectorstores/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * Experimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Chains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_transformers/","title":"Document transformers | 🦜️🔗 Langchain","description":"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example","language":"en","loc":{"lines":{"from":23,"to":39}}}}],["189",{"pageContent":"* Memory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\n[/docs/modules/data_connection/] * Document transformers On this page DOCUMENT\nTRANSFORMERS Once you've loaded documents, you'll often want to transform them\nto better suit your application. The simplest example is you may want to split a\nlong document into smaller chunks that can fit into your model's context window.\nLangChain has a number of built-in document transformers that make it easy to\nsplit, combine, filter, and otherwise manipulate documents. TEXT SPLITTERS When\nyou","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_transformers/","title":"Document transformers | 🦜️🔗 Langchain","description":"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example","language":"en","loc":{"lines":{"from":39,"to":69}}}}],["190",{"pageContent":"into your model's context window. LangChain has a number of built-in document\ntransformers that make it easy to split, combine, filter, and otherwise\nmanipulate documents. TEXT SPLITTERS When you want to deal with long pieces of\ntext, it is necessary to split up that text into chunks. As simple as this\nsounds, there is a lot of potential complexity here. Ideally, you want to keep\nthe semantically related pieces of text together. What \"semantically related\"\nmeans could depend on the type of text. This notebook showcases several ways to\ndo that. At a high level, text splitters work as following: 1. Split the text up\ninto small, semantically meaningful chunks (often sentences). 2. Start combining\nthese small chunks into a larger chunk until you reach a certain size (as\nmeasured by some function). 3. Once you reach that size, make that chunk its own\npiece of text and then start creating a new chunk of text with some overlap (to\nkeep context between chunks). That means there","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_transformers/","title":"Document transformers | 🦜️🔗 Langchain","description":"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example","language":"en","loc":{"lines":{"from":69,"to":86}}}}],["191",{"pageContent":"function). 3. Once you reach that size, make that chunk its own piece of text\nand then start creating a new chunk of text with some overlap (to keep context\nbetween chunks). That means there are two different axes along which you can\ncustomize your text splitter: 1. How the text is split 2. How the chunk size is\nmeasured GET STARTED WITH TEXT SPLITTERS The recommended TextSplitter is the\nRecursiveCharacterTextSplitter. This will split documents recursively by\ndifferent characters - starting with \"\\n\\n\", then \"\\n\", then \" \". This is nice\nbecause it will try to keep all the semantically relevant content in the same\nplace for as long as possible. Important parameters to know here are chunkSize\nand chunkOverlap. chunkSize controls the max size (in terms of number of\ncharacters) of the final documents. chunkOverlap specifies how much overlap\nthere should be between chunks. This is often helpful to make sure that the text\nisn't split weirdly. In the example below we set these","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_transformers/","title":"Document transformers | 🦜️🔗 Langchain","description":"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example","language":"en","loc":{"lines":{"from":86,"to":104}}}}],["192",{"pageContent":"of the final documents. chunkOverlap specifies how much overlap there should be\nbetween chunks. This is often helpful to make sure that the text isn't split\nweirdly. In the example below we set these values to be small (for illustration\npurposes), but in practice they default to 1000 and 200 respectively. import {\nRecursiveCharacterTextSplitter } from \"langchain/text_splitter\"; const text =\n`Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f. This is a weird text\nto write, but gotta test the splittingggg some how.\\n\\n Bye!\\n\\n-H.`; const\nsplitter = new RecursiveCharacterTextSplitter({ chunkSize: 10, chunkOverlap: 1,\n}); const output = await splitter.createDocuments([text]); You'll note that in\nthe above example we are splitting a raw text string and getting back a list of\ndocuments. We can also split documents directly. import { Document } from\n\"langchain/document\"; import { RecursiveCharacterTextSplitter } from\n\"langchain/text_splitter\"; const text = `Hi.\\n\\nI'm","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_transformers/","title":"Document transformers | 🦜️🔗 Langchain","description":"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example","language":"en","loc":{"lines":{"from":104,"to":129}}}}],["193",{"pageContent":"of documents. We can also split documents directly. import { Document } from\n\"langchain/document\"; import { RecursiveCharacterTextSplitter } from\n\"langchain/text_splitter\"; const text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are?\nYou?\\nOkay then f f f f. This is a weird text to write, but gotta test the\nsplittingggg some how.\\n\\n Bye!\\n\\n-H.`; const splitter = new\nRecursiveCharacterTextSplitter({ chunkSize: 10, chunkOverlap: 1, }); const\ndocOutput = await splitter.splitDocuments([ new Document({ pageContent: text }),\n]); Previous YouTube transcripts\n[/docs/modules/data_connection/document_loaders/integrations/web_loaders/youtube]\nNext html-to-text\n[/docs/modules/data_connection/document_transformers/integrations/html-to-text]\n* Text splitters * Get started with text splitters Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_transformers/","title":"Document transformers | 🦜️🔗 Langchain","description":"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example","language":"en","loc":{"lines":{"from":129,"to":164}}}}],["194",{"pageContent":"Get started with text splitters Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/document_transformers/","title":"Document transformers | 🦜️🔗 Langchain","description":"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example","language":"en","loc":{"lines":{"from":164,"to":178}}}}],["195",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * Document\ntransformers","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["196",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * Document transformers\n[/docs/modules/data_connection/document_transformers/] * Text embedding models\n[/docs/modules/data_connection/text_embedding/] * How-to\n[/docs/modules/data_connection/text_embedding/how_to/api_errors] * Integrations\n[/docs/modules/data_connection/text_embedding/integrations/azure_openai] *\nVector stores [/docs/modules/data_connection/vectorstores/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * Experimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Chains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":23,"to":40}}}}],["197",{"pageContent":"* Agents [/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] *\nModules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\n[/docs/modules/data_connection/] * Text embedding models On this page TEXT\nEMBEDDING MODELS The Embeddings class is a class designed for interfacing with\ntext embedding models. There are lots of embedding model providers (OpenAI,\nCohere, Hugging Face, etc) - this class is designed to provide a standard\ninterface for all of them. Embeddings create a vector representation of a piece\nof text. This is useful because it means we can think about text in the vector\nspace, and do things like semantic search where we look for","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":40,"to":66}}}}],["198",{"pageContent":"them. Embeddings create a vector representation of a piece of text. This is\nuseful because it means we can think about text in the vector space, and do\nthings like semantic search where we look for pieces of text that are most\nsimilar in the vector space. The base Embeddings class in LangChain exposes two\nmethods: one for embedding documents and one for embedding a query. The former\ntakes as input multiple texts, while the latter takes a single text. The reason\nfor having these as two separate methods is that some embedding providers have\ndifferent embedding methods for documents (to be searched over) vs queries (the\nsearch query itself). GET STARTED Embeddings can be used to create a numerical\nrepresentation of textual data. This numerical representation is useful because\nit can be used to find similar documents. Below is an example of how to use the\nOpenAI embeddings. Embeddings occasionally have different embedding methods for\nqueries versus documents, so the embedding class","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":66,"to":83}}}}],["199",{"pageContent":"used to find similar documents. Below is an example of how to use the OpenAI\nembeddings. Embeddings occasionally have different embedding methods for queries\nversus documents, so the embedding class exposes a embedQuery and embedDocuments\nmethod. import { OpenAIEmbeddings } from \"langchain/embeddings/openai\"; /*\nCreate instance */ const embeddings = new OpenAIEmbeddings(); /* Embed queries\n*/ const res = await embeddings.embedQuery(\"Hello world\"); /* [ -0.004845875,\n0.004899438, -0.016358767, -0.024475135, -0.017341806, 0.012571548,\n-0.019156644, 0.009036391, -0.010227379, -0.026945334, 0.022861943, 0.010321903,\n-0.023479493, -0.0066544134, 0.007977734, 0.0026371893, 0.025206111,\n-0.012048521, 0.012943339, 0.013094575, -0.010580265, -0.003509951, 0.004070787,\n0.008639394, -0.020631202, -0.0019203906, 0.012161949, -0.019194454,\n0.030373365, -0.031028723, 0.0036170771, -0.007813894, -0.0060778237,\n-0.017820721, 0.0048647798,","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":83,"to":103}}}}],["200",{"pageContent":"0.004070787, 0.008639394, -0.020631202, -0.0019203906, 0.012161949,\n-0.019194454, 0.030373365, -0.031028723, 0.0036170771, -0.007813894,\n-0.0060778237, -0.017820721, 0.0048647798, -0.015640393, 0.001373733,\n-0.015552171, 0.019534737, -0.016169721, 0.007316074, 0.008273906, 0.011418369,\n-0.01390117, -0.033347685, 0.011248227, 0.0042503807, -0.012792102,\n-0.0014595914, 0.028356876, 0.025407761, 0.00076445413, -0.016308354,\n0.017455231, -0.016396577, 0.008557475, -0.03312083, 0.031104341, 0.032389853,\n-0.02132437, 0.003324056, 0.0055610985, -0.0078012915, 0.006090427,\n0.0062038545, 0.0169133, 0.0036391325, 0.0076815626, -0.018841568, 0.026037913,\n0.024550753, 0.0055264398, -0.0015824712, -0.0047765584, 0.018425668,\n0.0030656934, -0.0113742575, -0.0020322427, 0.005069579, 0.0022701253,\n0.036095154, -0.027449455, -0.008475555, 0.015388331, 0.018917186, 0.0018999106,\n-0.003349262,","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":103,"to":116}}}}],["201",{"pageContent":"0.018425668, 0.0030656934, -0.0113742575, -0.0020322427, 0.005069579,\n0.0022701253, 0.036095154, -0.027449455, -0.008475555, 0.015388331, 0.018917186,\n0.0018999106, -0.003349262, 0.020895867, -0.014480911, -0.025042271,\n0.012546342, 0.013850759, 0.0069253794, 0.008588983, -0.015199285,\n-0.0029585673, -0.008759124, 0.016749462, 0.004111747, -0.04804285, ... 1436\nmore items ] */ /* Embed documents */ const documentRes = await\nembeddings.embedDocuments([\"Hello world\", \"Bye bye\"]); /* [ [ -0.0047852774,\n0.0048640342, -0.01645707, -0.024395779, -0.017263541, 0.012512918,\n-0.019191515, 0.009053908, -0.010213212, -0.026890801, 0.022883644, 0.010251015,\n-0.023589306, -0.006584088, 0.007989113, 0.002720268, 0.025088841, -0.012153786,\n0.012928754, 0.013054766, -0.010395928, -0.0035566676, 0.0040008575,\n0.008600268, -0.020678446, -0.0019106456, 0.012178987, -0.019241918,\n0.030444318,","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":116,"to":136}}}}],["202",{"pageContent":"-0.012153786, 0.012928754, 0.013054766, -0.010395928, -0.0035566676,\n0.0040008575, 0.008600268, -0.020678446, -0.0019106456, 0.012178987,\n-0.019241918, 0.030444318, -0.03102397, 0.0035692686, -0.007749692, -0.00604854,\n-0.01781799, 0.004860884, -0.015612794, 0.0014097509, -0.015637996, 0.019443536,\n-0.01612944, 0.0072960514, 0.008316742, 0.011548932, -0.013987249, -0.03336778,\n0.011341013, 0.00425603, -0.0126578305, -0.0013861238, 0.028302127, 0.025466874,\n0.0007029065, -0.016318457, 0.017427357, -0.016394064, 0.008499459,\n-0.033241767, 0.031200387, 0.03238489, -0.0212833, 0.0032416396, 0.005443686,\n-0.007749692, 0.0060201874, 0.006281661, 0.016923312, 0.003528315, 0.0076740854,\n-0.01881348, 0.026109532, 0.024660403, 0.005472039, -0.0016712243,\n-0.0048136297, 0.018397642, 0.003011669, -0.011385117, -0.0020193304,\n0.005138109, 0.0022335495,","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":136,"to":148}}}}],["203",{"pageContent":"-0.01881348, 0.026109532, 0.024660403, 0.005472039, -0.0016712243,\n-0.0048136297, 0.018397642, 0.003011669, -0.011385117, -0.0020193304,\n0.005138109, 0.0022335495, 0.03603922, -0.027495656, -0.008575066, 0.015436378,\n0.018851284, 0.0018019609, -0.0034338066, 0.02094307, -0.014503895,\n-0.024950229, 0.012632628, 0.013735226, 0.0069936244, 0.008575066, -0.015196957,\n-0.0030541976, -0.008745181, 0.016746895, 0.0040481114, -0.048010286, ... 1436\nmore items ], [ -0.009446913, -0.013253193, 0.013174579, 0.0057552797,\n-0.038993083, 0.0077763423, -0.0260478, -0.0114384955, -0.0022683728,\n-0.016509168, 0.041797023, 0.01787183, 0.00552271, -0.0049789557, 0.018146982,\n-0.01542166, 0.033752076, 0.006112323, 0.023872782, -0.016535373, -0.006623321,\n0.016116094, -0.0061090477, -0.0044155475, -0.016627092, -0.022077737,\n-0.0009286407, -0.02156674, 0.011890532,","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":148,"to":163}}}}],["204",{"pageContent":"0.006112323, 0.023872782, -0.016535373, -0.006623321, 0.016116094,\n-0.0061090477, -0.0044155475, -0.016627092, -0.022077737, -0.0009286407,\n-0.02156674, 0.011890532, -0.026283644, 0.02630985, 0.011942943, -0.026126415,\n-0.018264906, -0.014045896, -0.024187243, -0.019037955, -0.005037917,\n0.020780588, -0.0049527506, 0.002399398, 0.020767486, 0.0080908025,\n-0.019666875, -0.027934562, 0.017688395, 0.015225122, 0.0046186363,\n-0.0045007137, 0.024265857, 0.03244183, 0.0038848957, -0.03244183, -0.018893827,\n-0.0018065092, 0.023440398, -0.021763276, 0.015120302, -0.01568371,\n-0.010861984, 0.011739853, -0.024501702, -0.005214801, 0.022955606, 0.001315165,\n-0.00492327, 0.0020358032, -0.003468891, -0.031079166, 0.0055259857,\n0.0028547104, 0.012087069, 0.007992534, -0.0076256637, 0.008110457, 0.002998838,\n-0.024265857, 0.006977089, -0.015185814, -0.0069115767,","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":163,"to":175}}}}],["205",{"pageContent":"-0.031079166, 0.0055259857, 0.0028547104, 0.012087069, 0.007992534,\n-0.0076256637, 0.008110457, 0.002998838, -0.024265857, 0.006977089,\n-0.015185814, -0.0069115767, 0.006466091, -0.029428247, -0.036241557,\n0.036713246, 0.032284595, -0.0021144184, -0.014255536, 0.011228855,\n-0.027227025, -0.021619149, 0.00038242966, 0.02245771, -0.0014748519,\n0.01573612, 0.0041010873, 0.006256451, -0.007992534, 0.038547598, 0.024658933,\n-0.012958387, ... 1436 more items ] ] */ Previous TokenTextSplitter\n[/docs/modules/data_connection/document_transformers/text_splitters/token] Next\nDealing with API errors\n[/docs/modules/data_connection/text_embedding/how_to/api_errors] * Get started\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":175,"to":206}}}}],["206",{"pageContent":"* Twitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/text_embedding/","title":"Text embedding models | 🦜️🔗 Langchain","description":"The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.","language":"en","loc":{"lines":{"from":206,"to":216}}}}],["207",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * Document\ntransformers","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["208",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * Document transformers\n[/docs/modules/data_connection/document_transformers/] * Text embedding models\n[/docs/modules/data_connection/text_embedding/] * Vector stores\n[/docs/modules/data_connection/vectorstores/] * Integrations\n[/docs/modules/data_connection/vectorstores/integrations/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * Experimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Chains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":23,"to":42}}}}],["209",{"pageContent":"* Modules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem]\n* Additional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\n[/docs/modules/data_connection/] * Vector stores On this page VECTOR STORES One\nof the most common ways to store and search over unstructured data is to embed\nit and store the resulting embedding vectors, and then at query time to embed\nthe unstructured query and retrieve the embedding vectors that are 'most\nsimilar' to the embedded query. A vector store takes care of storing embedded\ndata and performing vector search for you. GET STARTED This walkthrough\nshowcases basic functionality related to VectorStores. A key part of working\nwith vector stores is creating the vector to put in them,","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":42,"to":70}}}}],["210",{"pageContent":"vector search for you. GET STARTED This walkthrough showcases basic\nfunctionality related to VectorStores. A key part of working with vector stores\nis creating the vector to put in them, which is usually created via embeddings.\nTherefore, it is recommended that you familiarize yourself with the text\nembedding model [/docs/modules/data_connection/text_embedding/] interfaces\nbefore diving into this. This walkthrough uses a basic, unoptimized\nimplementation called MemoryVectorStore that stores embeddings in-memory and\ndoes an exact, linear search for the most similar embeddings. USAGE CREATE A NEW\nINDEX FROM TEXTS import { MemoryVectorStore } from\n\"langchain/vectorstores/memory\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; const vectorStore = await\nMemoryVectorStore.fromTexts( [\"Hello world\", \"Bye bye\", \"hello nice world\"], [{\nid: 2 }, { id: 1 }, { id: 3 }], new OpenAIEmbeddings() ); const resultOne =\nawait vectorStore.similaritySearch(\"hello world\",","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":70,"to":97}}}}],["211",{"pageContent":"[\"Hello world\", \"Bye bye\", \"hello nice world\"], [{ id: 2 }, { id: 1 }, { id: 3\n}], new OpenAIEmbeddings() ); const resultOne = await\nvectorStore.similaritySearch(\"hello world\", 1); console.log(resultOne); /* [\nDocument { pageContent: \"Hello world\", metadata: { id: 2 } } ] */ API REFERENCE:\n* MemoryVectorStore [/docs/api/vectorstores_memory/classes/MemoryVectorStore]\nfrom langchain/vectorstores/memory * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai CREATE A NEW INDEX FROM A LOADER import {\nMemoryVectorStore } from \"langchain/vectorstores/memory\"; import {\nOpenAIEmbeddings } from \"langchain/embeddings/openai\"; import { TextLoader }\nfrom \"langchain/document_loaders/fs/text\"; // Create docs with a loader const\nloader = new TextLoader(\"src/document_loaders/example_data/example.txt\"); const\ndocs = await loader.load(); // Load the docs into the vector store const\nvectorStore = await","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":97,"to":134}}}}],["212",{"pageContent":"docs with a loader const loader = new\nTextLoader(\"src/document_loaders/example_data/example.txt\"); const docs = await\nloader.load(); // Load the docs into the vector store const vectorStore = await\nMemoryVectorStore.fromDocuments( docs, new OpenAIEmbeddings() ); // Search for\nthe most similar document const resultOne = await\nvectorStore.similaritySearch(\"hello world\", 1); console.log(resultOne); /* [\nDocument { pageContent: \"Hello world\", metadata: { id: 2 } } ] */ API REFERENCE:\n* MemoryVectorStore [/docs/api/vectorstores_memory/classes/MemoryVectorStore]\nfrom langchain/vectorstores/memory * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * TextLoader\n[/docs/api/document_loaders_fs_text/classes/TextLoader] from\nlangchain/document_loaders/fs/text Here is the current base interface all vector\nstores share: interface VectorStore { /** * Add more documents to an existing\nVectorStore.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":134,"to":171}}}}],["213",{"pageContent":"from langchain/document_loaders/fs/text Here is the current base interface all\nvector stores share: interface VectorStore { /** * Add more documents to an\nexisting VectorStore. * Some providers support additional parameters, e.g. to\nassociate custom ids * with added documents or to change the batch size of bulk\ninserts. * Returns an array of ids for the documents or nothing. */\naddDocuments( documents: Document[], options?: Record ): Promise; /** * Search\nfor the most similar documents to a query */ similaritySearch( query: string,\nk?: number, filter?: object | undefined ): Promise; /** * Search for the most\nsimilar documents to a query, * and return their similarity score */\nsimilaritySearchWithScore( query: string, k = 4, filter: object | undefined =\nundefined ): Promise<[object, number][]>; /** * Turn a VectorStore into a\nRetriever */","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":171,"to":208}}}}],["214",{"pageContent":"*/ similaritySearchWithScore( query: string, k = 4, filter: object | undefined =\nundefined ): Promise<[object, number][]>; /** * Turn a VectorStore into a\nRetriever */ asRetriever(k?: number): BaseRetriever; /** * Delete embedded\ndocuments from the vector store matching the passed in parameter. * Not\nsupported by every provider. */ delete(params?: Record): Promise; /** *\nAdvanced: Add more documents to an existing VectorStore, * when you already have\ntheir embeddings */ addVectors( vectors: number[][], documents: Document[],\noptions?: Record ): Promise; /** * Advanced: Search for the most similar\ndocuments to a query, * when you already have the embedding of the query */\nsimilaritySearchVectorWithScore( query: number[], k: number, filter?: object ):\nPromise<[Document, number][]>; } You can create a vector store from a list of\nDocuments","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":208,"to":250}}}}],["215",{"pageContent":"query */ similaritySearchVectorWithScore( query: number[], k: number, filter?:\nobject ): Promise<[Document, number][]>; } You can create a vector store from a\nlist of Documents [/docs/api/document/classes/Document], or from a list of texts\nand their corresponding metadata. You can also create a vector store from an\nexisting index, the signature of this method depends on the vector store you're\nusing, check the documentation of the vector store you're interested in.\nabstract class BaseVectorStore implements VectorStore { static fromTexts( texts:\nstring[], metadatas: object[] | object, embeddings: Embeddings, dbConfig: Record\n): Promise; static fromDocuments( docs: Document[], embeddings: Embeddings,\ndbConfig: Record ): Promise; } WHICH ONE TO PICK? Here's a quick guide to help\nyou pick the right vector store for your use case: * If you're after something\nthat can","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":250,"to":289}}}}],["216",{"pageContent":"Record ): Promise; } WHICH ONE TO PICK? Here's a quick guide to help you pick\nthe right vector store for your use case: * If you're after something that can\njust run inside your Node.js application, in-memory, without any other servers\nto stand up, then go for HNSWLib\n[/docs/modules/data_connection/vectorstores/integrations/hnswlib], Faiss\n[/docs/modules/data_connection/vectorstores/integrations/faiss], or LanceDB\n[/docs/modules/data_connection/vectorstores/integrations/lancedb] * If you're\nlooking for something that can run in-memory in browser-like environments, then\ngo for MemoryVectorStore\n[/docs/modules/data_connection/vectorstores/integrations/memory] * If you come\nfrom Python and you were looking for something similar to FAISS, try HNSWLib\n[/docs/modules/data_connection/vectorstores/integrations/hnswlib] or Faiss\n[/docs/modules/data_connection/vectorstores/integrations/faiss] * If you're\nlooking for an open-source","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":289,"to":310}}}}],["217",{"pageContent":"try HNSWLib [/docs/modules/data_connection/vectorstores/integrations/hnswlib] or\nFaiss [/docs/modules/data_connection/vectorstores/integrations/faiss] * If\nyou're looking for an open-source full-featured vector database that you can run\nlocally in a docker container, then go for Chroma\n[/docs/modules/data_connection/vectorstores/integrations/chroma] * If you're\nlooking for an open-source vector database that offers low-latency, local\nembedding of documents and supports apps on the edge, then go for Zep\n[/docs/modules/data_connection/vectorstores/integrations/zep] * If you're\nlooking for an open-source production-ready vector database that you can run\nlocally (in a docker container) or hosted in the cloud, then go for Weaviate\n[/docs/modules/data_connection/vectorstores/integrations/weaviate]. * If you're\nusing Supabase already then look at the Supabase\n[/docs/modules/data_connection/vectorstores/integrations/supabase] vector store\nto use the same Postgres","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":310,"to":320}}}}],["218",{"pageContent":"* If you're using Supabase already then look at the Supabase\n[/docs/modules/data_connection/vectorstores/integrations/supabase] vector store\nto use the same Postgres database for your embeddings too * If you're looking\nfor a production-ready vector store you don't have to worry about hosting\nyourself, then go for Pinecone\n[/docs/modules/data_connection/vectorstores/integrations/pinecone] * If you are\nalready utilizing SingleStore, or if you find yourself in need of a distributed,\nhigh-performance database, you might want to consider the SingleStore\n[/docs/modules/data_connection/vectorstores/integrations/singlestore] vector\nstore. * If you are looking for an online MPP (Massively Parallel Processing)\ndata warehousing service, you might want to consider the AnalyticDB\n[/docs/modules/data_connection/vectorstores/integrations/analyticdb] vector\nstore. * If you're in search of a cost-effective vector database that allows run\nvector search with SQL, look no further than","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":320,"to":328}}}}],["219",{"pageContent":"vector store. * If you're in search of a cost-effective vector database that\nallows run vector search with SQL, look no further than MyScale\n[/docs/modules/data_connection/vectorstores/integrations/myscale]. Previous\nHuggingFace Transformers\n[/docs/modules/data_connection/text_embedding/integrations/transformers] Next\nIntegrations [/docs/modules/data_connection/vectorstores/integrations/] * Get\nstarted Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/","title":"Vector stores | 🦜️🔗 Langchain","description":"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding","language":"en","loc":{"lines":{"from":328,"to":352}}}}],["220",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * Document\ntransformers","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/","title":"Retrievers | 🦜️🔗 Langchain","description":"A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["221",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * Document transformers\n[/docs/modules/data_connection/document_transformers/] * Text embedding models\n[/docs/modules/data_connection/text_embedding/] * Vector stores\n[/docs/modules/data_connection/vectorstores/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * How-to\n[/docs/modules/data_connection/retrievers/how_to/contextual_compression] *\nIntegrations\n[/docs/modules/data_connection/retrievers/integrations/chaindesk-retriever] *\nExperimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Chains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/","title":"Retrievers | 🦜️🔗 Langchain","description":"A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.","language":"en","loc":{"lines":{"from":23,"to":40}}}}],["222",{"pageContent":"* Agents [/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] *\nModules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\n[/docs/modules/data_connection/] * Retrievers On this page RETRIEVERS A\nretriever is an interface that returns documents given an unstructured query. It\nis more general than a vector store. A retriever does not need to be able to\nstore documents, only to return (or retrieve) it. Vector stores can be used as\nthe backbone of a retriever, but there are other types of retrievers as well.\nGET STARTED The public API of the BaseRetriever class in LangChain.js is as\nfollows: export abstract class BaseRetriever { abstract","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/","title":"Retrievers | 🦜️🔗 Langchain","description":"A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.","language":"en","loc":{"lines":{"from":40,"to":72}}}}],["223",{"pageContent":"a retriever, but there are other types of retrievers as well. GET STARTED The\npublic API of the BaseRetriever class in LangChain.js is as follows: export\nabstract class BaseRetriever { abstract getRelevantDocuments(query: string):\nPromise; } It's that simple! You can call getRelevantDocuments to retrieve\ndocuments relevant to a query, where \"relevance\" is defined by the specific\nretriever object you are calling. Of course, we also help construct what we\nthink useful Retrievers are. The main type of Retriever in LangChain is a vector\nstore retriever. We will focus on that here. Note: Before reading, it's\nimportant to understand what a vector store is\n[/docs/modules/data_connection/vectorstores]. This example showcases question\nanswering over documents. We have chosen this as the example for getting started\nbecause it nicely combines a lot of different elements (Text splitters,\nembeddings, vectorstores) and then also shows how to use them in a chain.\nQuestion","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/","title":"Retrievers | 🦜️🔗 Langchain","description":"A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.","language":"en","loc":{"lines":{"from":72,"to":97}}}}],["224",{"pageContent":"this as the example for getting started because it nicely combines a lot of\ndifferent elements (Text splitters, embeddings, vectorstores) and then also\nshows how to use them in a chain. Question answering over documents consists of\nfour steps: 1. Create an index 2. Create a Retriever from that index 3. Create a\nquestion answering chain 4. Ask questions! Each of the steps has multiple sub\nsteps and potential configurations, but we'll go through one common flow using\nHNSWLib, a local vector store. This assumes you're using Node, but you can swap\nin another integration if necessary. First, install the required dependency: *\nnpm * Yarn * pnpm npm install -S hnswlib-node yarn add hnswlib-node pnpm add\nhnswlib-node You can download the state_of_the_union.txt file here\n[https://github.com/hwchase17/langchain/blob/master/docs/extras/modules/state_of_the_union.txt].\nimport { OpenAI } from \"langchain/llms/openai\"; import { RetrievalQAChain } from","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/","title":"Retrievers | 🦜️🔗 Langchain","description":"A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.","language":"en","loc":{"lines":{"from":97,"to":135}}}}],["225",{"pageContent":"file here\n[https://github.com/hwchase17/langchain/blob/master/docs/extras/modules/state_of_the_union.txt].\nimport { OpenAI } from \"langchain/llms/openai\"; import { RetrievalQAChain } from\n\"langchain/chains\"; import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\"; import {\nRecursiveCharacterTextSplitter } from \"langchain/text_splitter\"; import * as fs\nfrom \"fs\"; // Initialize the LLM to use to answer the question. const model =\nnew OpenAI({}); const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]); // Create a vector\nstore from the documents. const vectorStore = await HNSWLib.fromDocuments(docs,\nnew OpenAIEmbeddings()); // Initialize a retriever wrapper around the vector\nstore const vectorStoreRetriever = vectorStore.asRetriever(); // Create a chain\nthat uses the OpenAI LLM","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/","title":"Retrievers | 🦜️🔗 Langchain","description":"A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.","language":"en","loc":{"lines":{"from":135,"to":157}}}}],["226",{"pageContent":"new OpenAIEmbeddings()); // Initialize a retriever wrapper around the vector\nstore const vectorStoreRetriever = vectorStore.asRetriever(); // Create a chain\nthat uses the OpenAI LLM and HNSWLib vector store. const chain =\nRetrievalQAChain.fromLLM(model, vectorStoreRetriever); const res = await\nchain.call({ query: \"What did the president say about Justice Breyer?\", });\nconsole.log({ res }); /* { res: { text: 'The president said that Justice Breyer\nwas an Army veteran, Constitutional scholar, and retiring Justice of the United\nStates Supreme Court and thanked him for his service.' } } */ API REFERENCE: *\nOpenAI [/docs/api/llms_openai/classes/OpenAI] from langchain/llms/openai *\nRetrievalQAChain [/docs/api/chains/classes/RetrievalQAChain] from\nlangchain/chains * HNSWLib [/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai *","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/","title":"Retrievers | 🦜️🔗 Langchain","description":"A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.","language":"en","loc":{"lines":{"from":157,"to":186}}}}],["227",{"pageContent":"[/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter Let's walk through what's happening here. 1. We first\nload a long text and split it into smaller documents using a text splitter. We\nthen load those documents (which also embeds the documents using the passed\nOpenAIEmbeddings instance) into HNSWLib, our vector store, creating our index.\n2. Though we can query the vector store directly, we convert the vector store\ninto a retriever to return retrieved documents in the right format for the\nquestion answering chain. 3. We initialize a RetrievalQAChain with the .fromLLM\nmethod, which we'll call later in step 4. 4. We ask questions! See the\nindividual sections for deeper dives on specific","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/","title":"Retrievers | 🦜️🔗 Langchain","description":"A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.","language":"en","loc":{"lines":{"from":186,"to":202}}}}],["228",{"pageContent":"answering chain. 3. We initialize a RetrievalQAChain with the .fromLLM method,\nwhich we'll call later in step 4. 4. We ask questions! See the individual\nsections for deeper dives on specific retrievers. Previous Zep\n[/docs/modules/data_connection/vectorstores/integrations/zep] Next Contextual\ncompression\n[/docs/modules/data_connection/retrievers/how_to/contextual_compression] * Get\nstarted Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/","title":"Retrievers | 🦜️🔗 Langchain","description":"A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.","language":"en","loc":{"lines":{"from":202,"to":230}}}}],["229",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * Document\ntransformers","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai","title":"Google Vertex AI | 🦜️🔗 Langchain","description":"This API is new and may change in future LangChainJS versions.","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["230",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * Document transformers\n[/docs/modules/data_connection/document_transformers/] * Text embedding models\n[/docs/modules/data_connection/text_embedding/] * Vector stores\n[/docs/modules/data_connection/vectorstores/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * Experimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Multimodal embedding models\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Google Vertex AI\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Graph databases\n[/docs/modules/data_connection/experimental/graph_databases/neo4j] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] *","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai","title":"Google Vertex AI | 🦜️🔗 Langchain","description":"This API is new and may change in future LangChainJS versions.","language":"en","loc":{"lines":{"from":23,"to":37}}}}],["231",{"pageContent":"* Graph databases\n[/docs/modules/data_connection/experimental/graph_databases/neo4j] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\n[/docs/modules/data_connection/] * Experimental * Multimodal embedding models *\nGoogle Vertex AI On this page GOOGLE VERTEX AI Experimental This API is new and\nmay change in future LangChainJS versions. The\nGoogleVertexAIMultimodalEmbeddings class provides additional methods that are\nparallels to the embedDocuments() and embedQuery() methods: *","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai","title":"Google Vertex AI | 🦜️🔗 Langchain","description":"This API is new and may change in future LangChainJS versions.","language":"en","loc":{"lines":{"from":37,"to":71}}}}],["232",{"pageContent":"new and may change in future LangChainJS versions. The\nGoogleVertexAIMultimodalEmbeddings class provides additional methods that are\nparallels to the embedDocuments() and embedQuery() methods: * embedImage() and\nembedImageQuery() take node Buffer objects that are expected to contain an\nimage. * embedMedia() and embedMediaQuery() take an object that contain a text\nstring field, an image Buffer field, or both and returns a similarly constructed\nobject containing the respective vectors. Note: The Google Vertex AI embeddings\nmodels have different vector sizes than OpenAI's standard model, so some vector\nstores may not handle them correctly. * The textembedding-gecko model in\nGoogleVertexAIEmbeddings provides 768 dimensions. * The multimodalembedding@001\nmodel in GoogleVertexAIMultimodalEmbeddings provides 1408 dimensions. SETUP The\nVertex AI implementation is meant to be used in Node.js and not directly in a\nbrowser, since it requires a service account to use. Before","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai","title":"Google Vertex AI | 🦜️🔗 Langchain","description":"This API is new and may change in future LangChainJS versions.","language":"en","loc":{"lines":{"from":71,"to":92}}}}],["233",{"pageContent":"provides 1408 dimensions. SETUP The Vertex AI implementation is meant to be used\nin Node.js and not directly in a browser, since it requires a service account to\nuse. Before running this code, you should make sure the Vertex AI API is enabled\nfor the relevant project in your Google Cloud dashboard and that you've\nauthenticated to Google Cloud using one of these methods: * You are logged into\nan account (using gcloud auth application-default login) permitted to that\nproject. * You are running on a machine using a service account that is\npermitted to the project. * You have downloaded the credentials for a service\naccount that is permitted to the project and set the\nGOOGLE_APPLICATION_CREDENTIALS environment variable to the path of this file. *\nnpm * Yarn * pnpm npm install google-auth-library yarn add google-auth-library\npnpm add google-auth-library USAGE Here's a basic example that shows how to\nembed image queries: import fs from \"fs\"; import {","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai","title":"Google Vertex AI | 🦜️🔗 Langchain","description":"This API is new and may change in future LangChainJS versions.","language":"en","loc":{"lines":{"from":92,"to":133}}}}],["234",{"pageContent":"install google-auth-library yarn add google-auth-library pnpm add\ngoogle-auth-library USAGE Here's a basic example that shows how to embed image\nqueries: import fs from \"fs\"; import { GoogleVertexAIMultimodalEmbeddings } from\n\"langchain/experimental/multimodal_embeddings/googlevertexai\"; const model = new\nGoogleVertexAIMultimodalEmbeddings(); // Load the image into a buffer to get the\nembedding of it const img = fs.readFileSync(\"/path/to/file.jpg\"); const\nimgEmbedding = await model.embedImageQuery(img); console.log({ imgEmbedding });\n// You can also get text embeddings const textEmbedding = await\nmodel.embedQuery( \"What would be a good company name for a company that makes\ncolorful socks?\" ); console.log({ textEmbedding }); API REFERENCE: *\nGoogleVertexAIMultimodalEmbeddings\n[/docs/api/experimental_multimodal_embeddings_googlevertexai/classes/GoogleVertexAIMultimodalEmbeddings]\nfrom langchain/experimental/multimodal_embeddings/googlevertexai ADVANCED","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai","title":"Google Vertex AI | 🦜️🔗 Langchain","description":"This API is new and may change in future LangChainJS versions.","language":"en","loc":{"lines":{"from":133,"to":179}}}}],["235",{"pageContent":"[/docs/api/experimental_multimodal_embeddings_googlevertexai/classes/GoogleVertexAIMultimodalEmbeddings]\nfrom langchain/experimental/multimodal_embeddings/googlevertexai ADVANCED USAGE\nHere's a more advanced example that shows how to integrate these new embeddings\nwith a LangChain vector store. import fs from \"fs\"; import {\nGoogleVertexAIMultimodalEmbeddings } from\n\"langchain/experimental/multimodal_embeddings/googlevertexai\"; import {\nFaissStore } from \"langchain/vectorstores/faiss\"; import { Document } from\n\"langchain/document\"; const embeddings = new\nGoogleVertexAIMultimodalEmbeddings(); const vectorStore = await\nFaissStore.fromTexts( [\"dog\", \"cat\", \"horse\", \"seagull\"], [{ id: 2 }, { id: 1 },\n{ id: 3 }, { id: 4 }], embeddings ); const img = fs.readFileSync(\"parrot.jpeg\");\nconst vectors: number[] = await embeddings.embedImageQuery(img); const document\n= new Document({ pageContent: img.toString(\"base64\"), // Metadata is optional\nbut helps track what kind of","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai","title":"Google Vertex AI | 🦜️🔗 Langchain","description":"This API is new and may change in future LangChainJS versions.","language":"en","loc":{"lines":{"from":179,"to":204}}}}],["236",{"pageContent":"vectors: number[] = await embeddings.embedImageQuery(img); const document = new\nDocument({ pageContent: img.toString(\"base64\"), // Metadata is optional but\nhelps track what kind of document is being retrieved metadata: { id: 5,\nmediaType: \"image\", }, }); // Add the image embedding vectors to the vector\nstore directly await vectorStore.addVectors([vectors], [document]); // Use a\nsimilar image to the one just added const img2 =\nfs.readFileSync(\"parrot-icon.png\"); const vectors2: number[] = await\nembeddings.embedImageQuery(img2); // Use the lower level, direct API const\nresultTwo = await vectorStore.similaritySearchVectorWithScore( vectors2, 2 );\nconsole.log(JSON.stringify(resultTwo, null, 2)); /* [ [ Document { pageContent:\n'' metadata: { id: 5, mediaType: \"image\" } }, 0.8931522965431213 ], [ Document {\npageContent: 'seagull', metadata: {","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai","title":"Google Vertex AI | 🦜️🔗 Langchain","description":"This API is new and may change in future LangChainJS versions.","language":"en","loc":{"lines":{"from":204,"to":243}}}}],["237",{"pageContent":"metadata: { id: 5, mediaType: \"image\" } }, 0.8931522965431213 ], [ Document {\npageContent: 'seagull', metadata: { id: 4 } }, 1.9188631772994995 ] ] */ API\nREFERENCE: * GoogleVertexAIMultimodalEmbeddings\n[/docs/api/experimental_multimodal_embeddings_googlevertexai/classes/GoogleVertexAIMultimodalEmbeddings]\nfrom langchain/experimental/multimodal_embeddings/googlevertexai * FaissStore\n[/docs/api/vectorstores_faiss/classes/FaissStore] from\nlangchain/vectorstores/faiss * Document [/docs/api/document/classes/Document]\nfrom langchain/document Previous Zep Retriever\n[/docs/modules/data_connection/retrievers/integrations/zep-retriever] Next Neo4j\n[/docs/modules/data_connection/experimental/graph_databases/neo4j] * Setup *\nUsage * Advanced usage Community * Discord [https://discord.gg/cU2adEyC7w] *\nTwitter [https://twitter.com/LangChainAI] GitHub * Python","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai","title":"Google Vertex AI | 🦜️🔗 Langchain","description":"This API is new and may change in future LangChainJS versions.","language":"en","loc":{"lines":{"from":243,"to":288}}}}],["238",{"pageContent":"* Setup * Usage * Advanced usage Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai","title":"Google Vertex AI | 🦜️🔗 Langchain","description":"This API is new and may change in future LangChainJS versions.","language":"en","loc":{"lines":{"from":288,"to":304}}}}],["239",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * Document\ntransformers","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["240",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * Document transformers\n[/docs/modules/data_connection/document_transformers/] * Text embedding models\n[/docs/modules/data_connection/text_embedding/] * Vector stores\n[/docs/modules/data_connection/vectorstores/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * How-to\n[/docs/modules/data_connection/retrievers/how_to/contextual_compression] *\nContextual compression\n[/docs/modules/data_connection/retrievers/how_to/contextual_compression] *\nMultiQuery Retriever\n[/docs/modules/data_connection/retrievers/how_to/multi-query-retriever] *\nMultiVector Retriever\n[/docs/modules/data_connection/retrievers/how_to/multi-vector-retriever] *\nParent Document Retriever\n[/docs/modules/data_connection/retrievers/how_to/parent-document-retriever] *\nSelf-querying","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":23,"to":35}}}}],["241",{"pageContent":"* Parent Document Retriever\n[/docs/modules/data_connection/retrievers/how_to/parent-document-retriever] *\nSelf-querying [/docs/modules/data_connection/retrievers/how_to/self_query/] *\nSimilarity Score Threshold\n[/docs/modules/data_connection/retrievers/how_to/similarity-score-threshold-retriever]\n* Time-weighted vector store retriever\n[/docs/modules/data_connection/retrievers/how_to/time_weighted_vectorstore] *\nVector store-backed retriever\n[/docs/modules/data_connection/retrievers/how_to/vectorstore] * Integrations\n[/docs/modules/data_connection/retrievers/integrations/chaindesk-retriever] *\nExperimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Chains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":35,"to":49}}}}],["242",{"pageContent":"* Memory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\n[/docs/modules/data_connection/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * How-to * Parent Document Retriever\nPARENT DOCUMENT RETRIEVER When splitting documents for retrieval, there are\noften conflicting desires: 1. You may want to have small documents, so that\ntheir embeddings can most accurately reflect their meaning. If too long, then\nthe embeddings can lose meaning. 2. You want to have long enough documents that\nthe context of each chunk is retained. The","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":49,"to":78}}}}],["243",{"pageContent":"can most accurately reflect their meaning. If too long, then the embeddings can\nlose meaning. 2. You want to have long enough documents that the context of each\nchunk is retained. The ParentDocumentRetriever strikes that balance by splitting\nand storing small chunks of data. During retrieval, it first fetches the small\nchunks but then looks up the parent ids for those chunks and returns those\nlarger documents. Note that \"parent document\" refers to the document that a\nsmall chunk originated from. This can either be the whole raw document OR a\nlarger chunk. USAGE import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { MemoryVectorStore } from\n\"langchain/vectorstores/memory\"; import { InMemoryDocstore } from\n\"langchain/stores/doc/in_memory\"; import { ParentDocumentRetriever } from\n\"langchain/retrievers/parent_document\"; import { RecursiveCharacterTextSplitter\n} from \"langchain/text_splitter\"; import { TextLoader } from","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":78,"to":96}}}}],["244",{"pageContent":"{ ParentDocumentRetriever } from \"langchain/retrievers/parent_document\"; import\n{ RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"; import {\nTextLoader } from \"langchain/document_loaders/fs/text\"; const vectorstore = new\nMemoryVectorStore(new OpenAIEmbeddings()); const docstore = new\nInMemoryDocstore(); const retriever = new ParentDocumentRetriever({ vectorstore,\ndocstore, // Optional, not required if you're already passing in split documents\nparentSplitter: new RecursiveCharacterTextSplitter({ chunkOverlap: 0, chunkSize:\n500, }), childSplitter: new RecursiveCharacterTextSplitter({ chunkOverlap: 0,\nchunkSize: 50, }), // Optional `k` parameter to search for more child documents\nin VectorStore. // Note that this does not exactly correspond to the number of\nfinal (parent) documents // retrieved, as multiple child documents can point to\nthe same parent. childK: 20, // Optional `k` parameter to limit number of final,\nparent documents","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":96,"to":118}}}}],["245",{"pageContent":"the number of final (parent) documents // retrieved, as multiple child documents\ncan point to the same parent. childK: 20, // Optional `k` parameter to limit\nnumber of final, parent documents returned from this // retriever and sent to\nLLM. This is an upper-bound, and the final count may be lower than this.\nparentK: 5, }); const textLoader = new\nTextLoader(\"../examples/state_of_the_union.txt\"); const parentDocuments = await\ntextLoader.load(); // We must add the parent documents via the retriever's\naddDocuments method await retriever.addDocuments(parentDocuments); const\nretrievedDocs = await retriever.getRelevantDocuments(\"justice breyer\"); //\nRetrieved chunks are the larger parent chunks console.log(retrievedDocs); /* [\nDocument { pageContent: 'Tonight, I call on the Senate to pass — pass the\nFreedom to Vote Act. Pass the John Lewis Act — Voting Rights Act. And while\nyou’re at it, pass the DISCLOSE Act so Americans know who is funding our\nelections.\\n' +","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":118,"to":138}}}}],["246",{"pageContent":"Senate to pass — pass the Freedom to Vote Act. Pass the John Lewis Act — Voting\nRights Act. And while you’re at it, pass the DISCLOSE Act so Americans know who\nis funding our elections.\\n' + '\\n' + 'Look, tonight, I’d — I’d like to honor\nsomeone who has dedicated his life to serve this country: Justice Breyer — an\nArmy veteran, Constitutional scholar, retiring Justice of the United States\nSupreme Court.', metadata: { source: '../examples/state_of_the_union.txt', loc:\n[Object] } }, Document { pageContent: 'As I did four days ago, I’ve nominated a\nCircuit Court of Appeals — Ketanji Brown Jackson. One of our nation’s top legal\nminds who will continue in just Brey- — Justice Breyer’s legacy of excellence. A\nformer top litigator in private practice, a former federal public defender from\na family of public-school educators and police officers — she’s a consensus\nbuilder.', metadata: { source: '../examples/state_of_the_union.txt', loc:\n[Object] }","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":138,"to":145}}}}],["247",{"pageContent":"federal public defender from a family of public-school educators and police\nofficers — she’s a consensus builder.', metadata: { source:\n'../examples/state_of_the_union.txt', loc: [Object] } }, Document { pageContent:\n'Justice Breyer, thank you for your service. Thank you, thank you, thank you. I\nmean it. Get up. Stand — let me see you. Thank you.\\n' + '\\n' + 'And we all know\n— no matter what your ideology, we all know one of the most serious\nconstitutional responsibilities a President has is nominating someone to serve\non the United States Supreme Court.', metadata: { source:\n'../examples/state_of_the_union.txt', loc: [Object] } } ] */ API REFERENCE: *\nOpenAIEmbeddings [/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * MemoryVectorStore\n[/docs/api/vectorstores_memory/classes/MemoryVectorStore] from\nlangchain/vectorstores/memory * InMemoryDocstore","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":145,"to":164}}}}],["248",{"pageContent":"from langchain/embeddings/openai * MemoryVectorStore\n[/docs/api/vectorstores_memory/classes/MemoryVectorStore] from\nlangchain/vectorstores/memory * InMemoryDocstore\n[/docs/api/stores_doc_in_memory/classes/InMemoryDocstore] from\nlangchain/stores/doc/in_memory * ParentDocumentRetriever\n[/docs/api/retrievers_parent_document/classes/ParentDocumentRetriever] from\nlangchain/retrievers/parent_document * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter * TextLoader\n[/docs/api/document_loaders_fs_text/classes/TextLoader] from\nlangchain/document_loaders/fs/text Previous MultiVector Retriever\n[/docs/modules/data_connection/retrievers/how_to/multi-vector-retriever] Next\nSelf-querying [/docs/modules/data_connection/retrievers/how_to/self_query/]\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":164,"to":184}}}}],["249",{"pageContent":"* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/parent-document-retriever","title":"Parent Document Retriever | 🦜️🔗 Langchain","description":"When splitting documents for retrieval, there are often conflicting desires:","language":"en","loc":{"lines":{"from":184,"to":195}}}}],["250",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * Document\ntransformers","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["251",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * Document transformers\n[/docs/modules/data_connection/document_transformers/] * Text embedding models\n[/docs/modules/data_connection/text_embedding/] * Vector stores\n[/docs/modules/data_connection/vectorstores/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * How-to\n[/docs/modules/data_connection/retrievers/how_to/contextual_compression] *\nContextual compression\n[/docs/modules/data_connection/retrievers/how_to/contextual_compression] *\nMultiQuery Retriever\n[/docs/modules/data_connection/retrievers/how_to/multi-query-retriever] *\nMultiVector Retriever\n[/docs/modules/data_connection/retrievers/how_to/multi-vector-retriever] *\nParent Document Retriever\n[/docs/modules/data_connection/retrievers/how_to/parent-document-retriever] *\nSelf-querying","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":23,"to":35}}}}],["252",{"pageContent":"* Parent Document Retriever\n[/docs/modules/data_connection/retrievers/how_to/parent-document-retriever] *\nSelf-querying [/docs/modules/data_connection/retrievers/how_to/self_query/] *\nChroma Self Query Retriever\n[/docs/modules/data_connection/retrievers/how_to/self_query/chroma-self-query] *\nHNSWLib Self Query Retriever\n[/docs/modules/data_connection/retrievers/how_to/self_query/hnswlib-self-query]\n* Memory Vector Store Self Query Retriever\n[/docs/modules/data_connection/retrievers/how_to/self_query/memory-self-query] *\nPinecone Self Query Retriever\n[/docs/modules/data_connection/retrievers/how_to/self_query/pinecone-self-query]\n* Supabase Self Query Retriever\n[/docs/modules/data_connection/retrievers/how_to/self_query/supabase-self-query]\n* Weaviate Self Query Retriever\n[/docs/modules/data_connection/retrievers/how_to/self_query/weaviate-self-query]\n* Similarity Score Threshold","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":35,"to":44}}}}],["253",{"pageContent":"* Weaviate Self Query Retriever\n[/docs/modules/data_connection/retrievers/how_to/self_query/weaviate-self-query]\n* Similarity Score Threshold\n[/docs/modules/data_connection/retrievers/how_to/similarity-score-threshold-retriever]\n* Time-weighted vector store retriever\n[/docs/modules/data_connection/retrievers/how_to/time_weighted_vectorstore] *\nVector store-backed retriever\n[/docs/modules/data_connection/retrievers/how_to/vectorstore] * Integrations\n[/docs/modules/data_connection/retrievers/integrations/chaindesk-retriever] *\nExperimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Chains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":44,"to":58}}}}],["254",{"pageContent":"Callbacks [/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides\n[/docs/guides] * Ecosystem [/docs/ecosystem] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\n[/docs/modules/data_connection/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * How-to * Self-querying\nSELF-QUERYING A self-querying retriever is one that, as the name suggests, has\nthe ability to query itself. Specifically, given any natural language query, the\nretriever uses a query-constructing LLM chain to write a structured query and\nthen applies that structured query to it's underlying VectorStore. This allows\nthe retriever to not only use the user-input query for semantic similarity\ncomparison with the contents of stored documented, but","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":58,"to":82}}}}],["255",{"pageContent":"that structured query to it's underlying VectorStore. This allows the retriever\nto not only use the user-input query for semantic similarity comparison with the\ncontents of stored documented, but to also extract filters from the user query\non the metadata of stored documents and to execute those filters.\n[https://drive.google.com/uc?id=1OQUN-0MJcDUxmPXofgS7MqReEs720pqS] All Self\nQuery retrievers require peggy as a peer dependency: * npm * Yarn * pnpm npm\ninstall -S peggy yarn add peggy pnpm add peggy USAGE Here's a basic example with\nan in-memory, unoptimized vector store: import { MemoryVectorStore } from\n\"langchain/vectorstores/memory\"; import { AttributeInfo } from\n\"langchain/schema/query_constructor\"; import { Document } from\n\"langchain/document\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { SelfQueryRetriever } from\n\"langchain/retrievers/self_query\"; import { FunctionalTranslator } from","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":82,"to":120}}}}],["256",{"pageContent":"} from \"langchain/document\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { SelfQueryRetriever } from\n\"langchain/retrievers/self_query\"; import { FunctionalTranslator } from\n\"langchain/retrievers/self_query/functional\"; import { OpenAI } from\n\"langchain/llms/openai\"; /** * First, we create a bunch of documents. You can\nload your own documents here instead. * Each document has a pageContent and a\nmetadata field. Make sure your metadata matches the AttributeInfo below. */\nconst docs = [ new Document({ pageContent: \"A bunch of scientists bring back\ndinosaurs and mayhem breaks loose\", metadata: { year: 1993, rating: 7.7, genre:\n\"science fiction\" }, }), new Document({ pageContent: \"Leo DiCaprio gets lost in\na dream within a dream within a dream within a ...\", metadata: { year: 2010,\ndirector: \"Christopher Nolan\", rating: 8.2 }, }), new Document({ pageContent: \"A\npsychologist / detective gets lost in a series of","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":120,"to":143}}}}],["257",{"pageContent":"a dream within a ...\", metadata: { year: 2010, director: \"Christopher Nolan\",\nrating: 8.2 }, }), new Document({ pageContent: \"A psychologist / detective gets\nlost in a series of dreams within dreams within dreams and Inception reused the\nidea\", metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 }, }), new\nDocument({ pageContent: \"A bunch of normal-sized women are supremely wholesome\nand some men pine after them\", metadata: { year: 2019, director: \"Greta Gerwig\",\nrating: 8.3 }, }), new Document({ pageContent: \"Toys come alive and have a blast\ndoing so\", metadata: { year: 1995, genre: \"animated\" }, }), new Document({\npageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\nmetadata: { year: 1979, director: \"Andrei Tarkovsky\", genre: \"science fiction\",\nrating: 9.9, }, }), ]; /** * Next, we define the attributes we want to be able\nto query on. * in this case, we","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":143,"to":173}}}}],["258",{"pageContent":"1979, director: \"Andrei Tarkovsky\", genre: \"science fiction\", rating: 9.9, },\n}), ]; /** * Next, we define the attributes we want to be able to query on. * in\nthis case, we want to be able to query on the genre, year, director, rating, and\nlength of the movie. * We also provide a description of each attribute and the\ntype of the attribute. * This is used to generate the query prompts. */ const\nattributeInfo: AttributeInfo[] = [ { name: \"genre\", description: \"The genre of\nthe movie\", type: \"string or array of strings\", }, { name: \"year\", description:\n\"The year the movie was released\", type: \"number\", }, { name: \"director\",\ndescription: \"The director of the movie\", type: \"string\", }, { name: \"rating\",\ndescription: \"The rating of the movie (1-10)\", type: \"number\", }, { name:\n\"length\", description: \"The length of the movie in minutes\", type: \"number\", },\n]; /** * Next, we","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":173,"to":216}}}}],["259",{"pageContent":"description: \"The rating of the movie (1-10)\", type: \"number\", }, { name:\n\"length\", description: \"The length of the movie in minutes\", type: \"number\", },\n]; /** * Next, we instantiate a vector store. This is where we store the\nembeddings of the documents. * We also need to provide an embeddings object.\nThis is used to embed the documents. */ const embeddings = new\nOpenAIEmbeddings(); const llm = new OpenAI(); const documentContents = \"Brief\nsummary of a movie\"; const vectorStore = await\nMemoryVectorStore.fromDocuments(docs, embeddings); const selfQueryRetriever =\nawait SelfQueryRetriever.fromLLM({ llm, vectorStore, documentContents,\nattributeInfo, /** * We need to use a translator that translates the queries\ninto a * filter format that the vector store can understand. We provide a basic\ntranslator * translator here, but you can create your own translator by\nextending BaseTranslator * abstract class. Note that the vector store needs to","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":216,"to":243}}}}],["260",{"pageContent":"store can understand. We provide a basic translator * translator here, but you\ncan create your own translator by extending BaseTranslator * abstract class.\nNote that the vector store needs to support filtering on the metadata *\nattributes you want to query on. */ structuredQueryTranslator: new\nFunctionalTranslator(), }); /** * Now we can query the vector store. * We can\nask questions like \"Which movies are less than 90 minutes?\" or \"Which movies are\nrated higher than 8.5?\". * We can also ask questions like \"Which movies are\neither comedy or drama and are less than 90 minutes?\". * The retriever will\nautomatically convert these questions into queries that can be used to retrieve\ndocuments. */ const query1 = await selfQueryRetriever.getRelevantDocuments(\n\"Which movies are less than 90 minutes?\" ); const query2 = await\nselfQueryRetriever.getRelevantDocuments( \"Which movies are rated higher than\n8.5?\" ); const query3 = await selfQueryRetriever.getRelevantDocuments(","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":243,"to":263}}}}],["261",{"pageContent":"less than 90 minutes?\" ); const query2 = await\nselfQueryRetriever.getRelevantDocuments( \"Which movies are rated higher than\n8.5?\" ); const query3 = await selfQueryRetriever.getRelevantDocuments( \"Which\nmovies are directed by Greta Gerwig?\" ); const query4 = await\nselfQueryRetriever.getRelevantDocuments( \"Which movies are either comedy or\ndrama and are less than 90 minutes?\" ); console.log(query1, query2, query3,\nquery4); API REFERENCE: * MemoryVectorStore\n[/docs/api/vectorstores_memory/classes/MemoryVectorStore] from\nlangchain/vectorstores/memory * AttributeInfo\n[/docs/api/schema_query_constructor/classes/AttributeInfo] from\nlangchain/schema/query_constructor * Document\n[/docs/api/document/classes/Document] from langchain/document * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * SelfQueryRetriever\n[/docs/api/retrievers_self_query/classes/SelfQueryRetriever] from\nlangchain/retrievers/self_query *","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":263,"to":286}}}}],["262",{"pageContent":"from langchain/embeddings/openai * SelfQueryRetriever\n[/docs/api/retrievers_self_query/classes/SelfQueryRetriever] from\nlangchain/retrievers/self_query * FunctionalTranslator\n[/docs/api/retrievers_self_query_functional/classes/FunctionalTranslator] from\nlangchain/retrievers/self_query/functional * OpenAI\n[/docs/api/llms_openai/classes/OpenAI] from langchain/llms/openai SETTING\nDEFAULT SEARCH PARAMS You can also pass in a default filter when initializing\nthe self-query retriever that will be used in combination with or as a fallback\nto the generated query. For example, if you wanted to ensure that your query\ndocuments tagged as genre: \"animated\", you could initialize the above retriever\nas follows: const selfQueryRetriever = await SelfQueryRetriever.fromLLM({ llm,\nvectorStore, documentContents, attributeInfo, structuredQueryTranslator: new\nFunctionalTranslator(), searchParams: { filter: (doc: Document) => doc.metadata\n&& doc.metadata.genre === \"animated\",","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":286,"to":306}}}}],["263",{"pageContent":"documentContents, attributeInfo, structuredQueryTranslator: new\nFunctionalTranslator(), searchParams: { filter: (doc: Document) => doc.metadata\n&& doc.metadata.genre === \"animated\", mergeFiltersOperator: \"and\", }, }); The\ntype of filter required will depend on the specific translator used for the\nretriever. See the individual pages for examples. Other supported values for\nmergeFiltersOperator are \"or\" or \"replace\". Previous Parent Document Retriever\n[/docs/modules/data_connection/retrievers/how_to/parent-document-retriever] Next\nChroma Self Query Retriever\n[/docs/modules/data_connection/retrievers/how_to/self_query/chroma-self-query]\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query","title":"Self-querying | 🦜️🔗 Langchain","description":"A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.","language":"en","loc":{"lines":{"from":306,"to":340}}}}],["264",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Prompts [/docs/modules/model_io/prompts/] * Language\nmodels [/docs/modules/model_io/models/] * Output parsers\n[/docs/modules/model_io/output_parsers/] *","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/structured","title":"Structured output parser | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["265",{"pageContent":"* Prompts [/docs/modules/model_io/prompts/] * Language models\n[/docs/modules/model_io/models/] * Output parsers\n[/docs/modules/model_io/output_parsers/] * How-to\n[/docs/modules/model_io/output_parsers/how_to/use_with_llm_chain] * Bytes output\nparser [/docs/modules/model_io/output_parsers/bytes] * Combining output parsers\n[/docs/modules/model_io/output_parsers/combining_output_parser] * List parser\n[/docs/modules/model_io/output_parsers/comma_separated] * Custom list parser\n[/docs/modules/model_io/output_parsers/custom_list_parser] * Auto-fixing parser\n[/docs/modules/model_io/output_parsers/output_fixing_parser] * String output\nparser [/docs/modules/model_io/output_parsers/string] * Structured output parser\n[/docs/modules/model_io/output_parsers/structured] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * Memory\n[/docs/modules/memory/] * Agents [/docs/modules/agents/] *","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/structured","title":"Structured output parser | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":24,"to":39}}}}],["266",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Output parsers\n[/docs/modules/model_io/output_parsers/] * Structured output parser STRUCTURED\nOUTPUT PARSER This output parser can be used when you want to return multiple\nfields. If you want complex schema returned (i.e. a JSON object with arrays of\nstrings), use the Zod Schema detailed below. import { OpenAI } from\n\"langchain/llms/openai\"; import { PromptTemplate } from","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/structured","title":"Structured output parser | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":39,"to":67}}}}],["267",{"pageContent":"If you want complex schema returned (i.e. a JSON object with arrays of strings),\nuse the Zod Schema detailed below. import { OpenAI } from\n\"langchain/llms/openai\"; import { PromptTemplate } from \"langchain/prompts\";\nimport { StructuredOutputParser } from \"langchain/output_parsers\"; import {\nRunnableSequence } from \"langchain/schema/runnable\"; const parser =\nStructuredOutputParser.fromNamesAndDescriptions({ answer: \"answer to the user's\nquestion\", source: \"source used to answer the user's question, should be a\nwebsite.\", }); const chain = RunnableSequence.from([\nPromptTemplate.fromTemplate( \"Answer the users question as best as\npossible.\\n{format_instructions}\\n{question}\" ), new OpenAI({ temperature: 0 }),\nparser, ]); console.log(parser.getFormatInstructions()); /* Answer the users\nquestion as best as possible. The output should be formatted as a JSON instance\nthat conforms to the JSON schema below. As an example, for the schema\n{{\"properties\": {{\"foo\":","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/structured","title":"Structured output parser | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":67,"to":94}}}}],["268",{"pageContent":"the users question as best as possible. The output should be formatted as a JSON\ninstance that conforms to the JSON schema below. As an example, for the schema\n{{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\",\n\"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}} the\nobject {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The\nobject {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted. Here\nis the output schema: ```\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer\nto the user's\nquestion\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources\nused to answer the question, should be\nwebsites.\"}},\"required\":[\"answer\",\"sources\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n``` What is the capital of France? */ const response = await chain.invoke({\nquestion: \"What is the capital of France?\", format_instructions:","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/structured","title":"Structured output parser | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":94,"to":110}}}}],["269",{"pageContent":"is the capital of France? */ const response = await chain.invoke({ question:\n\"What is the capital of France?\", format_instructions:\nparser.getFormatInstructions(), }); console.log(response); // { answer: 'Paris',\nsource: 'https://en.wikipedia.org/wiki/Paris' } API REFERENCE: * OpenAI\n[/docs/api/llms_openai/classes/OpenAI] from langchain/llms/openai *\nPromptTemplate [/docs/api/prompts/classes/PromptTemplate] from langchain/prompts\n* StructuredOutputParser\n[/docs/api/output_parsers/classes/StructuredOutputParser] from\nlangchain/output_parsers * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable STRUCTURED OUTPUT PARSER WITH ZOD SCHEMA This output\nparser can be also be used when you want to define the output schema using Zod,\na TypeScript validation library. The Zod schema passed in needs be parseable\nfrom a JSON string, so eg. z.date() is not allowed. import { z } from \"zod\";\nimport { OpenAI } from","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/structured","title":"Structured output parser | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":110,"to":138}}}}],["270",{"pageContent":"schema using Zod, a TypeScript validation library. The Zod schema passed in\nneeds be parseable from a JSON string, so eg. z.date() is not allowed. import {\nz } from \"zod\"; import { OpenAI } from \"langchain/llms/openai\"; import {\nPromptTemplate } from \"langchain/prompts\"; import { StructuredOutputParser }\nfrom \"langchain/output_parsers\"; import { RunnableSequence } from\n\"langchain/schema/runnable\"; // We can use zod to define a schema for the output\nusing the `fromZodSchema` method of `StructuredOutputParser`. const parser =\nStructuredOutputParser.fromZodSchema( z.object({ answer:\nz.string().describe(\"answer to the user's question\"), sources: z\n.array(z.string()) .describe(\"sources used to answer the question, should be\nwebsites.\"), }) ); const chain = RunnableSequence.from([\nPromptTemplate.fromTemplate( \"Answer the users question as best as\npossible.\\n{format_instructions}\\n{question}\" ), new OpenAI({ temperature: 0 }),","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/structured","title":"Structured output parser | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":138,"to":161}}}}],["271",{"pageContent":"chain = RunnableSequence.from([ PromptTemplate.fromTemplate( \"Answer the users\nquestion as best as possible.\\n{format_instructions}\\n{question}\" ), new\nOpenAI({ temperature: 0 }), parser, ]);\nconsole.log(parser.getFormatInstructions()); /* Answer the users question as\nbest as possible. The output should be formatted as a JSON instance that\nconforms to the JSON schema below. As an example, for the schema {{\"properties\":\n{{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\",\n\"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}} the object {{\"foo\":\n[\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object\n{{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted. Here is the\noutput schema: ```\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer\nto the user's\nquestion\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources\nused to answer the question, should be","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/structured","title":"Structured output parser | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":161,"to":180}}}}],["272",{"pageContent":"to the user's\nquestion\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources\nused to answer the question, should be\nwebsites.\"}},\"required\":[\"answer\",\"sources\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n``` What is the capital of France? */ const response = await chain.invoke({\nquestion: \"What is the capital of France?\", format_instructions:\nparser.getFormatInstructions(), }); console.log(response); /* { answer: 'Paris',\nsources: [ 'https://en.wikipedia.org/wiki/Paris' ] } */ API REFERENCE: * OpenAI\n[/docs/api/llms_openai/classes/OpenAI] from langchain/llms/openai *\nPromptTemplate [/docs/api/prompts/classes/PromptTemplate] from langchain/prompts\n* StructuredOutputParser\n[/docs/api/output_parsers/classes/StructuredOutputParser] from\nlangchain/output_parsers * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable Previous String output","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/structured","title":"Structured output parser | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":245,"to":272}}}}],["273",{"pageContent":"from langchain/output_parsers * RunnableSequence\n[/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable Previous String output parser\n[/docs/modules/model_io/output_parsers/string] Next Retrieval\n[/docs/modules/data_connection/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/output_parsers/structured","title":"Structured output parser | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":272,"to":293}}}}],["274",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * How to [/docs/modules/chains/how_to/] * Foundational","metadata":{"source":"https://js.langchain.com/docs/modules/chains/","title":"Chains | 🦜️🔗 Langchain","description":"Using an LLM in isolation is fine for simple applications,","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["275",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * How to\n[/docs/modules/chains/how_to/] * Foundational\n[/docs/modules/chains/foundational/] * Documents\n[/docs/modules/chains/document/] * Popular [/docs/modules/chains/popular/] *\nAdditional [/docs/modules/chains/additional/] * Memory [/docs/modules/memory/] *\nAgents [/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Chains On this\npage CHAINS Using an LLM in isolation is fine for simple applications, but more\ncomplex applications","metadata":{"source":"https://js.langchain.com/docs/modules/chains/","title":"Chains | 🦜️🔗 Langchain","description":"Using an LLM in isolation is fine for simple applications,","language":"en","loc":{"lines":{"from":24,"to":54}}}}],["276",{"pageContent":"* API reference [/docs/api/] * / * Modules [/docs/modules/] * Chains On this\npage CHAINS Using an LLM in isolation is fine for simple applications, but more\ncomplex applications require chaining LLMs - either with each other or with\nother components. LangChain provides the Chain interface for such \"chained\"\napplications. We define a Chain very generically as a sequence of calls to\ncomponents, which can include other chains. The base interface is simple: import\n{ CallbackManagerForChainRun } from \"langchain/callbacks\"; import { BaseChain as\n_ } from \"langchain/chains\"; import { BaseMemory } from \"langchain/memory\";\nimport { ChainValues } from \"langchain/schema\"; abstract class BaseChain {\nmemory?: BaseMemory; /** * Run the core logic of this chain and return the\noutput */ abstract _call( values: ChainValues, runManager?:\nCallbackManagerForChainRun ): Promise; /** * Return the string type key uniquely\nidentifying this class of chain.","metadata":{"source":"https://js.langchain.com/docs/modules/chains/","title":"Chains | 🦜️🔗 Langchain","description":"Using an LLM in isolation is fine for simple applications,","language":"en","loc":{"lines":{"from":54,"to":88}}}}],["277",{"pageContent":"*/ abstract _call( values: ChainValues, runManager?: CallbackManagerForChainRun\n): Promise; /** * Return the string type key uniquely identifying this class of\nchain. */ abstract _chainType(): string; /** * Return the list of input keys\nthis chain expects to receive when called. */ abstract get inputKeys():\nstring[]; /** * Return the list of output keys this chain will produce when\ncalled. */ abstract get outputKeys(): string[]; } API REFERENCE: *\nCallbackManagerForChainRun\n[/docs/api/callbacks/classes/CallbackManagerForChainRun] from\nlangchain/callbacks * BaseChain [/docs/api/chains/classes/BaseChain] from\nlangchain/chains * BaseMemory [/docs/api/memory/classes/BaseMemory] from\nlangchain/memory * ChainValues [/docs/api/schema/types/ChainValues] from\nlangchain/schema This idea of composing components together in a chain is simple\nbut powerful. It drastically simplifies and makes more modular the\nimplementation of","metadata":{"source":"https://js.langchain.com/docs/modules/chains/","title":"Chains | 🦜️🔗 Langchain","description":"Using an LLM in isolation is fine for simple applications,","language":"en","loc":{"lines":{"from":88,"to":121}}}}],["278",{"pageContent":"from langchain/schema This idea of composing components together in a chain is\nsimple but powerful. It drastically simplifies and makes more modular the\nimplementation of complex applications, which in turn makes it much easier to\ndebug, maintain, and improve your applications. For more specifics check out: *\nHow-to [/docs/modules/chains/how_to/] for walkthroughs of different chain\nfeatures * Foundational [/docs/modules/chains/foundational/] to get acquainted\nwith core building block chains * Document [/docs/modules/chains/document/] to\nlearn how to incorporate documents into chains * Popular\n[/docs/modules/chains/popular/] chains for the most common use cases *\nAdditional [/docs/modules/chains/additional/] to see some of the more advanced\nchains and integrations that you can use out of the box WHY DO WE NEED CHAINS?\nChains allow us to combine multiple components together to create a single,\ncoherent application. For example, we can create a chain that takes user input,","metadata":{"source":"https://js.langchain.com/docs/modules/chains/","title":"Chains | 🦜️🔗 Langchain","description":"Using an LLM in isolation is fine for simple applications,","language":"en","loc":{"lines":{"from":121,"to":139}}}}],["279",{"pageContent":"out of the box WHY DO WE NEED CHAINS? Chains allow us to combine multiple\ncomponents together to create a single, coherent application. For example, we\ncan create a chain that takes user input, formats it with a PromptTemplate, and\nthen passes the formatted response to an LLM. We can build more complex chains\nby combining multiple chains together, or by combining chains with other\ncomponents. GET STARTED USING LLMCHAIN The LLMChain is most basic building block\nchain. It takes in a prompt template, formats it with the user input and returns\nthe response from an LLM. To use the LLMChain, first create a prompt template.\nimport { OpenAI } from \"langchain/llms/openai\"; import { PromptTemplate } from\n\"langchain/prompts\"; import { LLMChain } from \"langchain/chains\"; // We can\nconstruct an LLMChain from a PromptTemplate and an LLM. const model = new\nOpenAI({ temperature: 0 }); const prompt = PromptTemplate.fromTemplate( \"What is\na good name for a company that makes","metadata":{"source":"https://js.langchain.com/docs/modules/chains/","title":"Chains | 🦜️🔗 Langchain","description":"Using an LLM in isolation is fine for simple applications,","language":"en","loc":{"lines":{"from":139,"to":167}}}}],["280",{"pageContent":"can construct an LLMChain from a PromptTemplate and an LLM. const model = new\nOpenAI({ temperature: 0 }); const prompt = PromptTemplate.fromTemplate( \"What is\na good name for a company that makes {product}?\" ); We can now create a very\nsimple chain that will take user input, format the prompt with it, and then send\nit to the LLM. const chain = new LLMChain({ llm: model, prompt }); // Since this\nLLMChain is a single-input, single-output chain, we can also `run` it. // This\nconvenience method takes in a string and returns the value // of the output key\nfield in the chain response. For LLMChains, this defaults to \"text\". const res =\nawait chain.run(\"colorful socks\"); console.log({ res }); // { res:\n\"\\n\\nSocktastic!\" } If there are multiple variables, you can input them all at\nonce using a dictionary. This will return the complete chain response. const\nprompt = PromptTemplate.fromTemplate( \"What is a good name for {company} that\nmakes {product}?\" ); const chain = new","metadata":{"source":"https://js.langchain.com/docs/modules/chains/","title":"Chains | 🦜️🔗 Langchain","description":"Using an LLM in isolation is fine for simple applications,","language":"en","loc":{"lines":{"from":167,"to":197}}}}],["281",{"pageContent":"once using a dictionary. This will return the complete chain response. const\nprompt = PromptTemplate.fromTemplate( \"What is a good name for {company} that\nmakes {product}?\" ); const chain = new LLMChain({ llm: model, prompt }); const\nres = await chain.call({ company: \"a startup\", product: \"colorful socks\" });\nconsole.log({ res }); // { res: { text: '\\n\\Socktopia Colourful Creations.' } }\nYou can use a chat model in an LLMChain as well: import { ChatPromptTemplate }\nfrom \"langchain/prompts\"; import { LLMChain } from \"langchain/chains\"; import {\nChatOpenAI } from \"langchain/chat_models/openai\"; // We can also construct an\nLLMChain from a ChatPromptTemplate and a chat model. const chat = new\nChatOpenAI({ temperature: 0 }); const chatPrompt =\nChatPromptTemplate.fromMessages([ [ \"system\", \"You are a helpful assistant that\ntranslates {input_language} to {output_language}.\", ], [\"human\", \"{text}\"], ]);\nconst chainB = new LLMChain({ prompt: chatPrompt, llm:","metadata":{"source":"https://js.langchain.com/docs/modules/chains/","title":"Chains | 🦜️🔗 Langchain","description":"Using an LLM in isolation is fine for simple applications,","language":"en","loc":{"lines":{"from":197,"to":233}}}}],["282",{"pageContent":"[ \"system\", \"You are a helpful assistant that translates {input_language} to\n{output_language}.\", ], [\"human\", \"{text}\"], ]); const chainB = new LLMChain({\nprompt: chatPrompt, llm: chat, }); const resB = await chainB.call({\ninput_language: \"English\", output_language: \"French\", text: \"I love\nprogramming.\", }); console.log({ resB }); // { resB: { text: \"J'adore la\nprogrammation.\" } } API REFERENCE: * ChatPromptTemplate\n[/docs/api/prompts/classes/ChatPromptTemplate] from langchain/prompts * LLMChain\n[/docs/api/chains/classes/LLMChain] from langchain/chains * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai Previous Neo4j\n[/docs/modules/data_connection/experimental/graph_databases/neo4j] Next How to\n[/docs/modules/chains/how_to/] * Why do we need chains? * Get started Community\n* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python","metadata":{"source":"https://js.langchain.com/docs/modules/chains/","title":"Chains | 🦜️🔗 Langchain","description":"Using an LLM in isolation is fine for simple applications,","language":"en","loc":{"lines":{"from":233,"to":275}}}}],["283",{"pageContent":"to [/docs/modules/chains/how_to/] * Why do we need chains? * Get started\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/chains/","title":"Chains | 🦜️🔗 Langchain","description":"Using an LLM in isolation is fine for simple applications,","language":"en","loc":{"lines":{"from":275,"to":292}}}}],["284",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * How to [/docs/modules/chains/how_to/] * Debugging\nchains","metadata":{"source":"https://js.langchain.com/docs/modules/chains/how_to/","title":"How to | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["285",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * How to\n[/docs/modules/chains/how_to/] * Debugging chains\n[/docs/modules/chains/how_to/debugging] * Adding memory (state)\n[/docs/modules/chains/how_to/memory] * Foundational\n[/docs/modules/chains/foundational/] * Documents\n[/docs/modules/chains/document/] * Popular [/docs/modules/chains/popular/] *\nAdditional [/docs/modules/chains/additional/] * Memory [/docs/modules/memory/] *\nAgents [/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules","metadata":{"source":"https://js.langchain.com/docs/modules/chains/how_to/","title":"How to | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":24,"to":48}}}}],["286",{"pageContent":"*\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Chains\n[/docs/modules/chains/] * How to HOW TO 📄️ DEBUGGING CHAINS It can be hard to\ndebug a Chain object solely from its output as most Chain objects involve a fair\namount of input prompt preprocessing and LLM output post-processing.\n[/docs/modules/chains/how_to/debugging] 📄️ ADDING MEMORY (STATE) Chains can be\ninitialized with a Memory object, which will persist data across calls to the\nchain. This makes a Chain stateful. [/docs/modules/chains/how_to/memory]\nPrevious Chains [/docs/modules/chains/] Next Debugging chains\n[/docs/modules/chains/how_to/debugging] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More *","metadata":{"source":"https://js.langchain.com/docs/modules/chains/how_to/","title":"How to | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":48,"to":89}}}}],["287",{"pageContent":"[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/chains/how_to/","title":"How to | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":89,"to":100}}}}],["288",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * How to [/docs/modules/chains/how_to/] * Foundational","metadata":{"source":"https://js.langchain.com/docs/modules/chains/foundational/","title":"Foundational | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["289",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * How to\n[/docs/modules/chains/how_to/] * Foundational\n[/docs/modules/chains/foundational/] * LLM\n[/docs/modules/chains/foundational/llm_chain] * Sequential\n[/docs/modules/chains/foundational/sequential_chains] * Documents\n[/docs/modules/chains/document/] * Popular [/docs/modules/chains/popular/] *\nAdditional [/docs/modules/chains/additional/] * Memory [/docs/modules/memory/] *\nAgents [/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules","metadata":{"source":"https://js.langchain.com/docs/modules/chains/foundational/","title":"Foundational | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":24,"to":48}}}}],["290",{"pageContent":"*\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Chains\n[/docs/modules/chains/] * Foundational FOUNDATIONAL 📄️ LLM An LLMChain is a\nsimple chain that adds some functionality around language models. It is used\nwidely throughout LangChain, including in other chains and agents.\n[/docs/modules/chains/foundational/llm_chain] 📄️ SEQUENTIAL The next step after\ncalling a language model is make a series of calls to a language model. This is\nparticularly useful when you want to take the output from one call and use it as\nthe input to another. [/docs/modules/chains/foundational/sequential_chains]\nPrevious Adding memory (state) [/docs/modules/chains/how_to/memory] Next LLM\n[/docs/modules/chains/foundational/llm_chain] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python","metadata":{"source":"https://js.langchain.com/docs/modules/chains/foundational/","title":"Foundational | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":48,"to":86}}}}],["291",{"pageContent":"* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/chains/foundational/","title":"Foundational | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":86,"to":97}}}}],["292",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * How to [/docs/modules/chains/how_to/] * Foundational","metadata":{"source":"https://js.langchain.com/docs/modules/chains/document/","title":"Documents | 🦜️🔗 Langchain","description":"These are the core chains for working with Documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more.","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["293",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * How to\n[/docs/modules/chains/how_to/] * Foundational\n[/docs/modules/chains/foundational/] * Documents\n[/docs/modules/chains/document/] * Stuff [/docs/modules/chains/document/stuff] *\nRefine [/docs/modules/chains/document/refine] * Map reduce\n[/docs/modules/chains/document/map_reduce] * Popular\n[/docs/modules/chains/popular/] * Additional [/docs/modules/chains/additional/]\n* Memory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference","metadata":{"source":"https://js.langchain.com/docs/modules/chains/document/","title":"Documents | 🦜️🔗 Langchain","description":"These are the core chains for working with Documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more.","language":"en","loc":{"lines":{"from":24,"to":46}}}}],["294",{"pageContent":"* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Chains\n[/docs/modules/chains/] * Documents DOCUMENTS These are the core chains for\nworking with Documents. They are useful for summarizing documents, answering\nquestions over documents, extracting information from documents, and more. These\nchains are all loaded in a similar way: import { OpenAI } from\n\"langchain/llms/openai\"; import { loadQAStuffChain, loadQAMapReduceChain,\nloadQARefineChain } from \"langchain/chains\"; import { Document } from\n\"langchain/document\"; // This first example uses the `StuffDocumentsChain`.\nconst llmA = new OpenAI({}); const chainA = loadQAStuffChain(llmA); const docs =\n[ new Document({ pageContent: \"Harrison went to Harvard.\" }), new Document({\npageContent: \"Ankush went to Princeton.\" }), ]; const","metadata":{"source":"https://js.langchain.com/docs/modules/chains/document/","title":"Documents | 🦜️🔗 Langchain","description":"These are the core chains for working with Documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more.","language":"en","loc":{"lines":{"from":46,"to":80}}}}],["295",{"pageContent":"OpenAI({}); const chainA = loadQAStuffChain(llmA); const docs = [ new Document({\npageContent: \"Harrison went to Harvard.\" }), new Document({ pageContent: \"Ankush\nwent to Princeton.\" }), ]; const resA = await chainA.call({ input_documents:\ndocs, question: \"Where did Harrison go to college?\", }); console.log({ resA });\n// { resA: { text: ' Harrison went to Harvard.' } } // This second example uses\nthe `MapReduceChain`. // Optionally limit the number of concurrent requests to\nthe language model. const llmB = new OpenAI({ maxConcurrency: 10 }); const\nchainB = loadQAMapReduceChain(llmB); const resB = await chainB.call({\ninput_documents: docs, question: \"Where did Harrison go to college?\", });\nconsole.log({ resB }); // { resB: { text: ' Harrison went to Harvard.' } } 📄️\nSTUFF The stuff documents chain (\"stuff\" as in \"to stuff\" or \"to fill\") is the\nmost straightforward of the document chains. It takes a list of documents,\ninserts them all into a prompt and passes that","metadata":{"source":"https://js.langchain.com/docs/modules/chains/document/","title":"Documents | 🦜️🔗 Langchain","description":"These are the core chains for working with Documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more.","language":"en","loc":{"lines":{"from":80,"to":111}}}}],["296",{"pageContent":"stuff documents chain (\"stuff\" as in \"to stuff\" or \"to fill\") is the most\nstraightforward of the document chains. It takes a list of documents, inserts\nthem all into a prompt and passes that prompt to an LLM.\n[/docs/modules/chains/document/stuff] 📄️ REFINE The refine documents chain\nconstructs a response by looping over the input documents and iteratively\nupdating its answer. For each document, it passes all non-document inputs, the\ncurrent document, and the latest intermediate answer to an LLM chain to get a\nnew answer. [/docs/modules/chains/document/refine] 📄️ MAP REDUCE The map reduce\ndocuments chain first applies an LLM chain to each document individually (the\nMap step), treating the chain output as a new document. It then passes all the\nnew documents to a separate combine documents chain to get a single output (the\nReduce step). It can optionally first compress, or collapse, the mapped\ndocuments to make sure that they fit in the combine documents chain (which will\noften","metadata":{"source":"https://js.langchain.com/docs/modules/chains/document/","title":"Documents | 🦜️🔗 Langchain","description":"These are the core chains for working with Documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more.","language":"en","loc":{"lines":{"from":111,"to":131}}}}],["297",{"pageContent":"chain to get a single output (the Reduce step). It can optionally first\ncompress, or collapse, the mapped documents to make sure that they fit in the\ncombine documents chain (which will often pass them to an LLM). This compression\nstep is performed recursively if necessary.\n[/docs/modules/chains/document/map_reduce] Previous Sequential\n[/docs/modules/chains/foundational/sequential_chains] Next Stuff\n[/docs/modules/chains/document/stuff] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/chains/document/","title":"Documents | 🦜️🔗 Langchain","description":"These are the core chains for working with Documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more.","language":"en","loc":{"lines":{"from":131,"to":154}}}}],["298",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * How to [/docs/modules/chains/how_to/] * Foundational","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/","title":"Popular | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["299",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * How to\n[/docs/modules/chains/how_to/] * Foundational\n[/docs/modules/chains/foundational/] * Documents\n[/docs/modules/chains/document/] * Popular [/docs/modules/chains/popular/] * API\nchains [/docs/modules/chains/popular/api] * Retrieval QA\n[/docs/modules/chains/popular/vector_db_qa] * Conversational Retrieval QA\n[/docs/modules/chains/popular/chat_vector_db] * SQL\n[/docs/modules/chains/popular/sqlite] * Structured Output with OpenAI functions\n[/docs/modules/chains/popular/structured_output] * Summarization\n[/docs/modules/chains/popular/summarize] * Additional\n[/docs/modules/chains/additional/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/","title":"Popular | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":24,"to":44}}}}],["300",{"pageContent":"* Memory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Chains\n[/docs/modules/chains/] * Popular POPULAR 📄️ API CHAINS APIChain enables using\nLLMs to interact with APIs to retrieve relevant information. Construct the chain\nby providing a question relevant to the provided API documentation.\n[/docs/modules/chains/popular/api] 📄️ RETRIEVAL QA This example showcases\nquestion answering over an index. [/docs/modules/chains/popular/vector_db_qa]\n📄️ CONVERSATIONAL RETRIEVAL QA The ConversationalRetrievalQA chain builds on\nRetrievalQAChain to","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/","title":"Popular | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":44,"to":83}}}}],["301",{"pageContent":"example showcases question answering over an index.\n[/docs/modules/chains/popular/vector_db_qa] 📄️ CONVERSATIONAL RETRIEVAL QA The\nConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat\nhistory component. [/docs/modules/chains/popular/chat_vector_db] 📄️ SQL This\nexample demonstrates the use of the SQLDatabaseChain for answering questions\nover a SQL database. [/docs/modules/chains/popular/sqlite] 📄️ STRUCTURED OUTPUT\nWITH OPENAI FUNCTIONS Must be used with an OpenAI functions model.\n[/docs/modules/chains/popular/structured_output] 📄️ SUMMARIZATION A\nsummarization chain can be used to summarize multiple documents. One way is to\ninput multiple smaller documents, after they have been divided into chunks, and\noperate over them with a MapReduceDocumentsChain. You can also choose instead\nfor the chain that does summarization to be a StuffDocumentsChain, or a\nRefineDocumentsChain. [/docs/modules/chains/popular/summarize] Previous Map","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/","title":"Popular | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":83,"to":117}}}}],["302",{"pageContent":"You can also choose instead for the chain that does summarization to be a\nStuffDocumentsChain, or a RefineDocumentsChain.\n[/docs/modules/chains/popular/summarize] Previous Map reduce\n[/docs/modules/chains/document/map_reduce] Next API chains\n[/docs/modules/chains/popular/api] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/","title":"Popular | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":117,"to":139}}}}],["303",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * How to [/docs/modules/chains/how_to/] * Foundational","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/","title":"Additional | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["304",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * How to\n[/docs/modules/chains/how_to/] * Foundational\n[/docs/modules/chains/foundational/] * Documents\n[/docs/modules/chains/document/] * Popular [/docs/modules/chains/popular/] *\nAdditional [/docs/modules/chains/additional/] * OpenAI functions chains\n[/docs/modules/chains/additional/openai_functions/] * Analyze Document\n[/docs/modules/chains/additional/analyze_document] * Self-critique chain with\nconstitutional AI [/docs/modules/chains/additional/constitutional_chain] * Neo4j\nCypher graph QA [/docs/modules/chains/additional/cypher_chain] * Moderation\n[/docs/modules/chains/additional/moderation] * Dynamically selecting from\nmultiple prompts [/docs/modules/chains/additional/multi_prompt_router] *\nDynamically selecting from multiple retrievers","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/","title":"Additional | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":24,"to":38}}}}],["305",{"pageContent":"* Dynamically selecting from multiple prompts\n[/docs/modules/chains/additional/multi_prompt_router] * Dynamically selecting\nfrom multiple retrievers\n[/docs/modules/chains/additional/multi_retrieval_qa_router] * Memory\n[/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Chains\n[/docs/modules/chains/] * Additional ADDITIONAL 🗃️ OPENAI FUNCTIONS CHAINS 3\nitems [/docs/modules/chains/additional/openai_functions/] 📄️ ANALYZE DOCUMENT\nThe AnalyzeDocumentChain can be used as an end-to-end to chain. This chain takes\nin a single document,","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/","title":"Additional | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":38,"to":71}}}}],["306",{"pageContent":"CHAINS 3 items [/docs/modules/chains/additional/openai_functions/] 📄️ ANALYZE\nDOCUMENT The AnalyzeDocumentChain can be used as an end-to-end to chain. This\nchain takes in a single document, splits it up, and then runs it through a\nCombineDocumentsChain. [/docs/modules/chains/additional/analyze_document] 📄️\nSELF-CRITIQUE CHAIN WITH CONSTITUTIONAL AI The ConstitutionalChain is a chain\nthat ensures the output of a language model adheres to a predefined set of\nconstitutional principles. By incorporating specific rules and guidelines, the\nConstitutionalChain filters and modifies the generated content to align with\nthese principles, thus providing more controlled, ethical, and contextually\nappropriate responses. This mechanism helps maintain the integrity of the output\nwhile minimizing the risk of generating content that may violate guidelines, be\noffensive, or deviate from the desired context.\n[/docs/modules/chains/additional/constitutional_chain] 📄️ NEO4J CYPHER GRAPH","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/","title":"Additional | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":71,"to":97}}}}],["307",{"pageContent":"the risk of generating content that may violate guidelines, be offensive, or\ndeviate from the desired context.\n[/docs/modules/chains/additional/constitutional_chain] 📄️ NEO4J CYPHER GRAPH QA\nThis example uses Neo4j database, which is a native graph database.\n[/docs/modules/chains/additional/cypher_chain] 📄️ MODERATION This notebook\nwalks through examples of how to use a moderation chain, and several common ways\nfor doing so. Moderation chains are useful for detecting text that could be\nhateful, violent, etc. This can be useful to apply on both user input, but also\non the output of a Language Model. Some API providers, like OpenAI, specifically\nprohibit you, or your end users, from generating some types of harmful content.\nTo comply with this (and to just generally prevent your application from being\nharmful) you may often want to append a moderation chain to any LLMChains, in\norder to make sure any output the LLM generates is not","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/","title":"Additional | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":97,"to":116}}}}],["308",{"pageContent":"this (and to just generally prevent your application from being harmful) you may\noften want to append a moderation chain to any LLMChains, in order to make sure\nany output the LLM generates is not harmful.\n[/docs/modules/chains/additional/moderation] 📄️ DYNAMICALLY SELECTING FROM\nMULTIPLE PROMPTS This notebook demonstrates how to use the RouterChain paradigm\nto create a chain that dynamically selects the prompt to use for a given input.\nSpecifically we show how to use the MultiPromptChain to create a\nquestion-answering chain that selects the prompt which is most relevant for a\ngiven question, and then answers the question using that prompt.\n[/docs/modules/chains/additional/multi_prompt_router] 📄️ DYNAMICALLY SELECTING\nFROM MULTIPLE RETRIEVERS This notebook demonstrates how to use the RouterChain\nparadigm to create a chain that dynamically selects which Retrieval system to\nuse. Specifically we show how to use the MultiRetrievalQAChain to create a\nquestion-answering chain that","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/","title":"Additional | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":116,"to":134}}}}],["309",{"pageContent":"the RouterChain paradigm to create a chain that dynamically selects which\nRetrieval system to use. Specifically we show how to use the\nMultiRetrievalQAChain to create a question-answering chain that selects the\nretrieval QA chain which is most relevant for a given question, and then answers\nthe question using it.\n[/docs/modules/chains/additional/multi_retrieval_qa_router] Previous\nSummarization [/docs/modules/chains/popular/summarize] Next OpenAI functions\nchains [/docs/modules/chains/additional/openai_functions/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/","title":"Additional | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":134,"to":157}}}}],["310",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * Document\ntransformers","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/graph_databases/neo4j","title":"Neo4j | 🦜️🔗 Langchain","description":"Setup","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["311",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * Document transformers\n[/docs/modules/data_connection/document_transformers/] * Text embedding models\n[/docs/modules/data_connection/text_embedding/] * Vector stores\n[/docs/modules/data_connection/vectorstores/] * Retrievers\n[/docs/modules/data_connection/retrievers/] * Experimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Multimodal embedding models\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Graph databases\n[/docs/modules/data_connection/experimental/graph_databases/neo4j] * Neo4j\n[/docs/modules/data_connection/experimental/graph_databases/neo4j] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/graph_databases/neo4j","title":"Neo4j | 🦜️🔗 Langchain","description":"Setup","language":"en","loc":{"lines":{"from":23,"to":37}}}}],["312",{"pageContent":"* Neo4j [/docs/modules/data_connection/experimental/graph_databases/neo4j] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\n[/docs/modules/data_connection/] * Experimental * Graph databases * Neo4j On\nthis page NEO4J SETUP Install the dependencies needed for Neo4j: * npm * Yarn *\npnpm npm install neo4j-driver yarn add neo4j-driver pnpm add neo4j-driver USAGE\nThis walkthrough uses Neo4j to demonstrate a graph database integration.\nINSTANTIATE A GRAPH AND RETRIEVE","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/graph_databases/neo4j","title":"Neo4j | 🦜️🔗 Langchain","description":"Setup","language":"en","loc":{"lines":{"from":37,"to":94}}}}],["313",{"pageContent":"install neo4j-driver yarn add neo4j-driver pnpm add neo4j-driver USAGE This\nwalkthrough uses Neo4j to demonstrate a graph database integration. INSTANTIATE\nA GRAPH AND RETRIEVE INFORMATION THE THE GRAPH BY GENERATING CYPHER QUERY\nLANGUAGE STATEMENTS USING GRAPHCYPHERQACHAIN. import { Neo4jGraph } from\n\"langchain/graphs/neo4j_graph\"; import { OpenAI } from \"langchain/llms/openai\";\nimport { GraphCypherQAChain } from \"langchain/chains/graph_qa/cypher\"; /** *\nThis example uses Neo4j database, which is native graph database. * To set it up\nfollow the instructions on\nhttps://neo4j.com/docs/operations-manual/current/installation/. */ const url =\n\"bolt://localhost:7687\"; const username = \"neo4j\"; const password =\n\"pleaseletmein\"; const graph = await Neo4jGraph.initialize({ url, username,\npassword }); const model = new OpenAI({ temperature: 0 }); // Populate the\ndatabase with two nodes and a relationship await graph.query( \"CREATE (a:Actor\n{name:'Bruce Willis'})\" +","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/graph_databases/neo4j","title":"Neo4j | 🦜️🔗 Langchain","description":"Setup","language":"en","loc":{"lines":{"from":94,"to":136}}}}],["314",{"pageContent":"username, password }); const model = new OpenAI({ temperature: 0 }); // Populate\nthe database with two nodes and a relationship await graph.query( \"CREATE\n(a:Actor {name:'Bruce Willis'})\" + \"-[:ACTED_IN]->(:Movie {title: 'Pulp\nFiction'})\" ); const chain = GraphCypherQAChain.fromLLM({ llm: model, graph, });\nconst res = await chain.run(\"Who played in Pulp Fiction?\"); console.log(res); //\nBruce Willis played in Pulp Fiction. API REFERENCE: * Neo4jGraph\n[/docs/api/graphs_neo4j_graph/classes/Neo4jGraph] from\nlangchain/graphs/neo4j_graph * OpenAI [/docs/api/llms_openai/classes/OpenAI]\nfrom langchain/llms/openai * GraphCypherQAChain\n[/docs/api/chains_graph_qa_cypher/classes/GraphCypherQAChain] from\nlangchain/chains/graph_qa/cypher Previous Google Vertex AI\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\nNext Chains [/docs/modules/chains/] * Setup * Usage * Instantiate a graph and\nretrieve information the the graph by generating","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/graph_databases/neo4j","title":"Neo4j | 🦜️🔗 Langchain","description":"Setup","language":"en","loc":{"lines":{"from":136,"to":171}}}}],["315",{"pageContent":"* Setup * Usage * Instantiate a graph and retrieve information the the graph by\ngenerating Cypher query language statements using GraphCypherQAChain. Community\n* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/experimental/graph_databases/neo4j","title":"Neo4j | 🦜️🔗 Langchain","description":"Setup","language":"en","loc":{"lines":{"from":171,"to":188}}}}],["316",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * How-to\n[/docs/modules/memory/how_to/buffer] *","metadata":{"source":"https://js.langchain.com/docs/modules/memory/","title":"Memory | 🦜️🔗 Langchain","description":"🚧 Docs under construction 🚧","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["317",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * How-to [/docs/modules/memory/how_to/buffer] *\nIntegrations [/docs/modules/memory/integrations/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Memory On this\npage MEMORY 🚧 Docs under construction 🚧 By default, Chains and Agents are\nstateless, meaning that they treat each incoming query independently (like the\nunderlying LLMs and chat models themselves). In some applications, like\nchatbots, it is essential to remember previous interactions,","metadata":{"source":"https://js.langchain.com/docs/modules/memory/","title":"Memory | 🦜️🔗 Langchain","description":"🚧 Docs under construction 🚧","language":"en","loc":{"lines":{"from":25,"to":54}}}}],["318",{"pageContent":"that they treat each incoming query independently (like the underlying LLMs and\nchat models themselves). In some applications, like chatbots, it is essential to\nremember previous interactions, both in the short and long-term. The Memory\nclass does exactly that. LangChain provides memory components in two forms.\nFirst, LangChain provides helper utilities for managing and manipulating\nprevious chat messages. These are designed to be modular and useful regardless\nof how they are used. Secondly, LangChain provides easy ways to incorporate\nthese utilities into chains. GET STARTED Memory involves keeping a concept of\nstate around throughout a user's interactions with a language model. A user's\ninteractions with a language model are captured in the concept of ChatMessages,\nso this boils down to ingesting, capturing, transforming and extracting\nknowledge from a sequence of chat messages. There are many different ways to do\nthis, each of which exists as its own memory type. In general,","metadata":{"source":"https://js.langchain.com/docs/modules/memory/","title":"Memory | 🦜️🔗 Langchain","description":"🚧 Docs under construction 🚧","language":"en","loc":{"lines":{"from":54,"to":70}}}}],["319",{"pageContent":"to ingesting, capturing, transforming and extracting knowledge from a sequence\nof chat messages. There are many different ways to do this, each of which exists\nas its own memory type. In general, for each type of memory there are two ways\nto understanding using memory. These are the standalone functions which extract\ninformation from a sequence of messages, and then there is the way you can use\nthis type of memory in a chain. Memory can return multiple pieces of information\n(for example, the most recent N messages and a summary of all previous\nmessages). The returned information can either be a string or a list of\nmessages. We will walk through the simplest form of memory: \"buffer\" memory,\nwhich just involves keeping a buffer of all prior messages. We will show how to\nuse the modular utility functions here, then show how it can be used in a chain\n(both returning a string as well as a list of messages). CHATMESSAGEHISTORY One\nof the core utility classes underpinning most (if not","metadata":{"source":"https://js.langchain.com/docs/modules/memory/","title":"Memory | 🦜️🔗 Langchain","description":"🚧 Docs under construction 🚧","language":"en","loc":{"lines":{"from":70,"to":87}}}}],["320",{"pageContent":"functions here, then show how it can be used in a chain (both returning a string\nas well as a list of messages). CHATMESSAGEHISTORY One of the core utility\nclasses underpinning most (if not all) memory modules is the ChatMessageHistory\nclass. This is a super lightweight wrapper which exposes convenience methods for\nsaving Human messages, AI messages, and then fetching them all. Subclassing this\nclass allows you to use different storage solutions, such as Redis, to keep\npersistent chat message histories. import { ChatMessageHistory } from\n\"langchain/memory\"; const history = new ChatMessageHistory(); await\nhistory.addUserMessage(\"Hi!\"); await history.addAIChatMessage(\"What's up?\");\nconst messages = await history.getMessages(); console.log(messages); /* [\nHumanMessage { content: 'Hi!', }, AIMessage { content: \"What's up?\", } ] */ You\ncan also load messages into memory instances by creating and passing in a\nChatHistory object. This lets you","metadata":{"source":"https://js.langchain.com/docs/modules/memory/","title":"Memory | 🦜️🔗 Langchain","description":"🚧 Docs under construction 🚧","language":"en","loc":{"lines":{"from":87,"to":124}}}}],["321",{"pageContent":"content: 'Hi!', }, AIMessage { content: \"What's up?\", } ] */ You can also load\nmessages into memory instances by creating and passing in a ChatHistory object.\nThis lets you easily pick up state from past conversations. In addition to the\nabove technique, you can do: import { BufferMemory, ChatMessageHistory } from\n\"langchain/memory\"; import { HumanChatMessage, AIChatMessage } from\n\"langchain/schema\"; const pastMessages = [ new HumanMessage(\"My name's Jonas\"),\nnew AIMessage(\"Nice to meet you, Jonas!\"), ]; const memory = new BufferMemory({\nchatHistory: new ChatMessageHistory(pastMessages), }); note Do not share the\nsame history or memory instance between two different chains, a memory instance\nrepresents the history of a single conversation note If you deploy your\nLangChain app on a serverless environment do not store memory instances in a\nvariable, as your hosting provider may have reset it by the next time the\nfunction is","metadata":{"source":"https://js.langchain.com/docs/modules/memory/","title":"Memory | 🦜️🔗 Langchain","description":"🚧 Docs under construction 🚧","language":"en","loc":{"lines":{"from":124,"to":160}}}}],["322",{"pageContent":"you deploy your LangChain app on a serverless environment do not store memory\ninstances in a variable, as your hosting provider may have reset it by the next\ntime the function is called. BUFFERMEMORY We now show how to use this simple\nconcept in a chain. We first showcase BufferMemory, a wrapper around\nChatMessageHistory that extracts the messages into an input variable. import {\nOpenAI } from \"langchain/llms/openai\"; import { BufferMemory } from\n\"langchain/memory\"; import { ConversationChain } from \"langchain/chains\"; const\nmodel = new OpenAI({}); const memory = new BufferMemory(); // This chain is\npreconfigured with a default prompt const chain = new ConversationChain({ llm:\nmodel, memory: memory }); const res1 = await chain.call({ input: \"Hi! I'm Jim.\"\n}); console.log({ res1 }); {response: \" Hi Jim! It's nice to meet you. My name\nis AI. What would you like to talk about?\"} const res2 = await chain.call({\ninput: \"What's my name?\" }); console.log({ res2","metadata":{"source":"https://js.langchain.com/docs/modules/memory/","title":"Memory | 🦜️🔗 Langchain","description":"🚧 Docs under construction 🚧","language":"en","loc":{"lines":{"from":160,"to":189}}}}],["323",{"pageContent":"res1 }); {response: \" Hi Jim! It's nice to meet you. My name is AI. What would\nyou like to talk about?\"} const res2 = await chain.call({ input: \"What's my\nname?\" }); console.log({ res2 }); {response: ' You said your name is Jim. Is\nthere anything else you would like to talk about?'} There are plenty of\ndifferent types of memory, check out our examples to see more! CREATING YOUR OWN\nMEMORY CLASS The BaseMemory interface has two methods: export type InputValues =\nRecord; export type OutputValues = Record; interface BaseMemory {\nloadMemoryVariables(values: InputValues): Promise; saveContext( inputValues:\nInputValues, outputValues: OutputValues ): Promise; } To implement your own\nmemory class you have two options: SUBCLASSING BASECHATMEMORY This is the\neasiest way to implement your own memory class. You can subclass BaseChatMemory,\nwhich takes care of saveContext by saving inputs and outputs as","metadata":{"source":"https://js.langchain.com/docs/modules/memory/","title":"Memory | 🦜️🔗 Langchain","description":"🚧 Docs under construction 🚧","language":"en","loc":{"lines":{"from":189,"to":239}}}}],["324",{"pageContent":"two options: SUBCLASSING BASECHATMEMORY This is the easiest way to implement\nyour own memory class. You can subclass BaseChatMemory, which takes care of\nsaveContext by saving inputs and outputs as Chat Messages\n[/docs/api/schema/classes/BaseMessage], and implement only the\nloadMemoryVariables method. This method is responsible for returning the memory\nvariables that are relevant for the current input values. abstract class\nBaseChatMemory extends BaseMemory { chatHistory: ChatMessageHistory; abstract\nloadMemoryVariables(values: InputValues): Promise; } SUBCLASSING BASEMEMORY If\nyou want to implement a more custom memory class, you can subclass BaseMemory\nand implement both loadMemoryVariables and saveContext methods. The saveContext\nmethod is responsible for storing the input and output values in memory. The\nloadMemoryVariables method is responsible for returning the memory variables\nthat are relevant for the current input values. abstract class","metadata":{"source":"https://js.langchain.com/docs/modules/memory/","title":"Memory | 🦜️🔗 Langchain","description":"🚧 Docs under construction 🚧","language":"en","loc":{"lines":{"from":239,"to":264}}}}],["325",{"pageContent":"for storing the input and output values in memory. The loadMemoryVariables\nmethod is responsible for returning the memory variables that are relevant for\nthe current input values. abstract class BaseMemory { abstract\nloadMemoryVariables(values: InputValues): Promise; abstract saveContext(\ninputValues: InputValues, outputValues: OutputValues ): Promise; } Previous\nDynamically selecting from multiple retrievers\n[/docs/modules/chains/additional/multi_retrieval_qa_router] Next Conversation\nbuffer memory [/docs/modules/memory/how_to/buffer] * Get started Community *\nDiscord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/memory/","title":"Memory | 🦜️🔗 Langchain","description":"🚧 Docs under construction 🚧","language":"en","loc":{"lines":{"from":264,"to":298}}}}],["326",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * How-to\n[/docs/modules/memory/how_to/buffer] *","metadata":{"source":"https://js.langchain.com/docs/modules/memory/how_to/buffer","title":"Conversation buffer memory | 🦜️🔗 Langchain","description":"This notebook shows how to use BufferMemory. This memory allows for storing of messages, then later formats the messages into a prompt input variable.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["327",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * How-to [/docs/modules/memory/how_to/buffer] *\nConversation buffer memory [/docs/modules/memory/how_to/buffer] * Using Buffer\nMemory with Chat Models [/docs/modules/memory/how_to/buffer_memory_chat] *\nConversation buffer window memory [/docs/modules/memory/how_to/buffer_window] *\nBuffer Window Memory [/docs/modules/memory/how_to/buffer_window_memory] * Entity\nmemory [/docs/modules/memory/how_to/entity_summary_memory] * How to use multiple\nmemory classes in the same chain [/docs/modules/memory/how_to/multiple_memory] *\nConversation summary memory [/docs/modules/memory/how_to/summary] * Conversation\nsummary buffer memory [/docs/modules/memory/how_to/summary_buffer] * Vector\nstore-backed memory [/docs/modules/memory/how_to/vectorstore_retriever_memory] *\nIntegrations","metadata":{"source":"https://js.langchain.com/docs/modules/memory/how_to/buffer","title":"Conversation buffer memory | 🦜️🔗 Langchain","description":"This notebook shows how to use BufferMemory. This memory allows for storing of messages, then later formats the messages into a prompt input variable.","language":"en","loc":{"lines":{"from":25,"to":38}}}}],["328",{"pageContent":"* Conversation summary buffer memory\n[/docs/modules/memory/how_to/summary_buffer] * Vector store-backed memory\n[/docs/modules/memory/how_to/vectorstore_retriever_memory] * Integrations\n[/docs/modules/memory/integrations/] * Agents [/docs/modules/agents/] *\nCallbacks [/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides\n[/docs/guides] * Ecosystem [/docs/ecosystem] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Memory\n[/docs/modules/memory/] * How-to * Conversation buffer memory CONVERSATION\nBUFFER MEMORY This notebook shows how to use BufferMemory. This memory allows\nfor storing of messages, then later formats the messages into a prompt input\nvariable. We can first extract it as a string. import {","metadata":{"source":"https://js.langchain.com/docs/modules/memory/how_to/buffer","title":"Conversation buffer memory | 🦜️🔗 Langchain","description":"This notebook shows how to use BufferMemory. This memory allows for storing of messages, then later formats the messages into a prompt input variable.","language":"en","loc":{"lines":{"from":38,"to":67}}}}],["329",{"pageContent":"notebook shows how to use BufferMemory. This memory allows for storing of\nmessages, then later formats the messages into a prompt input variable. We can\nfirst extract it as a string. import { OpenAI } from \"langchain/llms/openai\";\nimport { BufferMemory } from \"langchain/memory\"; import { ConversationChain }\nfrom \"langchain/chains\"; const model = new OpenAI({}); const memory = new\nBufferMemory(); const chain = new ConversationChain({ llm: model, memory: memory\n}); const res1 = await chain.call({ input: \"Hi! I'm Jim.\" }); console.log({ res1\n}); {response: \" Hi Jim! It's nice to meet you. My name is AI. What would you\nlike to talk about?\"} const res2 = await chain.call({ input: \"What's my name?\"\n}); console.log({ res2 }); {response: ' You said your name is Jim. Is there\nanything else you would like to talk about?'} You can also load messages into a\nBufferMemory instance by creating and passing in a ChatHistory object. This lets\nyou easily pick up state from past","metadata":{"source":"https://js.langchain.com/docs/modules/memory/how_to/buffer","title":"Conversation buffer memory | 🦜️🔗 Langchain","description":"This notebook shows how to use BufferMemory. This memory allows for storing of messages, then later formats the messages into a prompt input variable.","language":"en","loc":{"lines":{"from":67,"to":102}}}}],["330",{"pageContent":"anything else you would like to talk about?'} You can also load messages into a\nBufferMemory instance by creating and passing in a ChatHistory object. This lets\nyou easily pick up state from past conversations: import { BufferMemory,\nChatMessageHistory } from \"langchain/memory\"; import { HumanMessage, AIMessage }\nfrom \"langchain/schema\"; const pastMessages = [ new HumanMessage(\"My name's\nJonas\"), new AIMessage(\"Nice to meet you, Jonas!\"), ]; const memory = new\nBufferMemory({ chatHistory: new ChatMessageHistory(pastMessages), }); Previous\nMemory [/docs/modules/memory/] Next Using Buffer Memory with Chat Models\n[/docs/modules/memory/how_to/buffer_memory_chat] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain,","metadata":{"source":"https://js.langchain.com/docs/modules/memory/how_to/buffer","title":"Conversation buffer memory | 🦜️🔗 Langchain","description":"This notebook shows how to use BufferMemory. This memory allows for storing of messages, then later formats the messages into a prompt input variable.","language":"en","loc":{"lines":{"from":102,"to":142}}}}],["331",{"pageContent":"* JS/TS [https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/memory/how_to/buffer","title":"Conversation buffer memory | 🦜️🔗 Langchain","description":"This notebook shows how to use BufferMemory. This memory allows for storing of messages, then later formats the messages into a prompt input variable.","language":"en","loc":{"lines":{"from":142,"to":148}}}}],["332",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * How-to\n[/docs/modules/memory/how_to/buffer] *","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/","title":"Examples: Memory | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["333",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * How-to [/docs/modules/memory/how_to/buffer] *\nIntegrations [/docs/modules/memory/integrations/] * Cloudflare D1-Backed Chat\nMemory [/docs/modules/memory/integrations/cloudflare_d1] * DynamoDB-Backed Chat\nMemory [/docs/modules/memory/integrations/dynamodb] * Firestore Chat Memory\n[/docs/modules/memory/integrations/firestore] * Momento-Backed Chat Memory\n[/docs/modules/memory/integrations/momento] * MongoDB Chat Memory\n[/docs/modules/memory/integrations/mongodb] * Motörhead Memory\n[/docs/modules/memory/integrations/motorhead_memory] * PlanetScale Chat Memory\n[/docs/modules/memory/integrations/planetscale] * Redis-Backed Chat Memory\n[/docs/modules/memory/integrations/redis] * Upstash Redis-Backed Chat Memory\n[/docs/modules/memory/integrations/upstash_redis] * Xata Chat Memory","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/","title":"Examples: Memory | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":25,"to":39}}}}],["334",{"pageContent":"* Redis-Backed Chat Memory [/docs/modules/memory/integrations/redis] * Upstash\nRedis-Backed Chat Memory [/docs/modules/memory/integrations/upstash_redis] *\nXata Chat Memory [/docs/modules/memory/integrations/xata] * Zep Memory\n[/docs/modules/memory/integrations/zep_memory] * Agents [/docs/modules/agents/]\n* Callbacks [/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides\n[/docs/guides] * Ecosystem [/docs/ecosystem] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Memory\n[/docs/modules/memory/] * Integrations EXAMPLES: MEMORY 📄️ CLOUDFLARE D1-BACKED\nCHAT MEMORY This integration is only supported in Cloudflare Workers.\n[/docs/modules/memory/integrations/cloudflare_d1] 📄️ DYNAMODB-BACKED","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/","title":"Examples: Memory | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":39,"to":71}}}}],["335",{"pageContent":"MEMORY 📄️ CLOUDFLARE D1-BACKED CHAT MEMORY This integration is only supported\nin Cloudflare Workers. [/docs/modules/memory/integrations/cloudflare_d1] 📄️\nDYNAMODB-BACKED CHAT MEMORY For longer-term persistence across chat sessions,\nyou can swap out the default in-memory chatHistory that backs chat memory\nclasses like BufferMemory for a DynamoDB instance.\n[/docs/modules/memory/integrations/dynamodb] 📄️ FIRESTORE CHAT MEMORY For\nlonger-term persistence across chat sessions, you can swap out the default\nin-memory chatHistory that backs chat memory classes like BufferMemory for a\nfirestore. [/docs/modules/memory/integrations/firestore] 📄️ MOMENTO-BACKED CHAT\nMEMORY For distributed, serverless persistence across chat sessions, you can\nswap in a Momento-backed chat message history.\n[/docs/modules/memory/integrations/momento] 📄️ MONGODB CHAT MEMORY For\nlonger-term persistence across chat sessions, you can swap out the default\nin-memory chatHistory that backs chat","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/","title":"Examples: Memory | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":71,"to":106}}}}],["336",{"pageContent":"history. [/docs/modules/memory/integrations/momento] 📄️ MONGODB CHAT MEMORY For\nlonger-term persistence across chat sessions, you can swap out the default\nin-memory chatHistory that backs chat memory classes like BufferMemory for a\nMongoDB instance. [/docs/modules/memory/integrations/mongodb] 📄️ MOTÖRHEAD\nMEMORY Motörhead is a memory server implemented in Rust. It automatically\nhandles incremental summarization in the background and allows for stateless\napplications. [/docs/modules/memory/integrations/motorhead_memory] 📄️\nPLANETSCALE CHAT MEMORY Because PlanetScale works via a REST API, you can use\nthis with Vercel Edge, Cloudflare Workers and other Serverless environments.\n[/docs/modules/memory/integrations/planetscale] 📄️ REDIS-BACKED CHAT MEMORY For\nlonger-term persistence across chat sessions, you can swap out the default\nin-memory chatHistory that backs chat memory classes like BufferMemory for a\nRedis instance. [/docs/modules/memory/integrations/redis] 📄️","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/","title":"Examples: Memory | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":106,"to":142}}}}],["337",{"pageContent":"across chat sessions, you can swap out the default in-memory chatHistory that\nbacks chat memory classes like BufferMemory for a Redis instance.\n[/docs/modules/memory/integrations/redis] 📄️ UPSTASH REDIS-BACKED CHAT MEMORY\nBecause Upstash Redis works via a REST API, you can use this with Vercel Edge,\nCloudflare Workers and other Serverless environments.\n[/docs/modules/memory/integrations/upstash_redis] 📄️ XATA CHAT MEMORY Xata is a\nserverless data platform, based on PostgreSQL. It provides a type-safe\nTypeScript/JavaScript SDK for interacting with your database, and a\n[/docs/modules/memory/integrations/xata] 📄️ ZEP MEMORY Zep is a memory server\nthat stores, summarizes, embeds, indexes, and enriches conversational AI chat\nhistories, autonomous agent histories, document Q&A histories and exposes them\nvia simple, low-latency APIs. [/docs/modules/memory/integrations/zep_memory]\nPrevious Vector store-backed","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/","title":"Examples: Memory | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":142,"to":171}}}}],["338",{"pageContent":"AI chat histories, autonomous agent histories, document Q&A histories and\nexposes them via simple, low-latency APIs.\n[/docs/modules/memory/integrations/zep_memory] Previous Vector store-backed\nmemory [/docs/modules/memory/how_to/vectorstore_retriever_memory] Next\nCloudflare D1-Backed Chat Memory\n[/docs/modules/memory/integrations/cloudflare_d1] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/","title":"Examples: Memory | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":171,"to":193}}}}],["339",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * How to [/docs/modules/chains/how_to/] * Foundational","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router","title":"Dynamically selecting from multiple retrievers | 🦜️🔗 Langchain","description":"This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["340",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * How to\n[/docs/modules/chains/how_to/] * Foundational\n[/docs/modules/chains/foundational/] * Documents\n[/docs/modules/chains/document/] * Popular [/docs/modules/chains/popular/] *\nAdditional [/docs/modules/chains/additional/] * OpenAI functions chains\n[/docs/modules/chains/additional/openai_functions/] * Analyze Document\n[/docs/modules/chains/additional/analyze_document] * Self-critique chain with\nconstitutional AI [/docs/modules/chains/additional/constitutional_chain] * Neo4j\nCypher graph QA [/docs/modules/chains/additional/cypher_chain] * Moderation\n[/docs/modules/chains/additional/moderation] * Dynamically selecting from\nmultiple prompts [/docs/modules/chains/additional/multi_prompt_router] *\nDynamically selecting from multiple retrievers","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router","title":"Dynamically selecting from multiple retrievers | 🦜️🔗 Langchain","description":"This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.","language":"en","loc":{"lines":{"from":24,"to":38}}}}],["341",{"pageContent":"* Dynamically selecting from multiple prompts\n[/docs/modules/chains/additional/multi_prompt_router] * Dynamically selecting\nfrom multiple retrievers\n[/docs/modules/chains/additional/multi_retrieval_qa_router] * Memory\n[/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Chains\n[/docs/modules/chains/] * Additional [/docs/modules/chains/additional/] *\nDynamically selecting from multiple retrievers DYNAMICALLY SELECTING FROM\nMULTIPLE RETRIEVERS This notebook demonstrates how to use the RouterChain\nparadigm to create a chain that dynamically","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router","title":"Dynamically selecting from multiple retrievers | 🦜️🔗 Langchain","description":"This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.","language":"en","loc":{"lines":{"from":38,"to":62}}}}],["342",{"pageContent":"* Dynamically selecting from multiple retrievers DYNAMICALLY SELECTING FROM\nMULTIPLE RETRIEVERS This notebook demonstrates how to use the RouterChain\nparadigm to create a chain that dynamically selects which Retrieval system to\nuse. Specifically we show how to use the MultiRetrievalQAChain to create a\nquestion-answering chain that selects the retrieval QA chain which is most\nrelevant for a given question, and then answers the question using it. import {\nMultiRetrievalQAChain } from \"langchain/chains\"; import { OpenAIChat } from\n\"langchain/llms/openai\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { MemoryVectorStore } from\n\"langchain/vectorstores/memory\"; const embeddings = new OpenAIEmbeddings();\nconst aquaTeen = await MemoryVectorStore.fromTexts( [ \"My name is shake zula,\nthe mike rula, the old schoola, you want a trip I'll bring it to ya\", \"Frylock\nand I'm on top rock you like a cop meatwad you're up next with your knock\nknock\", \"Meatwad","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router","title":"Dynamically selecting from multiple retrievers | 🦜️🔗 Langchain","description":"This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.","language":"en","loc":{"lines":{"from":62,"to":81}}}}],["343",{"pageContent":"name is shake zula, the mike rula, the old schoola, you want a trip I'll bring\nit to ya\", \"Frylock and I'm on top rock you like a cop meatwad you're up next\nwith your knock knock\", \"Meatwad make the money see meatwad get the honeys g\ndrivin' in my car livin' like a star\", \"Ice on my fingers and my toes and I'm a\ntaurus uh check-check it yeah\", \"Cause we are the Aqua Teens make the homies say\nho and the girlies wanna scream\", \"Aqua Teen Hunger Force number one in the hood\nG\", ], { series: \"Aqua Teen Hunger Force\" }, embeddings ); const mst3k = await\nMemoryVectorStore.fromTexts( [ \"In the not too distant future next Sunday A.D.\nThere was a guy named Joel not too different from you or me. He worked at\nGizmonic Institute, just another face in a red jumpsuit\", \"He did a good job\ncleaning up the place but his bosses didn't like him so they shot him into\nspace. We'll send him cheesy movies the worst we can find He'll have to sit and\nwatch them all and","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router","title":"Dynamically selecting from multiple retrievers | 🦜️🔗 Langchain","description":"This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.","language":"en","loc":{"lines":{"from":81,"to":94}}}}],["344",{"pageContent":"\"He did a good job cleaning up the place but his bosses didn't like him so they\nshot him into space. We'll send him cheesy movies the worst we can find He'll\nhave to sit and watch them all and we'll monitor his mind\", \"Now keep in mind\nJoel can't control where the movies begin or end Because he used those special\nparts to make his robot friends. Robot Roll Call Cambot Gypsy Tom Servo\nCroooow\", \"If you're wondering how he eats and breathes and other science facts\nLa la la just repeat to yourself it's just a show I should really just relax.\nFor Mystery Science Theater 3000\", ], { series: \"Mystery Science Theater 3000\"\n}, embeddings ); const animaniacs = await MemoryVectorStore.fromTexts( [ \"It's\ntime for Animaniacs And we're zany to the max So just sit back and relax You'll\nlaugh 'til you collapse We're Animaniacs\", \"Come join the Warner Brothers And\nthe Warner Sister Dot Just for fun we run around the Warner movie lot\", \"They\nlock us in the tower whenever","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router","title":"Dynamically selecting from multiple retrievers | 🦜️🔗 Langchain","description":"This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.","language":"en","loc":{"lines":{"from":94,"to":105}}}}],["345",{"pageContent":"laugh 'til you collapse We're Animaniacs\", \"Come join the Warner Brothers And\nthe Warner Sister Dot Just for fun we run around the Warner movie lot\", \"They\nlock us in the tower whenever we get caught But we break loose and then vamoose\nAnd now you know the plot\", \"We're Animaniacs, Dot is cute, and Yakko yaks,\nWakko packs away the snacks While Bill Clinton plays the sax\", \"We're Animaniacs\nMeet Pinky and the Brain who want to rule the universe Goodfeathers flock\ntogether Slappy whacks 'em with her purse\", \"Buttons chases Mindy while Rita\nsings a verse The writers flipped we have no script Why bother to rehearse\",\n\"We're Animaniacs We have pay-or-play contracts We're zany to the max There's\nbaloney in our slacks\", \"We're Animanie Totally insaney Here's the show's\nnamey\", \"Animaniacs Those are the facts\", ], { series: \"Animaniacs\" },\nembeddings ); const llm = new OpenAIChat(); const retrieverNames = [\"aqua teen\",\n\"mst3k\", \"animaniacs\"]; const","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router","title":"Dynamically selecting from multiple retrievers | 🦜️🔗 Langchain","description":"This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.","language":"en","loc":{"lines":{"from":105,"to":122}}}}],["346",{"pageContent":"namey\", \"Animaniacs Those are the facts\", ], { series: \"Animaniacs\" },\nembeddings ); const llm = new OpenAIChat(); const retrieverNames = [\"aqua teen\",\n\"mst3k\", \"animaniacs\"]; const retrieverDescriptions = [ \"Good for answering\nquestions about Aqua Teen Hunger Force theme song\", \"Good for answering\nquestions about Mystery Science Theater 3000 theme song\", \"Good for answering\nquestions about Animaniacs theme song\", ]; const retrievers = [\naquaTeen.asRetriever(3), mst3k.asRetriever(3), animaniacs.asRetriever(3), ];\nconst multiRetrievalQAChain = MultiRetrievalQAChain.fromLLMAndRetrievers(llm, {\nretrieverNames, retrieverDescriptions, retrievers, /** * You can return the\ndocument that's being used by the * query by adding the following option for\nretrieval QA * chain. */ retrievalQAChainOpts: { returnSourceDocuments: true, },\n}); const testPromise1 = multiRetrievalQAChain.call({ input: \"In the Aqua Teen\nHunger Force theme song, who","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router","title":"Dynamically selecting from multiple retrievers | 🦜️🔗 Langchain","description":"This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.","language":"en","loc":{"lines":{"from":122,"to":158}}}}],["347",{"pageContent":"QA * chain. */ retrievalQAChainOpts: { returnSourceDocuments: true, }, }); const\ntestPromise1 = multiRetrievalQAChain.call({ input: \"In the Aqua Teen Hunger\nForce theme song, who calls himself the mike rula?\", }); const testPromise2 =\nmultiRetrievalQAChain.call({ input: \"In the Mystery Science Theater 3000 theme\nsong, who worked at Gizmonic Institute?\", }); const testPromise3 =\nmultiRetrievalQAChain.call({ input: \"In the Animaniacs theme song, who plays the\nsax while Wakko packs away the snacks?\", }); const [ { text: result1,\nsourceDocuments: sourceDocuments1 }, { text: result2, sourceDocuments:\nsourceDocuments2 }, { text: result3, sourceDocuments: sourceDocuments3 }, ] =\nawait Promise.all([testPromise1, testPromise2, testPromise3]);\nconsole.log(sourceDocuments1, sourceDocuments2, sourceDocuments3);\nconsole.log(result1, result2, result3); API REFERENCE: * MultiRetrievalQAChain\n[/docs/api/chains/classes/MultiRetrievalQAChain] from","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router","title":"Dynamically selecting from multiple retrievers | 🦜️🔗 Langchain","description":"This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.","language":"en","loc":{"lines":{"from":158,"to":194}}}}],["348",{"pageContent":"sourceDocuments2, sourceDocuments3); console.log(result1, result2, result3); API\nREFERENCE: * MultiRetrievalQAChain\n[/docs/api/chains/classes/MultiRetrievalQAChain] from langchain/chains *\nOpenAIChat [/docs/api/llms_openai/classes/OpenAIChat] from langchain/llms/openai\n* OpenAIEmbeddings [/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * MemoryVectorStore\n[/docs/api/vectorstores_memory/classes/MemoryVectorStore] from\nlangchain/vectorstores/memory Previous Dynamically selecting from multiple\nprompts [/docs/modules/chains/additional/multi_prompt_router] Next Memory\n[/docs/modules/memory/] Community * Discord [https://discord.gg/cU2adEyC7w] *\nTwitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router","title":"Dynamically selecting from multiple retrievers | 🦜️🔗 Langchain","description":"This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.","language":"en","loc":{"lines":{"from":194,"to":225}}}}],["349",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Agent types","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["350",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Agent types\n[/docs/modules/agents/agent_types/] * How-to\n[/docs/modules/agents/how_to/callbacks] * Tools [/docs/modules/agents/tools/] *\nToolkits [/docs/modules/agents/toolkits/] * Callbacks [/docs/modules/callbacks/]\n* Modules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem]\n* Additional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Agents On this\npage AGENTS Some applications require a flexible chain of calls to LLMs and\nother tools based on user input. The Agent interface provides the flexibility\nfor such applications. An agent has access to","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":25,"to":54}}}}],["351",{"pageContent":"applications require a flexible chain of calls to LLMs and other tools based on\nuser input. The Agent interface provides the flexibility for such applications.\nAn agent has access to a suite of tools, and determines which ones to use\ndepending on the user input. Agents can use multiple tools, and use the output\nof one tool as the input to the next. There are two main types of agents: *\nAction agents: at each timestep, decide on the next action using the outputs of\nall previous actions * Plan-and-execute agents: decide on the full sequence of\nactions up front, then execute them all without updating the plan Action agents\nare suitable for small tasks, while plan-and-execute agents are better for\ncomplex or long-running tasks that require maintaining long-term objectives and\nfocus. Often the best approach is to combine the dynamism of an action agent\nwith the planning abilities of a plan-and-execute agent by letting the\nplan-and-execute agent use action agents to execute plans. For","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":54,"to":67}}}}],["352",{"pageContent":"the best approach is to combine the dynamism of an action agent with the\nplanning abilities of a plan-and-execute agent by letting the plan-and-execute\nagent use action agents to execute plans. For a full list of agent types see\nagent types [/docs/modules/agents/agent_types/]. Additional abstractions\ninvolved in agents are: * Tools [/docs/modules/agents/tools/]: the actions an\nagent can take. What tools you give an agent highly depend on what you want the\nagent to do * Toolkits [/docs/modules/agents/toolkits/]: wrappers around\ncollections of tools that can be used together a specific use case. For example,\nin order for an agent to interact with a SQL database it will likely need one\ntool to execute queries and another to inspect tables ACTION AGENTS At a\nhigh-level an action agent: 1. Receives user input 2. Decides which tool, if\nany, to use and the tool input 3. Calls the tool and records the output (also\nknown as an \"observation\") 4. Decides the next step using","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":67,"to":87}}}}],["353",{"pageContent":"agent: 1. Receives user input 2. Decides which tool, if any, to use and the tool\ninput 3. Calls the tool and records the output (also known as an \"observation\")\n4. Decides the next step using the history of tools, tool inputs, and\nobservations 5. Repeats 3-4 until it determines it can respond directly to the\nuser Action agents are wrapped in agent executors, chains which are responsible\nfor calling the agent, getting back an action and action input, calling the tool\nthat the action references with the generated input, getting the output of the\ntool, and then passing all that information back into the agent to get the next\naction it should take. Although an agent can be constructed in many ways, it\ntypically involves these components: * Prompt template: Responsible for taking\nthe user input and previous steps and constructing a prompt to send to the\nlanguage model * Language model: Takes the prompt with user input and action\nhistory and decides what to do next * Output","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":87,"to":104}}}}],["354",{"pageContent":"user input and previous steps and constructing a prompt to send to the language\nmodel * Language model: Takes the prompt with user input and action history and\ndecides what to do next * Output parser: Takes the output of the language model\nand parses it into the next action or a final answer PLAN-AND-EXECUTE AGENTS At\na high-level a plan-and-execute agent: 1. Receives user input 2. Plans the full\nsequence of steps to take 3. Executes the steps in order, passing the outputs of\npast steps as inputs to future steps The most typical implementation is to have\nthe planner be a language model, and the executor be an action agent. Read more\nhere [/docs/modules/agents/agent_types/plan_and_execute]. GET STARTED LangChain\noffers several types of agents. Here's an example using one powered by OpenAI\nfunctions: import { initializeAgentExecutorWithOptions } from\n\"langchain/agents\"; import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { SerpAPI } from","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":104,"to":128}}}}],["355",{"pageContent":"using one powered by OpenAI functions: import {\ninitializeAgentExecutorWithOptions } from \"langchain/agents\"; import {\nChatOpenAI } from \"langchain/chat_models/openai\"; import { SerpAPI } from\n\"langchain/tools\"; import { Calculator } from \"langchain/tools/calculator\";\nconst tools = [new Calculator(), new SerpAPI()]; const chat = new ChatOpenAI({\nmodelName: \"gpt-4\", temperature: 0 }); const executor = await\ninitializeAgentExecutorWithOptions(tools, chat, { agentType: \"openai-functions\",\nverbose: true, }); const result = await executor.run(\"What is the weather in New\nYork?\"); console.log(result); /* The current weather in New York is 72°F with a\nwind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index\nis 0 out of 11. The cloud cover is 79% and there has been no rain. */ API\nREFERENCE: * initializeAgentExecutorWithOptions\n[/docs/api/agents/functions/initializeAgentExecutorWithOptions] from\nlangchain/agents * ChatOpenAI","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":128,"to":156}}}}],["356",{"pageContent":"is 79% and there has been no rain. */ API REFERENCE: *\ninitializeAgentExecutorWithOptions\n[/docs/api/agents/functions/initializeAgentExecutorWithOptions] from\nlangchain/agents * ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI]\nfrom langchain/chat_models/openai * SerpAPI [/docs/api/tools/classes/SerpAPI]\nfrom langchain/tools * Calculator\n[/docs/api/tools_calculator/classes/Calculator] from langchain/tools/calculator\nAnd here is the logged verbose output: [chain/start] [1:chain:AgentExecutor]\nEntering Chain run with input: { \"input\": \"What is the weather in New York?\",\n\"chat_history\": [] } [llm/start] [1:chain:AgentExecutor > 2:llm:ChatOpenAI]\nEntering LLM run with input: { \"messages\": [ [ { \"lc\": 1, \"type\": \"constructor\",\n\"id\": [ \"langchain\", \"schema\", \"SystemMessage\" ], \"kwargs\": { \"content\": \"You\nare a helpful AI assistant.\", \"additional_kwargs\": {}","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":156,"to":188}}}}],["357",{"pageContent":"[ \"langchain\", \"schema\", \"SystemMessage\" ], \"kwargs\": { \"content\": \"You are a\nhelpful AI assistant.\", \"additional_kwargs\": {} } }, { \"lc\": 1, \"type\":\n\"constructor\", \"id\": [ \"langchain\", \"schema\", \"HumanMessage\" ], \"kwargs\": {\n\"content\": \"What is the weather in New York?\", \"additional_kwargs\": {} } } ] ] }\n[llm/end] [1:chain:AgentExecutor > 2:llm:ChatOpenAI] [1.97s] Exiting LLM run\nwith output: { \"generations\": [ [ { \"text\": \"\", \"message\": { \"lc\": 1, \"type\":\n\"constructor\", \"id\": [ \"langchain\", \"schema\", \"AIMessage\" ], \"kwargs\": {\n\"content\": \"\", \"additional_kwargs\": { \"function_call\": { \"name\": \"search\",","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":188,"to":231}}}}],["358",{"pageContent":"\"AIMessage\" ], \"kwargs\": { \"content\": \"\", \"additional_kwargs\": {\n\"function_call\": { \"name\": \"search\", \"arguments\": \"{\\n \\\"input\\\": \\\"current\nweather in New York\\\"\\n}\" } } } } } ] ], \"llmOutput\": { \"tokenUsage\": {\n\"completionTokens\": 18, \"promptTokens\": 121, \"totalTokens\": 139 } } }\n[agent/action] [1:chain:AgentExecutor] Agent selected action: { \"tool\":\n\"search\", \"toolInput\": { \"input\": \"current weather in New York\" }, \"log\": \"\" }\n[tool/start] [1:chain:AgentExecutor > 3:tool:SerpAPI] Entering Tool run with\ninput: \"current weather in New York\" [tool/end] [1:chain:AgentExecutor >\n3:tool:SerpAPI] [1.90s] Exiting Tool run with output: \"1 am · Feels Like72° ·\nWindSSW 1 mph · Humidity89% · UV Index0 of 11 · Cloud Cover79% · Rain Amount0 in\n...\" [llm/start] [1:chain:AgentExecutor > 4:llm:ChatOpenAI]","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":231,"to":263}}}}],["359",{"pageContent":"Exiting Tool run with output: \"1 am · Feels Like72° · WindSSW 1 mph ·\nHumidity89% · UV Index0 of 11 · Cloud Cover79% · Rain Amount0 in ...\"\n[llm/start] [1:chain:AgentExecutor > 4:llm:ChatOpenAI] Entering LLM run with\ninput: { \"messages\": [ [ { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\",\n\"schema\", \"SystemMessage\" ], \"kwargs\": { \"content\": \"You are a helpful AI\nassistant.\", \"additional_kwargs\": {} } }, { \"lc\": 1, \"type\": \"constructor\",\n\"id\": [ \"langchain\", \"schema\", \"HumanMessage\" ], \"kwargs\": { \"content\": \"What is\nthe weather in New York?\", \"additional_kwargs\": {} } }, { \"lc\": 1, \"type\":\n\"constructor\", \"id\": [ \"langchain\", \"schema\", \"AIMessage\" ], \"kwargs\": {\n\"content\":","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":263,"to":302}}}}],["360",{"pageContent":"} }, { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\",\n\"AIMessage\" ], \"kwargs\": { \"content\": \"\", \"additional_kwargs\": {\n\"function_call\": { \"name\": \"search\", \"arguments\": \"{\\\"input\\\":\\\"current weather\nin New York\\\"}\" } } } }, { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\",\n\"schema\", \"FunctionMessage\" ], \"kwargs\": { \"content\": \"1 am · Feels Like72° ·\nWindSSW 1 mph · Humidity89% · UV Index0 of 11 · Cloud Cover79% · Rain Amount0 in\n...\", \"name\": \"search\", \"additional_kwargs\": {} } } ] ] } [llm/end]\n[1:chain:AgentExecutor > 4:llm:ChatOpenAI] [3.33s] Exiting LLM run with output:\n{ \"generations\": [ [ { \"text\": \"The current weather in New York is 72°F","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":302,"to":343}}}}],["361",{"pageContent":"} ] ] } [llm/end] [1:chain:AgentExecutor > 4:llm:ChatOpenAI] [3.33s] Exiting LLM\nrun with output: { \"generations\": [ [ { \"text\": \"The current weather in New York\nis 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89%\nand the UV index is 0 out of 11. The cloud cover is 79% and there has been no\nrain.\", \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\",\n\"schema\", \"AIMessage\" ], \"kwargs\": { \"content\": \"The current weather in New York\nis 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89%\nand the UV index is 0 out of 11. The cloud cover is 79% and there has been no\nrain.\", \"additional_kwargs\": {} } } } ] ], \"llmOutput\": { \"tokenUsage\": {\n\"completionTokens\": 58, \"promptTokens\": 180, \"totalTokens\": 238 } } }\n[chain/end]","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":343,"to":376}}}}],["362",{"pageContent":"{} } } } ] ], \"llmOutput\": { \"tokenUsage\": { \"completionTokens\": 58,\n\"promptTokens\": 180, \"totalTokens\": 238 } } } [chain/end]\n[1:chain:AgentExecutor] [7.73s] Exiting Chain run with output: { \"output\": \"The\ncurrent weather in New York is 72°F with a wind speed of 1 mph coming from the\nSSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is\n79% and there has been no rain.\" } Previous Zep Memory\n[/docs/modules/memory/integrations/zep_memory] Next Agent types\n[/docs/modules/agents/agent_types/] * Action agents * Plan-and-execute agents *\nGet started Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/agents/","title":"Agents | 🦜️🔗 Langchain","description":"Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.","language":"en","loc":{"lines":{"from":376,"to":418}}}}],["363",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Agent types","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/","title":"Agent types | 🦜️🔗 Langchain","description":"Action agents","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["364",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Agent types\n[/docs/modules/agents/agent_types/] * Conversational\n[/docs/modules/agents/agent_types/chat_conversation_agent] * OpenAI functions\n[/docs/modules/agents/agent_types/openai_functions_agent] * Plan and execute\n[/docs/modules/agents/agent_types/plan_and_execute] * ReAct\n[/docs/modules/agents/agent_types/react] * Structured tool chat\n[/docs/modules/agents/agent_types/structured_chat] * XML Agent\n[/docs/modules/agents/agent_types/xml] * How-to\n[/docs/modules/agents/how_to/callbacks] * Tools [/docs/modules/agents/tools/] *\nToolkits [/docs/modules/agents/toolkits/] * Callbacks [/docs/modules/callbacks/]\n* Modules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem]\n* Additional resources [/docs/additional_resources] * Community navigator","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/","title":"Agent types | 🦜️🔗 Langchain","description":"Action agents","language":"en","loc":{"lines":{"from":25,"to":44}}}}],["365",{"pageContent":"Callbacks [/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides\n[/docs/guides] * Ecosystem [/docs/ecosystem] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Agents\n[/docs/modules/agents/] * Agent types On this page AGENT TYPES ACTION AGENTS\nAgents use an LLM to determine which actions to take and in what order. An\naction can either be using a tool and observing its output, or returning a\nresponse to the user. Here are the agents available in LangChain. ZERO-SHOT\nREACT [/docs/modules/agents/agent_types/react] This agent uses the ReAct\n[https://arxiv.org/pdf/2205.00445.pdf] framework to determine which tool to use\nbased solely on the tool's description. Any number of tools can be provided.\nThis agent requires that a","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/","title":"Agent types | 🦜️🔗 Langchain","description":"Action agents","language":"en","loc":{"lines":{"from":44,"to":75}}}}],["366",{"pageContent":"uses the ReAct [https://arxiv.org/pdf/2205.00445.pdf] framework to determine\nwhich tool to use based solely on the tool's description. Any number of tools\ncan be provided. This agent requires that a description is provided for each\ntool. Note: This is the most general purpose action agent. OPENAI FUNCTIONS\n[/docs/modules/agents/agent_types/openai_functions_agent] Certain OpenAI models\n(like gpt-3.5-turbo-0613 and gpt-4-0613) have been explicitly fine-tuned to\ndetect when a function should be called and respond with the inputs that should\nbe passed to the function. The OpenAI Functions Agent is designed to work with\nthese models. CONVERSATIONAL\n[/docs/modules/agents/agent_types/chat_conversation_agent] This agent is\ndesigned to be used in conversational settings. The prompt is designed to make\nthe agent helpful and conversational. It uses the ReAct framework to decide\nwhich tool to use, and uses memory to remember the previous conversation\ninteractions. PLAN-AND-EXECUTE AGENTS","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/","title":"Agent types | 🦜️🔗 Langchain","description":"Action agents","language":"en","loc":{"lines":{"from":75,"to":94}}}}],["367",{"pageContent":"to make the agent helpful and conversational. It uses the ReAct framework to\ndecide which tool to use, and uses memory to remember the previous conversation\ninteractions. PLAN-AND-EXECUTE AGENTS\n[/docs/modules/agents/agent_types/plan_and_execute] Plan and execute agents\naccomplish an objective by first planning what to do, then executing the sub\ntasks. This idea is largely inspired by BabyAGI\n[https://github.com/yoheinakajima/babyagi] and then the \"Plan-and-Solve\" paper\n[https://arxiv.org/abs/2305.04091]. Previous Agents [/docs/modules/agents/] Next\nConversational [/docs/modules/agents/agent_types/chat_conversation_agent] *\nAction agents * Zero-shot ReAct * OpenAI Functions * Conversational *\nPlan-and-execute agents Community * Discord [https://discord.gg/cU2adEyC7w] *\nTwitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] *","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/","title":"Agent types | 🦜️🔗 Langchain","description":"Action agents","language":"en","loc":{"lines":{"from":94,"to":126}}}}],["368",{"pageContent":"* Twitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/","title":"Agent types | 🦜️🔗 Langchain","description":"Action agents","language":"en","loc":{"lines":{"from":126,"to":136}}}}],["369",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Agent types","metadata":{"source":"https://js.langchain.com/docs/modules/agents/how_to/callbacks","title":"Subscribing to events | 🦜️🔗 Langchain","description":"You can subscribe to a number of events that are emitted by the Agent and the underlying tools, chains and models via callbacks.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["370",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Agent types\n[/docs/modules/agents/agent_types/] * How-to\n[/docs/modules/agents/how_to/callbacks] * Subscribing to events\n[/docs/modules/agents/how_to/callbacks] * Cancelling requests\n[/docs/modules/agents/how_to/cancelling_requests] * Custom LLM Agent\n[/docs/modules/agents/how_to/custom_llm_agent] * Custom LLM Agent (with a\nChatModel) [/docs/modules/agents/how_to/custom_llm_chat_agent] * Handle parsing\nerrors [/docs/modules/agents/how_to/handle_parsing_errors] * Logging and tracing\n[/docs/modules/agents/how_to/logging_and_tracing] * Adding a timeout\n[/docs/modules/agents/how_to/timeouts] * Tools [/docs/modules/agents/tools/] *\nToolkits [/docs/modules/agents/toolkits/] * Callbacks [/docs/modules/callbacks/]\n* Modules [/docs/modules/] * Guides","metadata":{"source":"https://js.langchain.com/docs/modules/agents/how_to/callbacks","title":"Subscribing to events | 🦜️🔗 Langchain","description":"You can subscribe to a number of events that are emitted by the Agent and the underlying tools, chains and models via callbacks.","language":"en","loc":{"lines":{"from":25,"to":42}}}}],["371",{"pageContent":"* Tools [/docs/modules/agents/tools/] * Toolkits\n[/docs/modules/agents/toolkits/] * Callbacks [/docs/modules/callbacks/] *\nModules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Agents\n[/docs/modules/agents/] * How-to * Subscribing to events SUBSCRIBING TO EVENTS\nYou can subscribe to a number of events that are emitted by the Agent and the\nunderlying tools, chains and models via callbacks. For more info on the events\navailable see the Callbacks [/docs/modules/callbacks/] section of the docs.\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\"; import {\nOpenAI } from \"langchain/llms/openai\"; import { SerpAPI } from","metadata":{"source":"https://js.langchain.com/docs/modules/agents/how_to/callbacks","title":"Subscribing to events | 🦜️🔗 Langchain","description":"You can subscribe to a number of events that are emitted by the Agent and the underlying tools, chains and models via callbacks.","language":"en","loc":{"lines":{"from":42,"to":70}}}}],["372",{"pageContent":"[/docs/modules/callbacks/] section of the docs. import {\ninitializeAgentExecutorWithOptions } from \"langchain/agents\"; import { OpenAI }\nfrom \"langchain/llms/openai\"; import { SerpAPI } from \"langchain/tools\"; import\n{ Calculator } from \"langchain/tools/calculator\"; const model = new OpenAI({\ntemperature: 0 }); const tools = [ new SerpAPI(process.env.SERPAPI_API_KEY, {\nlocation: \"Austin,Texas,United States\", hl: \"en\", gl: \"us\", }), new\nCalculator(), ]; const executor = await\ninitializeAgentExecutorWithOptions(tools, model, { agentType:\n\"zero-shot-react-description\", }); const input = `Who is Olivia Wilde's\nboyfriend? What is his current age raised to the 0.23 power?`; const result =\nawait executor.run(input, [ { handleAgentAction(action, runId) {\nconsole.log(\"\\nhandleAgentAction\", action, runId); }, handleAgentEnd(action,\nrunId) { console.log(\"\\nhandleAgentEnd\", action, runId); },\nhandleToolEnd(output, runId) {","metadata":{"source":"https://js.langchain.com/docs/modules/agents/how_to/callbacks","title":"Subscribing to events | 🦜️🔗 Langchain","description":"You can subscribe to a number of events that are emitted by the Agent and the underlying tools, chains and models via callbacks.","language":"en","loc":{"lines":{"from":70,"to":99}}}}],["373",{"pageContent":"console.log(\"\\nhandleAgentAction\", action, runId); }, handleAgentEnd(action,\nrunId) { console.log(\"\\nhandleAgentEnd\", action, runId); },\nhandleToolEnd(output, runId) { console.log(\"\\nhandleToolEnd\", output, runId); },\n}, ]); /* handleAgentAction { tool: 'search', toolInput: 'Olivia Wilde\nboyfriend', log: \" I need to find out who Olivia Wilde's boyfriend is and then\ncalculate his age raised to the 0.23 power.\\n\" + 'Action: search\\n' + 'Action\nInput: \"Olivia Wilde boyfriend\"' } 9b978461-1f6f-4d5f-80cf-5b229ce181b6\nhandleToolEnd In January 2021, Wilde began dating singer Harry Styles after\nmeeting during the filming of Don't Worry Darling. Their relationship ended in\nNovember 2022. 062fef47-8ad1-4729-9949-a57be252e002 handleAgentAction { tool:\n'search', toolInput: 'Harry Styles age', log: \" I need to find out Harry Styles'\nage.\\n\" + 'Action: search\\n' + 'Action Input: \"Harry Styles age\"' }","metadata":{"source":"https://js.langchain.com/docs/modules/agents/how_to/callbacks","title":"Subscribing to events | 🦜️🔗 Langchain","description":"You can subscribe to a number of events that are emitted by the Agent and the underlying tools, chains and models via callbacks.","language":"en","loc":{"lines":{"from":99,"to":126}}}}],["374",{"pageContent":"{ tool: 'search', toolInput: 'Harry Styles age', log: \" I need to find out Harry\nStyles' age.\\n\" + 'Action: search\\n' + 'Action Input: \"Harry Styles age\"' }\n9b978461-1f6f-4d5f-80cf-5b229ce181b6 handleToolEnd 29 years\n9ec91e41-2fbf-4de0-85b6-12b3e6b3784e 61d77e10-c119-435d-a985-1f9d45f0ef08\nhandleAgentAction { tool: 'calculator', toolInput: '29^0.23', log: ' I need to\ncalculate 29 raised to the 0.23 power.\\n' + 'Action: calculator\\n' + 'Action\nInput: 29^0.23' } 9b978461-1f6f-4d5f-80cf-5b229ce181b6 handleToolEnd\n2.169459462491557 07aec96a-ce19-4425-b863-2eae39db8199 handleAgentEnd {\nreturnValues: { output: \"Harry Styles is Olivia Wilde's boyfriend and his\ncurrent age raised to the 0.23 power is 2.169459462491557.\" }, log: ' I now know\nthe final answer.\\n' + \"Final Answer: Harry Styles is Olivia Wilde's boyfriend\nand his current age raised to the 0.23 power is 2.169459462491557.\" }\n9b978461-1f6f-4d5f-80cf-5b229ce181b6 */ console.log({ result","metadata":{"source":"https://js.langchain.com/docs/modules/agents/how_to/callbacks","title":"Subscribing to events | 🦜️🔗 Langchain","description":"You can subscribe to a number of events that are emitted by the Agent and the underlying tools, chains and models via callbacks.","language":"en","loc":{"lines":{"from":126,"to":155}}}}],["375",{"pageContent":"+ \"Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age\nraised to the 0.23 power is 2.169459462491557.\" }\n9b978461-1f6f-4d5f-80cf-5b229ce181b6 */ console.log({ result }); // { result:\n\"Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23\npower is 2.169459462491557.\" } API REFERENCE: *\ninitializeAgentExecutorWithOptions\n[/docs/api/agents/functions/initializeAgentExecutorWithOptions] from\nlangchain/agents * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai * SerpAPI [/docs/api/tools/classes/SerpAPI] from\nlangchain/tools * Calculator [/docs/api/tools_calculator/classes/Calculator]\nfrom langchain/tools/calculator Previous XML Agent\n[/docs/modules/agents/agent_types/xml] Next Cancelling requests\n[/docs/modules/agents/how_to/cancelling_requests] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] *","metadata":{"source":"https://js.langchain.com/docs/modules/agents/how_to/callbacks","title":"Subscribing to events | 🦜️🔗 Langchain","description":"You can subscribe to a number of events that are emitted by the Agent and the underlying tools, chains and models via callbacks.","language":"en","loc":{"lines":{"from":155,"to":185}}}}],["376",{"pageContent":"* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/agents/how_to/callbacks","title":"Subscribing to events | 🦜️🔗 Langchain","description":"You can subscribe to a number of events that are emitted by the Agent and the underlying tools, chains and models via callbacks.","language":"en","loc":{"lines":{"from":185,"to":196}}}}],["377",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Agent types","metadata":{"source":"https://js.langchain.com/docs/modules/agents/tools/","title":"Tools | 🦜️🔗 Langchain","description":"Tools are interfaces that an agent can use to interact with the world.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["378",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Agent types\n[/docs/modules/agents/agent_types/] * How-to\n[/docs/modules/agents/how_to/callbacks] * Tools [/docs/modules/agents/tools/] *\nHow-to [/docs/modules/agents/tools/how_to/agents_with_vectorstores] *\nIntegrations [/docs/modules/agents/tools/integrations/] * Toolkits\n[/docs/modules/agents/toolkits/] * Callbacks [/docs/modules/callbacks/] *\nModules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Agents\n[/docs/modules/agents/] * Tools On this page TOOLS Tools are","metadata":{"source":"https://js.langchain.com/docs/modules/agents/tools/","title":"Tools | 🦜️🔗 Langchain","description":"Tools are interfaces that an agent can use to interact with the world.","language":"en","loc":{"lines":{"from":25,"to":56}}}}],["379",{"pageContent":"* API reference [/docs/api/] * / * Modules [/docs/modules/] * Agents\n[/docs/modules/agents/] * Tools On this page TOOLS Tools are interfaces that an\nagent can use to interact with the world. GET STARTED Tools are functions that\nagents can use to interact with the world. These tools can be generic utilities\n(e.g. search), other chains, or even other agents. Specifically, the interface\nof a tool has a single text input and a single text output. It includes a name\nand description that communicate to the model what the tool does and when to use\nit. interface Tool { call(arg: string): Promise; name: string; description:\nstring; } ADVANCED To implement your own tool you can subclass the Tool class\nand implement the _call method. The _call method is called with the input text\nand should return the output text. The Tool superclass implements the call\nmethod, which takes care of calling the right CallbackManager methods before and\nafter calling your _call","metadata":{"source":"https://js.langchain.com/docs/modules/agents/tools/","title":"Tools | 🦜️🔗 Langchain","description":"Tools are interfaces that an agent can use to interact with the world.","language":"en","loc":{"lines":{"from":56,"to":95}}}}],["380",{"pageContent":"the input text and should return the output text. The Tool superclass implements\nthe call method, which takes care of calling the right CallbackManager methods\nbefore and after calling your _call method. When an error occurs, the _call\nmethod should when possible return a string representing an error, rather than\nthrowing an error. This allows the error to be passed to the LLM and the LLM can\ndecide how to handle it. If an error is thrown then execution of the agent will\nstop. abstract class Tool { abstract _call(arg: string): Promise; abstract name:\nstring; abstract description: string; } Previous Adding a timeout\n[/docs/modules/agents/how_to/timeouts] Next Vector stores as tools\n[/docs/modules/agents/tools/how_to/agents_with_vectorstores] * Get started\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS","metadata":{"source":"https://js.langchain.com/docs/modules/agents/tools/","title":"Tools | 🦜️🔗 Langchain","description":"Tools are interfaces that an agent can use to interact with the world.","language":"en","loc":{"lines":{"from":95,"to":125}}}}],["381",{"pageContent":"* Get started Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/agents/tools/","title":"Tools | 🦜️🔗 Langchain","description":"Tools are interfaces that an agent can use to interact with the world.","language":"en","loc":{"lines":{"from":125,"to":139}}}}],["382",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Agent types","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/","title":"Toolkits | 🦜️🔗 Langchain","description":"Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["383",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Agent types\n[/docs/modules/agents/agent_types/] * How-to\n[/docs/modules/agents/how_to/callbacks] * Tools [/docs/modules/agents/tools/] *\nToolkits [/docs/modules/agents/toolkits/] * JSON Agent Toolkit\n[/docs/modules/agents/toolkits/json] * OpenAPI Agent Toolkit\n[/docs/modules/agents/toolkits/openapi] * AWS Step Functions Toolkit\n[/docs/modules/agents/toolkits/sfn_agent] * SQL Agent Toolkit\n[/docs/modules/agents/toolkits/sql] * VectorStore Agent Toolkit\n[/docs/modules/agents/toolkits/vectorstore] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/","title":"Toolkits | 🦜️🔗 Langchain","description":"Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods.","language":"en","loc":{"lines":{"from":25,"to":45}}}}],["384",{"pageContent":"* Modules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem]\n* Additional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Agents\n[/docs/modules/agents/] * Toolkits TOOLKITS Toolkits are collections of tools\nthat are designed to be used together for specific tasks and have convenience\nloading methods. 📄️ JSON AGENT TOOLKIT This example shows how to load and use\nan agent with a JSON toolkit. [/docs/modules/agents/toolkits/json] 📄️ OPENAPI\nAGENT TOOLKIT This example shows how to load and use an agent with a OpenAPI\ntoolkit. [/docs/modules/agents/toolkits/openapi] 📄️ AWS STEP FUNCTIONS TOOLKIT\nAWS Step Functions are a visual workflow service that helps developers use AWS\nservices to build distributed","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/","title":"Toolkits | 🦜️🔗 Langchain","description":"Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods.","language":"en","loc":{"lines":{"from":45,"to":82}}}}],["385",{"pageContent":"toolkit. [/docs/modules/agents/toolkits/openapi] 📄️ AWS STEP FUNCTIONS TOOLKIT\nAWS Step Functions are a visual workflow service that helps developers use AWS\nservices to build distributed applications, automate processes, orchestrate\nmicroservices, and create data and machine learning (ML) pipelines.\n[/docs/modules/agents/toolkits/sfn_agent] 📄️ SQL AGENT TOOLKIT This example\nshows how to load and use an agent with a SQL toolkit.\n[/docs/modules/agents/toolkits/sql] 📄️ VECTORSTORE AGENT TOOLKIT This example\nshows how to load and use an agent with a vectorstore toolkit.\n[/docs/modules/agents/toolkits/vectorstore] Previous Agent with Zapier NLA\nIntegration [/docs/modules/agents/tools/integrations/zapier_agent] Next JSON\nAgent Toolkit [/docs/modules/agents/toolkits/json] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/","title":"Toolkits | 🦜️🔗 Langchain","description":"Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods.","language":"en","loc":{"lines":{"from":82,"to":119}}}}],["386",{"pageContent":"* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/","title":"Toolkits | 🦜️🔗 Langchain","description":"Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods.","language":"en","loc":{"lines":{"from":119,"to":130}}}}],["387",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Agent types","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/plan_and_execute","title":"Plan and execute | 🦜️🔗 Langchain","description":"Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by BabyAGI and then the \"Plan-and-Solve\" paper.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["388",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Agent types\n[/docs/modules/agents/agent_types/] * Conversational\n[/docs/modules/agents/agent_types/chat_conversation_agent] * OpenAI functions\n[/docs/modules/agents/agent_types/openai_functions_agent] * Plan and execute\n[/docs/modules/agents/agent_types/plan_and_execute] * ReAct\n[/docs/modules/agents/agent_types/react] * Structured tool chat\n[/docs/modules/agents/agent_types/structured_chat] * XML Agent\n[/docs/modules/agents/agent_types/xml] * How-to\n[/docs/modules/agents/how_to/callbacks] * Tools [/docs/modules/agents/tools/] *\nToolkits [/docs/modules/agents/toolkits/] * Callbacks [/docs/modules/callbacks/]\n* Modules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem]\n* Additional resources [/docs/additional_resources] * Community navigator","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/plan_and_execute","title":"Plan and execute | 🦜️🔗 Langchain","description":"Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by BabyAGI and then the \"Plan-and-Solve\" paper.","language":"en","loc":{"lines":{"from":25,"to":44}}}}],["389",{"pageContent":"Callbacks [/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides\n[/docs/guides] * Ecosystem [/docs/ecosystem] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Agents\n[/docs/modules/agents/] * Agent types [/docs/modules/agents/agent_types/] * Plan\nand execute PLAN AND EXECUTE Plan and execute agents accomplish an objective by\nfirst planning what to do, then executing the sub tasks. This idea is largely\ninspired by BabyAGI [https://github.com/yoheinakajima/babyagi] and then the\n\"Plan-and-Solve\" paper [https://arxiv.org/abs/2305.04091]. The planning is\nalmost always done by an LLM. The execution is usually done by a separate agent\n(equipped with tools). This agent uses a two step process: 1. First, the agent\nuses an LLM","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/plan_and_execute","title":"Plan and execute | 🦜️🔗 Langchain","description":"Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by BabyAGI and then the \"Plan-and-Solve\" paper.","language":"en","loc":{"lines":{"from":44,"to":74}}}}],["390",{"pageContent":"planning is almost always done by an LLM. The execution is usually done by a\nseparate agent (equipped with tools). This agent uses a two step process: 1.\nFirst, the agent uses an LLM to create a plan to answer the query with clear\nsteps. 2. Once it has a plan, it uses an embedded traditional Action Agent to\nsolve each step. The idea is that the planning step keeps the LLM more \"on\ntrack\" by breaking up a larger task into simpler subtasks. However, this method\nrequires more individual LLM queries and has higher latency compared to Action\nAgents. Note: This agent currently only supports Chat Models. import {\nCalculator } from \"langchain/tools/calculator\"; import { SerpAPI } from\n\"langchain/tools\"; import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { PlanAndExecuteAgentExecutor } from\n\"langchain/experimental/plan_and_execute\"; const tools = [new Calculator(), new\nSerpAPI()]; const model = new ChatOpenAI({ temperature: 0, modelName:\n\"gpt-3.5-turbo\", verbose:","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/plan_and_execute","title":"Plan and execute | 🦜️🔗 Langchain","description":"Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by BabyAGI and then the \"Plan-and-Solve\" paper.","language":"en","loc":{"lines":{"from":74,"to":97}}}}],["391",{"pageContent":"} from \"langchain/experimental/plan_and_execute\"; const tools = [new\nCalculator(), new SerpAPI()]; const model = new ChatOpenAI({ temperature: 0,\nmodelName: \"gpt-3.5-turbo\", verbose: true, }); const executor =\nPlanAndExecuteAgentExecutor.fromLLMAndTools({ llm: model, tools, }); const\nresult = await executor.call({ input: `Who is the current president of the\nUnited States? What is their current age raised to the second power?`, });\nconsole.log({ result }); API REFERENCE: * Calculator\n[/docs/api/tools_calculator/classes/Calculator] from langchain/tools/calculator\n* SerpAPI [/docs/api/tools/classes/SerpAPI] from langchain/tools * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * PlanAndExecuteAgentExecutor\n[/docs/api/experimental_plan_and_execute/classes/PlanAndExecuteAgentExecutor]\nfrom langchain/experimental/plan_and_execute Previous OpenAI","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/plan_and_execute","title":"Plan and execute | 🦜️🔗 Langchain","description":"Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by BabyAGI and then the \"Plan-and-Solve\" paper.","language":"en","loc":{"lines":{"from":97,"to":128}}}}],["392",{"pageContent":"* PlanAndExecuteAgentExecutor\n[/docs/api/experimental_plan_and_execute/classes/PlanAndExecuteAgentExecutor]\nfrom langchain/experimental/plan_and_execute Previous OpenAI functions\n[/docs/modules/agents/agent_types/openai_functions_agent] Next ReAct\n[/docs/modules/agents/agent_types/react] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/plan_and_execute","title":"Plan and execute | 🦜️🔗 Langchain","description":"Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by BabyAGI and then the \"Plan-and-Solve\" paper.","language":"en","loc":{"lines":{"from":128,"to":149}}}}],["393",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * How-to\n[/docs/modules/memory/how_to/buffer] *","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/zep_memory","title":"Zep Memory | 🦜️🔗 Langchain","description":"Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["394",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * How-to [/docs/modules/memory/how_to/buffer] *\nIntegrations [/docs/modules/memory/integrations/] * Cloudflare D1-Backed Chat\nMemory [/docs/modules/memory/integrations/cloudflare_d1] * DynamoDB-Backed Chat\nMemory [/docs/modules/memory/integrations/dynamodb] * Firestore Chat Memory\n[/docs/modules/memory/integrations/firestore] * Momento-Backed Chat Memory\n[/docs/modules/memory/integrations/momento] * MongoDB Chat Memory\n[/docs/modules/memory/integrations/mongodb] * Motörhead Memory\n[/docs/modules/memory/integrations/motorhead_memory] * PlanetScale Chat Memory\n[/docs/modules/memory/integrations/planetscale] * Redis-Backed Chat Memory\n[/docs/modules/memory/integrations/redis] * Upstash Redis-Backed Chat Memory\n[/docs/modules/memory/integrations/upstash_redis] * Xata Chat Memory","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/zep_memory","title":"Zep Memory | 🦜️🔗 Langchain","description":"Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.","language":"en","loc":{"lines":{"from":25,"to":39}}}}],["395",{"pageContent":"* Redis-Backed Chat Memory [/docs/modules/memory/integrations/redis] * Upstash\nRedis-Backed Chat Memory [/docs/modules/memory/integrations/upstash_redis] *\nXata Chat Memory [/docs/modules/memory/integrations/xata] * Zep Memory\n[/docs/modules/memory/integrations/zep_memory] * Agents [/docs/modules/agents/]\n* Callbacks [/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides\n[/docs/guides] * Ecosystem [/docs/ecosystem] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Memory\n[/docs/modules/memory/] * Integrations [/docs/modules/memory/integrations/] *\nZep Memory ZEP MEMORY Zep [https://github.com/getzep/zep] is a memory server\nthat stores, summarizes, embeds, indexes, and enriches conversational","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/zep_memory","title":"Zep Memory | 🦜️🔗 Langchain","description":"Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.","language":"en","loc":{"lines":{"from":39,"to":64}}}}],["396",{"pageContent":"[/docs/modules/memory/integrations/] * Zep Memory ZEP MEMORY Zep\n[https://github.com/getzep/zep] is a memory server that stores, summarizes,\nembeds, indexes, and enriches conversational AI chat histories, autonomous agent\nhistories, document Q&A histories and exposes them via simple, low-latency APIs.\nKey Features: * Long-term memory persistence, with access to historical messages\nirrespective of your summarization strategy. * Auto-summarization of memory\nmessages based on a configurable message window. A series of summaries are\nstored, providing flexibility for future summarization strategies. * Vector\nsearch over memories, with messages automatically embedded on creation. *\nAuto-token counting of memories and summaries, allowing finer-grained control\nover prompt assembly. * Python [https://github.com/getzep/zep-python] and\nJavaScript [https://github.com/getzep/zep-js] SDKs. SETUP See the instructions\nfrom Zep [https://github.com/getzep/zep] for running the server","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/zep_memory","title":"Zep Memory | 🦜️🔗 Langchain","description":"Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.","language":"en","loc":{"lines":{"from":64,"to":85}}}}],["397",{"pageContent":"* Python [https://github.com/getzep/zep-python] and JavaScript\n[https://github.com/getzep/zep-js] SDKs. SETUP See the instructions from Zep\n[https://github.com/getzep/zep] for running the server locally or through an\nautomated hosting provider. USAGE import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; import { ConversationChain } from\n\"langchain/chains\"; import { ZepMemory } from \"langchain/memory/zep\"; import {\nrandomUUID } from \"crypto\"; const sessionId = randomUUID(); // This should be\nunique for each user or each user's session. const zepURL =\n\"http://localhost:8000\"; const memory = new ZepMemory({ sessionId, baseURL:\nzepURL, // This is optional. If you've enabled JWT authentication on your Zep\nserver, you can // pass it in here. See https://docs.getzep.com/deployment/auth\napiKey: \"change_this_key\", }); const model = new ChatOpenAI({ modelName:\n\"gpt-3.5-turbo\", temperature: 0, }); const chain = new ConversationChain({ llm:\nmodel, memory","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/zep_memory","title":"Zep Memory | 🦜️🔗 Langchain","description":"Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.","language":"en","loc":{"lines":{"from":85,"to":117}}}}],["398",{"pageContent":"apiKey: \"change_this_key\", }); const model = new ChatOpenAI({ modelName:\n\"gpt-3.5-turbo\", temperature: 0, }); const chain = new ConversationChain({ llm:\nmodel, memory }); console.log(\"Memory Keys:\", memory.memoryKeys); const res1 =\nawait chain.call({ input: \"Hi! I'm Jim.\" }); console.log({ res1 }); /* { res1: {\ntext: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you\ntoday?\" } } */ const res2 = await chain.call({ input: \"What did I just say my\nname was?\" }); console.log({ res2 }); /* { res1: { text: \"You said your name was\nJim.\" } } */ console.log(\"Session ID: \", sessionId); console.log(\"Memory: \",\nawait memory.loadMemoryVariables({})); API REFERENCE: * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * ConversationChain\n[/docs/api/chains/classes/ConversationChain] from langchain/chains * ZepMemory\n[/docs/api/memory_zep/classes/ZepMemory] from langchain/memory/zep Previous Xata\nChat","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/zep_memory","title":"Zep Memory | 🦜️🔗 Langchain","description":"Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.","language":"en","loc":{"lines":{"from":117,"to":161}}}}],["399",{"pageContent":"* ConversationChain [/docs/api/chains/classes/ConversationChain] from\nlangchain/chains * ZepMemory [/docs/api/memory_zep/classes/ZepMemory] from\nlangchain/memory/zep Previous Xata Chat Memory\n[/docs/modules/memory/integrations/xata] Next Agents [/docs/modules/agents/]\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/memory/integrations/zep_memory","title":"Zep Memory | 🦜️🔗 Langchain","description":"Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.","language":"en","loc":{"lines":{"from":161,"to":182}}}}],["400",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["401",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * How-to\n[/docs/modules/callbacks/how_to/background_callbacks] * Modules [/docs/modules/]\n* Guides [/docs/guides] * Ecosystem [/docs/ecosystem] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Callbacks\nCALLBACKS LangChain provides a callbacks system that allows you to hook into the\nvarious stages of your LLM application. This is useful for logging, monitoring,\nstreaming, and other tasks. You can subscribe to these events by using the\ncallbacks argument available throughout the API. This method accepts a","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":25,"to":52}}}}],["402",{"pageContent":"application. This is useful for logging, monitoring, streaming, and other tasks.\nYou can subscribe to these events by using the callbacks argument available\nthroughout the API. This method accepts a list of handler objects, which are\nexpected to implement one or more of the methods described in the API docs\n[/docs/api/callbacks/interfaces/CallbackHandlerMethods]. HOW TO USE CALLBACKS\nThe callbacks argument is available on most objects throughout the API (Chains\n[/docs/modules/chains/], Language Models [/docs/modules/model_io/models/], Tools\n[/docs/modules/agents/tools/], Agents [/docs/modules/agents/], etc.) in two\ndifferent places. CONSTRUCTOR CALLBACKS Defined in the constructor, eg. new\nLLMChain({ callbacks: [handler] }), which will be used for all calls made on\nthat object, and will be scoped to that object only, eg. if you pass a handler\nto the LLMChain constructor, it will not be used by the Model attached to that\nchain. import { ConsoleCallbackHandler } from","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":52,"to":73}}}}],["403",{"pageContent":"object, and will be scoped to that object only, eg. if you pass a handler to the\nLLMChain constructor, it will not be used by the Model attached to that chain.\nimport { ConsoleCallbackHandler } from \"langchain/callbacks\"; import { OpenAI }\nfrom \"langchain/llms/openai\"; const llm = new OpenAI({ temperature: 0, // These\ntags will be attached to all calls made with this LLM. tags: [\"example\",\n\"callbacks\", \"constructor\"], // This handler will be used for all calls made\nwith this LLM. callbacks: [new ConsoleCallbackHandler()], }); API REFERENCE: *\nConsoleCallbackHandler [/docs/api/callbacks/classes/ConsoleCallbackHandler] from\nlangchain/callbacks * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai REQUEST CALLBACKS Defined in the call()/run()/apply()\nmethods used for issuing a request, eg. chain.call({ input: '...' }, [handler]),\nwhich will be used for that specific request only, and all sub-requests that it\ncontains (eg. a call to an LLMChain","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":73,"to":100}}}}],["404",{"pageContent":"methods used for issuing a request, eg. chain.call({ input: '...' }, [handler]),\nwhich will be used for that specific request only, and all sub-requests that it\ncontains (eg. a call to an LLMChain triggers a call to a Model, which uses the\nsame handler passed in the call() method). import { ConsoleCallbackHandler }\nfrom \"langchain/callbacks\"; import { OpenAI } from \"langchain/llms/openai\";\nconst llm = new OpenAI({ temperature: 0, }); const response = await llm.call(\"1\n+ 1 =\", { // These tags will be attached only to this call to the LLM. tags:\n[\"example\", \"callbacks\", \"request\"], // This handler will be used only for this\ncall. callbacks: [new ConsoleCallbackHandler()], }); API REFERENCE: *\nConsoleCallbackHandler [/docs/api/callbacks/classes/ConsoleCallbackHandler] from\nlangchain/callbacks * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai VERBOSE MODE The verbose argument is available on most\nobjects throughout the API (Chains, Models,","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":100,"to":129}}}}],["405",{"pageContent":"langchain/callbacks * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai VERBOSE MODE The verbose argument is available on most\nobjects throughout the API (Chains, Models, Tools, Agents, etc.) as a\nconstructor argument, eg. new LLMChain({ verbose: true }), and it is equivalent\nto passing a ConsoleCallbackHandler to the callbacks argument of that object and\nall child objects. This is useful for debugging, as it will log all events to\nthe console. You can also enable verbose mode for the entire application by\nsetting the environment variable LANGCHAIN_VERBOSE=true. import { PromptTemplate\n} from \"langchain/prompts\"; import { LLMChain } from \"langchain/chains\"; import\n{ OpenAI } from \"langchain/llms/openai\"; const chain = new LLMChain({ llm: new\nOpenAI({ temperature: 0 }), prompt: PromptTemplate.fromTemplate(\"Hello,\nworld!\"), // This will enable logging of all Chain *and* LLM events to the\nconsole. verbose: true, }); API REFERENCE: *","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":129,"to":156}}}}],["406",{"pageContent":"temperature: 0 }), prompt: PromptTemplate.fromTemplate(\"Hello, world!\"), // This\nwill enable logging of all Chain *and* LLM events to the console. verbose: true,\n}); API REFERENCE: * PromptTemplate [/docs/api/prompts/classes/PromptTemplate]\nfrom langchain/prompts * LLMChain [/docs/api/chains/classes/LLMChain] from\nlangchain/chains * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai WHEN DO YOU WANT TO USE EACH OF THESE? * Constructor\ncallbacks are most useful for use cases such as logging, monitoring, etc., which\nare not specific to a single request, but rather to the entire chain. For\nexample, if you want to log all the requests made to an LLMChain, you would pass\na handler to the constructor. * Request callbacks are most useful for use cases\nsuch as streaming, where you want to stream the output of a single request to a\nspecific websocket connection, or other similar use cases. For example, if you\nwant to stream the output of a","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":156,"to":178}}}}],["407",{"pageContent":"cases such as streaming, where you want to stream the output of a single request\nto a specific websocket connection, or other similar use cases. For example, if\nyou want to stream the output of a single request to a websocket, you would pass\na handler to the call() method USAGE EXAMPLES BUILT-IN HANDLERS LangChain\nprovides a few built-in handlers that you can use to get started. These are\navailable in the langchain/callbacks module. The most basic handler is the\nConsoleCallbackHandler, which simply logs all events to the console. In the\nfuture we will add more default handlers to the library. Note that when the\nverbose flag on the object is set to true, the ConsoleCallbackHandler will be\ninvoked even without being explicitly passed in. import { ConsoleCallbackHandler\n} from \"langchain/callbacks\"; import { LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\"; import { PromptTemplate } from\n\"langchain/prompts\"; export const run = async () => {","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":178,"to":198}}}}],["408",{"pageContent":"{ LLMChain } from \"langchain/chains\"; import { OpenAI } from\n\"langchain/llms/openai\"; import { PromptTemplate } from \"langchain/prompts\";\nexport const run = async () => { const handler = new ConsoleCallbackHandler();\nconst llm = new OpenAI({ temperature: 0, callbacks: [handler] }); const prompt =\nPromptTemplate.fromTemplate(\"1 + {number} =\"); const chain = new LLMChain({\nprompt, llm, callbacks: [handler] }); const output = await chain.call({ number:\n2 }); /* Entering new llm_chain chain... Finished chain. */ console.log(output);\n/* { text: ' 3\\n\\n3 - 1 = 2' } */ // The non-enumerable key `__run` contains the\nrunId. console.log(output.__run); /* { runId:\n'90e1f42c-7cb4-484c-bf7a-70b73ef8e64b' } */ }; API REFERENCE: *\nConsoleCallbackHandler [/docs/api/callbacks/classes/ConsoleCallbackHandler] from\nlangchain/callbacks * LLMChain [/docs/api/chains/classes/LLMChain] from\nlangchain/chains * OpenAI [/docs/api/llms_openai/classes/OpenAI] from","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":198,"to":233}}}}],["409",{"pageContent":"from langchain/callbacks * LLMChain [/docs/api/chains/classes/LLMChain] from\nlangchain/chains * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts ONE-OFF\nHANDLERS You can create a one-off handler inline by passing a plain object to\nthe callbacks argument. This object should implement the CallbackHandlerMethods\n[/docs/api/callbacks/interfaces/CallbackHandlerMethods] interface. This is\nuseful if eg. you need to create a handler that you will use only for a single\nrequest, eg to stream the output of an LLM/Agent/etc to a websocket. import {\nOpenAI } from \"langchain/llms/openai\"; // To enable streaming, we pass in\n`streaming: true` to the LLM constructor. // Additionally, we pass in a handler\nfor the `handleLLMNewToken` event. const model = new OpenAI({ maxTokens: 25,\nstreaming: true, }); const response = await model.call(\"Tell me a joke.\", {\ncallbacks: [ {","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":233,"to":256}}}}],["410",{"pageContent":"in a handler for the `handleLLMNewToken` event. const model = new OpenAI({\nmaxTokens: 25, streaming: true, }); const response = await model.call(\"Tell me a\njoke.\", { callbacks: [ { handleLLMNewToken(token: string) { console.log({ token\n}); }, }, ], }); console.log(response); /* { token: '\\n' } { token: '\\n' } {\ntoken: 'Q' } { token: ':' } { token: ' Why' } { token: ' did' } { token: ' the'\n} { token: ' chicken' } { token: ' cross' } { token: ' the' } { token: '\nplayground' } { token: '?' } { token: '\\n' } { token: 'A' } { token: ':' } {\ntoken: ' To' } { token: ' get' } { token: ' to' } { token: ' the' } { token: '\nother' } { token: ' slide' } { token: '.' } Q: Why did the chicken cross the\nplayground? A: To get to the other slide. */ API REFERENCE: * OpenAI\n[/docs/api/llms_openai/classes/OpenAI] from langchain/llms/openai MULTIPLE\nHANDLERS We offer a method on the CallbackManager class that allows you to\ncreate a one-off handler. This is","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":256,"to":311}}}}],["411",{"pageContent":"* OpenAI [/docs/api/llms_openai/classes/OpenAI] from langchain/llms/openai\nMULTIPLE HANDLERS We offer a method on the CallbackManager class that allows you\nto create a one-off handler. This is useful if eg. you need to create a handler\nthat you will use only for a single request, eg to stream the output of an\nLLM/Agent/etc to a websocket. This is a more complete example that passes a\nCallbackManager to a ChatModel, and LLMChain, a Tool, and an Agent. import {\nLLMChain } from \"langchain/chains\"; import { AgentExecutor, ZeroShotAgent } from\n\"langchain/agents\"; import { BaseCallbackHandler } from \"langchain/callbacks\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\"; import { Calculator }\nfrom \"langchain/tools/calculator\"; import { AgentAction } from\n\"langchain/schema\"; import { Serialized } from \"langchain/load/serializable\";\nexport const run = async () => { // You can implement your own callback handler\nby extending BaseCallbackHandler class CustomHandler extends","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":311,"to":331}}}}],["412",{"pageContent":"{ Serialized } from \"langchain/load/serializable\"; export const run = async ()\n=> { // You can implement your own callback handler by extending\nBaseCallbackHandler class CustomHandler extends BaseCallbackHandler { name =\n\"custom_handler\"; handleLLMNewToken(token: string) { console.log(\"token\", {\ntoken }); } handleLLMStart(llm: Serialized, _prompts: string[]) {\nconsole.log(\"handleLLMStart\", { llm }); } handleChainStart(chain: Serialized) {\nconsole.log(\"handleChainStart\", { chain }); } handleAgentAction(action:\nAgentAction) { console.log(\"handleAgentAction\", action); } handleToolStart(tool:\nSerialized) { console.log(\"handleToolStart\", { tool }); } } const handler1 = new\nCustomHandler(); // Additionally, you can use the `fromMethods` method to create\na callback handler const handler2 = BaseCallbackHandler.fromMethods({\nhandleLLMStart(llm, _prompts: string[]) {","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":331,"to":363}}}}],["413",{"pageContent":"// Additionally, you can use the `fromMethods` method to create a callback\nhandler const handler2 = BaseCallbackHandler.fromMethods({ handleLLMStart(llm,\n_prompts: string[]) { console.log(\"handleLLMStart: I'm the second handler!!\", {\nllm }); }, handleChainStart(chain) { console.log(\"handleChainStart: I'm the\nsecond handler!!\", { chain }); }, handleAgentAction(action) {\nconsole.log(\"handleAgentAction\", action); }, handleToolStart(tool) {\nconsole.log(\"handleToolStart\", { tool }); }, }); // You can restrict callbacks\nto a particular object by passing it upon creation const model = new\nChatOpenAI({ temperature: 0, callbacks: [handler2], // this will issue handler2\ncallbacks related to this model streaming: true, // needed to enable streaming,\nwhich enables handleLLMNewToken }); const tools = [new Calculator()]; const\nagentPrompt = ZeroShotAgent.createPrompt(tools); const llmChain = new LLMChain({","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":363,"to":389}}}}],["414",{"pageContent":"needed to enable streaming, which enables handleLLMNewToken }); const tools =\n[new Calculator()]; const agentPrompt = ZeroShotAgent.createPrompt(tools); const\nllmChain = new LLMChain({ llm: model, prompt: agentPrompt, callbacks:\n[handler2], // this will issue handler2 callbacks related to this chain });\nconst agent = new ZeroShotAgent({ llmChain, allowedTools: [\"search\"], }); const\nagentExecutor = AgentExecutor.fromAgentAndTools({ agent, tools, }); /* * When we\npass the callback handler to the agent executor, it will be used for all *\ncallbacks related to the agent and all the objects involved in the agent's *\nexecution, in this case, the Tool, LLMChain, and LLM. * * The `handler2`\ncallback handler will only be used for callbacks related to the * LLMChain and\nLLM, since we passed it to the LLMChain and LLM objects upon creation. */ const\nresult = await agentExecutor.call( { input: \"What is 2 to the","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":389,"to":420}}}}],["415",{"pageContent":"callbacks related to the * LLMChain and LLM, since we passed it to the LLMChain\nand LLM objects upon creation. */ const result = await agentExecutor.call( {\ninput: \"What is 2 to the power of 8\", }, [handler1] ); // this is needed to see\nhandleAgentAction /* handleChainStart { chain: { name: 'agent_executor' } }\nhandleChainStart { chain: { name: 'llm_chain' } } handleChainStart: I'm the\nsecond handler!! { chain: { name: 'llm_chain' } } handleLLMStart { llm: { name:\n'openai' } } handleLLMStart: I'm the second handler!! { llm: { name: 'openai' }\n} token { token: '' } token { token: 'I' } token { token: ' can' } token {\ntoken: ' use' } token { token: ' the' } token { token: ' calculator' } token {\ntoken: ' tool' } token { token: ' to' } token { token: ' solve' } token { token:\n' this' } token { token: '.\\n' } token { token: 'Action' } token { token: ':' }\ntoken { token: ' calculator' } token { token: '\\n' } token {","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":420,"to":450}}}}],["416",{"pageContent":"} token { token: ' solve' } token { token: ' this' } token { token: '.\\n' }\ntoken { token: 'Action' } token { token: ':' } token { token: ' calculator' }\ntoken { token: '\\n' } token { token: 'Action' } token { token: ' Input' } token\n{ token: ':' } token { token: ' ' } token { token: '2' } token { token: '^' }\ntoken { token: '8' } token { token: '' } handleAgentAction { tool: 'calculator',\ntoolInput: '2^8', log: 'I can use the calculator tool to solve this.\\n' +\n'Action: calculator\\n' + 'Action Input: 2^8' } handleToolStart { tool: { name:\n'calculator' } } handleChainStart { chain: { name: 'llm_chain' } }\nhandleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }\nhandleLLMStart { llm: { name: 'openai' } } handleLLMStart: I'm the second\nhandler!! { llm: { name: 'openai' } } token { token: '' } token { token: 'That'\n} token { token: ' was' } token { token: ' easy' } token { token: '!\\n' }","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":450,"to":482}}}}],["417",{"pageContent":"handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } } token {\ntoken: '' } token { token: 'That' } token { token: ' was' } token { token: '\neasy' } token { token: '!\\n' } token { token: 'Final' } token { token: ' Answer'\n} token { token: ':' } token { token: ' ' } token { token: '256' } token {\ntoken: '' } */ console.log(result); /* { output: '256', __run: { runId:\n'26d481a6-4410-4f39-b74d-f9a4f572379a' } } */ }; API REFERENCE: * LLMChain\n[/docs/api/chains/classes/LLMChain] from langchain/chains * AgentExecutor\n[/docs/api/agents/classes/AgentExecutor] from langchain/agents * ZeroShotAgent\n[/docs/api/agents/classes/ZeroShotAgent] from langchain/agents *\nBaseCallbackHandler [/docs/api/callbacks/classes/BaseCallbackHandler] from\nlangchain/callbacks * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * Calculator\n[/docs/api/tools_calculator/classes/Calculator] from","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":482,"to":515}}}}],["418",{"pageContent":"from langchain/callbacks * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * Calculator\n[/docs/api/tools_calculator/classes/Calculator] from langchain/tools/calculator\n* AgentAction [/docs/api/schema/types/AgentAction] from langchain/schema *\nSerialized [/docs/api/load_serializable/types/Serialized] from\nlangchain/load/serializable Previous VectorStore Agent Toolkit\n[/docs/modules/agents/toolkits/vectorstore] Next Backgrounding callbacks\n[/docs/modules/callbacks/how_to/background_callbacks] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/","title":"Callbacks | 🦜️🔗 Langchain","description":"LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.","language":"en","loc":{"lines":{"from":515,"to":539}}}}],["419",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/how_to/background_callbacks","title":"Backgrounding callbacks | 🦜️🔗 Langchain","description":"By default callbacks run in-line with the your chain/LLM run. This means that if you have a slow callback you can see an impact on the overall latency of your runs. You can make callbacks not be awaited by setting the environment variable LANGCHAINCALLBACKSBACKGROUND=true. This will cause the callbacks to be run in the background, and will not impact the overall latency of your runs. When you do this you might need to await all pending callbacks before exiting your application. You can do this with the following method:","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["420",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * How-to\n[/docs/modules/callbacks/how_to/background_callbacks] * Backgrounding callbacks\n[/docs/modules/callbacks/how_to/background_callbacks] * Creating custom callback\nhandlers [/docs/modules/callbacks/how_to/create_handlers] * Callbacks in custom\nChains [/docs/modules/callbacks/how_to/creating_subclasses] * Tags\n[/docs/modules/callbacks/how_to/tags] * Modules [/docs/modules/] * Guides\n[/docs/guides] * Ecosystem [/docs/ecosystem] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] *","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/how_to/background_callbacks","title":"Backgrounding callbacks | 🦜️🔗 Langchain","description":"By default callbacks run in-line with the your chain/LLM run. This means that if you have a slow callback you can see an impact on the overall latency of your runs. You can make callbacks not be awaited by setting the environment variable LANGCHAINCALLBACKSBACKGROUND=true. This will cause the callbacks to be run in the background, and will not impact the overall latency of your runs. When you do this you might need to await all pending callbacks before exiting your application. You can do this with the following method:","language":"en","loc":{"lines":{"from":25,"to":48}}}}],["421",{"pageContent":"----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Callbacks\n[/docs/modules/callbacks/] * How-to * Backgrounding callbacks BACKGROUNDING\nCALLBACKS By default callbacks run in-line with the your chain/LLM run. This\nmeans that if you have a slow callback you can see an impact on the overall\nlatency of your runs. You can make callbacks not be awaited by setting the\nenvironment variable LANGCHAIN_CALLBACKS_BACKGROUND=true. This will cause the\ncallbacks to be run in the background, and will not impact the overall latency\nof your runs. When you do this you might need to await all pending callbacks\nbefore exiting your application. You can do this with the following method:\nimport { awaitAllCallbacks } from \"langchain/callbacks\"; await\nawaitAllCallbacks(); API REFERENCE: * awaitAllCallbacks","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/how_to/background_callbacks","title":"Backgrounding callbacks | 🦜️🔗 Langchain","description":"By default callbacks run in-line with the your chain/LLM run. This means that if you have a slow callback you can see an impact on the overall latency of your runs. You can make callbacks not be awaited by setting the environment variable LANGCHAINCALLBACKSBACKGROUND=true. This will cause the callbacks to be run in the background, and will not impact the overall latency of your runs. When you do this you might need to await all pending callbacks before exiting your application. You can do this with the following method:","language":"en","loc":{"lines":{"from":48,"to":76}}}}],["422",{"pageContent":"exiting your application. You can do this with the following method: import {\nawaitAllCallbacks } from \"langchain/callbacks\"; await awaitAllCallbacks(); API\nREFERENCE: * awaitAllCallbacks [/docs/api/callbacks/functions/awaitAllCallbacks]\nfrom langchain/callbacks Previous Callbacks [/docs/modules/callbacks/] Next\nCreating custom callback handlers\n[/docs/modules/callbacks/how_to/create_handlers] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/callbacks/how_to/background_callbacks","title":"Backgrounding callbacks | 🦜️🔗 Langchain","description":"By default callbacks run in-line with the your chain/LLM run. This means that if you have a slow callback you can see an impact on the overall latency of your runs. You can make callbacks not be awaited by setting the environment variable LANGCHAINCALLBACKSBACKGROUND=true. This will cause the callbacks to be run in the background, and will not impact the overall latency of your runs. When you do this you might need to await all pending callbacks before exiting your application. You can do this with the following method:","language":"en","loc":{"lines":{"from":76,"to":108}}}}],["423",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Agent types","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/vectorstore","title":"VectorStore Agent Toolkit | 🦜️🔗 Langchain","description":"This example shows how to load and use an agent with a vectorstore toolkit.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["424",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Agent types\n[/docs/modules/agents/agent_types/] * How-to\n[/docs/modules/agents/how_to/callbacks] * Tools [/docs/modules/agents/tools/] *\nToolkits [/docs/modules/agents/toolkits/] * JSON Agent Toolkit\n[/docs/modules/agents/toolkits/json] * OpenAPI Agent Toolkit\n[/docs/modules/agents/toolkits/openapi] * AWS Step Functions Toolkit\n[/docs/modules/agents/toolkits/sfn_agent] * SQL Agent Toolkit\n[/docs/modules/agents/toolkits/sql] * VectorStore Agent Toolkit\n[/docs/modules/agents/toolkits/vectorstore] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/vectorstore","title":"VectorStore Agent Toolkit | 🦜️🔗 Langchain","description":"This example shows how to load and use an agent with a vectorstore toolkit.","language":"en","loc":{"lines":{"from":25,"to":45}}}}],["425",{"pageContent":"* Modules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem]\n* Additional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Agents\n[/docs/modules/agents/] * Toolkits [/docs/modules/agents/toolkits/] *\nVectorStore Agent Toolkit VECTORSTORE AGENT TOOLKIT This example shows how to\nload and use an agent with a vectorstore toolkit. import { OpenAI } from\n\"langchain/llms/openai\"; import { HNSWLib } from\n\"langchain/vectorstores/hnswlib\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { RecursiveCharacterTextSplitter } from\n\"langchain/text_splitter\"; import * as fs from \"fs\"; import {\nVectorStoreToolkit, createVectorStoreAgent, VectorStoreInfo, } from\n\"langchain/agents\"; const model = new OpenAI({","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/vectorstore","title":"VectorStore Agent Toolkit | 🦜️🔗 Langchain","description":"This example shows how to load and use an agent with a vectorstore toolkit.","language":"en","loc":{"lines":{"from":45,"to":77}}}}],["426",{"pageContent":"} from \"langchain/text_splitter\"; import * as fs from \"fs\"; import {\nVectorStoreToolkit, createVectorStoreAgent, VectorStoreInfo, } from\n\"langchain/agents\"; const model = new OpenAI({ temperature: 0 }); /* Load in the\nfile we want to do question answering over */ const text =\nfs.readFileSync(\"state_of_the_union.txt\", \"utf8\"); /* Split the text into chunks\nusing character, not token, size */ const textSplitter = new\nRecursiveCharacterTextSplitter({ chunkSize: 1000 }); const docs = await\ntextSplitter.createDocuments([text]); /* Create the vectorstore */ const\nvectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings()); /*\nCreate the agent */ const vectorStoreInfo: VectorStoreInfo = { name:\n\"state_of_union_address\", description: \"the most recent state of the Union\naddress\", vectorStore, }; const toolkit = new\nVectorStoreToolkit(vectorStoreInfo, model); const agent =\ncreateVectorStoreAgent(model, toolkit); const input = \"What did biden say about\nKetanji Brown","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/vectorstore","title":"VectorStore Agent Toolkit | 🦜️🔗 Langchain","description":"This example shows how to load and use an agent with a vectorstore toolkit.","language":"en","loc":{"lines":{"from":77,"to":105}}}}],["427",{"pageContent":"vectorStore, }; const toolkit = new VectorStoreToolkit(vectorStoreInfo, model);\nconst agent = createVectorStoreAgent(model, toolkit); const input = \"What did\nbiden say about Ketanji Brown Jackson is the state of the union address?\";\nconsole.log(`Executing: ${input}`); const result = await agent.call({ input });\nconsole.log(`Got output ${result.output}`); console.log( `Got intermediate steps\n${JSON.stringify(result.intermediateSteps, null, 2)}` ); API REFERENCE: * OpenAI\n[/docs/api/llms_openai/classes/OpenAI] from langchain/llms/openai * HNSWLib\n[/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter * VectorStoreToolkit\n[/docs/api/agents/classes/VectorStoreToolkit] from langchain/agents *","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/vectorstore","title":"VectorStore Agent Toolkit | 🦜️🔗 Langchain","description":"This example shows how to load and use an agent with a vectorstore toolkit.","language":"en","loc":{"lines":{"from":105,"to":131}}}}],["428",{"pageContent":"[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter * VectorStoreToolkit\n[/docs/api/agents/classes/VectorStoreToolkit] from langchain/agents *\ncreateVectorStoreAgent [/docs/api/agents/functions/createVectorStoreAgent] from\nlangchain/agents * VectorStoreInfo [/docs/api/agents/interfaces/VectorStoreInfo]\nfrom langchain/agents Previous SQL Agent Toolkit\n[/docs/modules/agents/toolkits/sql] Next Callbacks [/docs/modules/callbacks/]\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/agents/toolkits/vectorstore","title":"VectorStore Agent Toolkit | 🦜️🔗 Langchain","description":"This example shows how to load and use an agent with a vectorstore toolkit.","language":"en","loc":{"lines":{"from":131,"to":154}}}}],["429",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/guides","title":"Guides | 🦜️🔗 Langchain","description":"Design guides for key parts of the development process","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["430",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Deployment [/docs/guides/deployment/]\n* Evaluation [/docs/guides/evaluation/] * Fallbacks [/docs/guides/fallbacks] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Guides GUIDES Design guides for key parts of\nthe development process 🗃️ DEPLOYMENT 1 items [/docs/guides/deployment/] 🗃️\nEVALUATION 4 items [/docs/guides/evaluation/] 📄️ FALLBACKS When working with\nlanguage models, you may often encounter issues from the underlying APIs,","metadata":{"source":"https://js.langchain.com/docs/guides","title":"Guides | 🦜️🔗 Langchain","description":"Design guides for key parts of the development process","language":"en","loc":{"lines":{"from":25,"to":69}}}}],["431",{"pageContent":"items [/docs/guides/deployment/] 🗃️ EVALUATION 4 items\n[/docs/guides/evaluation/] 📄️ FALLBACKS When working with language models, you\nmay often encounter issues from the underlying APIs, e.g. rate limits or\ndowntime. [/docs/guides/fallbacks] Previous Modules [/docs/modules/] Next\nDeployment [/docs/guides/deployment/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/guides","title":"Guides | 🦜️🔗 Langchain","description":"Design guides for key parts of the development process","language":"en","loc":{"lines":{"from":69,"to":104}}}}],["432",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/ecosystem","title":"Ecosystem | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["433",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nIntegrations [/docs/ecosystem/integrations/] * LangSmith\n[https://docs.smith.langchain.com] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Ecosystem ECOSYSTEM 🗃️ INTEGRATIONS 5 items\n[/docs/ecosystem/integrations/] 🔗 LANGSMITH [https://docs.smith.langchain.com]\nPrevious Fallbacks [/docs/guides/fallbacks] Next Integrations\n[/docs/ecosystem/integrations/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter","metadata":{"source":"https://js.langchain.com/docs/ecosystem","title":"Ecosystem | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":25,"to":68}}}}],["434",{"pageContent":"* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/ecosystem","title":"Ecosystem | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":68,"to":79}}}}],["435",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/additional_resources","title":"Additional resources | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["436",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * LangChain Expression\nLanguage Cheatsheet [/docs/additional_resources/expression_language_cheatsheet]\n* Scrimba interactive guides [/docs/additional_resources/scrimba] * Gallery\n[https://github.com/kyrolabs/awesome-langchain] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Additional resources ADDITIONAL RESOURCES 📄️\nLANGCHAIN EXPRESSION LANGUAGE CHEATSHEET For a quick reference for LangChain\nExpression Language, check out the below","metadata":{"source":"https://js.langchain.com/docs/additional_resources","title":"Additional resources | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":25,"to":53}}}}],["437",{"pageContent":"[/docs/api/] * / * Additional resources ADDITIONAL RESOURCES 📄️ LANGCHAIN\nEXPRESSION LANGUAGE CHEATSHEET For a quick reference for LangChain Expression\nLanguage, check out the below overview/cheatsheet made by @zhanghaili0610:\n[/docs/additional_resources/expression_language_cheatsheet] 📄️ SCRIMBA\nINTERACTIVE GUIDES Scrimba is a code-learning platform that allows you to\ninteractively edit and run [/docs/additional_resources/scrimba] 🔗 GALLERY\n[https://github.com/kyrolabs/awesome-langchain] Previous Unstructured\n[/docs/ecosystem/integrations/unstructured] Next LangChain Expression Language\nCheatsheet [/docs/additional_resources/expression_language_cheatsheet] Community\n* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023","metadata":{"source":"https://js.langchain.com/docs/additional_resources","title":"Additional resources | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":53,"to":97}}}}],["438",{"pageContent":"* Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/additional_resources","title":"Additional resources | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":97,"to":104}}}}],["439",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["440",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Community navigator COMMUNITY NAVIGATOR Hi!\nThanks for being here. We’re lucky to have a community of so many passionate\ndevelopers building with LangChain–we have so much to teach and learn from each\nother. Community members contribute code, host meetups, write blog posts,\namplify each other’s work, become each other's customers and collaborators, and\nso much more. Whether you’re new to LangChain, looking to","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":25,"to":51}}}}],["441",{"pageContent":"members contribute code, host meetups, write blog posts, amplify each other’s\nwork, become each other's customers and collaborators, and so much more. Whether\nyou’re new to LangChain, looking to go deeper, or just want to get more exposure\nto the world of building with LLMs, this page can point you in the right\ndirection. * 🦜 Contribute to LangChain * 🌍 Meetups, Events, and Hackathons *\n📣 Help Us Amplify Your Work * 💬 Stay in the loop 🦜 CONTRIBUTE TO LANGCHAIN\nLangChain is the product of over 5,000+ contributions by 1,500+ contributors,\nand there is **still** so much to do together. Here are some ways to get\ninvolved: * Open a pull request\n[https://github.com/hwchase17/langchainjs/issues]: we’d appreciate all forms of\ncontributions–new features, infrastructure improvements, better documentation,\nbug fixes, etc. If you have an improvement or an idea, we’d love to work on it\nwith you. * Read our contributor guidelines:","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":51,"to":74}}}}],["442",{"pageContent":"features, infrastructure improvements, better documentation, bug fixes, etc. If\nyou have an improvement or an idea, we’d love to work on it with you. * Read our\ncontributor guidelines:\n[https://github.com/hwchase17/langchainjs/blob/main/CONTRIBUTING.md] We ask\ncontributors to follow a \"fork and pull request\"\n[https://docs.github.com/en/get-started/quickstart/contributing-to-projects] workflow,\nrun a few local checks for formatting, linting, and testing before submitting,\nand follow certain documentation and testing conventions. * Become an expert:\nour experts help the community by answering product questions in Discord. If\nthat’s a role you’d like to play, we’d be so grateful! (And we have some special\nexperts-only goodies/perks we can tell you more about). Send us an email to\nintroduce yourself at hello@langchain.dev [hello@langchain.dev] and we’ll take\nit from there! * Integrate with LangChain: if your product integrates with\nLangChain–or aspires to–we want","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":74,"to":84}}}}],["443",{"pageContent":"email to introduce yourself at hello@langchain.dev [hello@langchain.dev] and\nwe’ll take it from there! * Integrate with LangChain: if your product integrates\nwith LangChain–or aspires to–we want to help make sure the experience is as\nsmooth as possible for you and end users. Send us an email at\nhello@langchain.dev [hello@langchain.dev] and tell us what you’re working on. *\nBecome an Integration Maintainer: Partner with our team to ensure your\nintegration stays up-to-date and talk directly with users (and answer their\ninquiries) in our Discord. Introduce yourself at hello@langchain.dev\n[hello@langchain.dev] if you’d like to explore this role. 🌍 MEETUPS, EVENTS,\nAND HACKATHONS One of our favorite things about working in AI is how much\nenthusiasm there is for building together. We want to help make that as easy and\nimpactful for you as possible! * Find a meetup, hackathon, or webinar: you can\nfind the one for you on on our global events calendar","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":84,"to":99}}}}],["444",{"pageContent":"for building together. We want to help make that as easy and impactful for you\nas possible! * Find a meetup, hackathon, or webinar: you can find the one for\nyou on on our global events calendar\n[https://mirror-feeling-d80.notion.site/0bc81da76a184297b86ca8fc782ee9a3?v=0d80342540df465396546976a50cfb3f].\n* Submit an event to our calendar: email us at events@langchain.dev\n[events@langchain.dev] with a link to your event page! We can also help you\nspread the word with our local communities. * Host a meetup: If you want to\nbring a group of builders together, we want to help! We can publicize your event\non our event calendar/Twitter, share with our local communities in Discord, send\nswag, or potentially hook you up with a sponsor. Email us at\nevents@langchain.dev [events@langchain.dev] to tell us about your event! *\nBecome a meetup sponsor: we often hear from groups of builders that want to get\ntogether, but are blocked or limited on some dimension (space to host,","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":99,"to":110}}}}],["445",{"pageContent":"to tell us about your event! * Become a meetup sponsor: we often hear from\ngroups of builders that want to get together, but are blocked or limited on some\ndimension (space to host, budget for snacks, prizes to distribute, etc.). If\nyou’d like to help, send us an email to events@langchain.dev\n[events@langchain.dev] we can share more about how it works! * Speak at an\nevent: meetup hosts are always looking for great speakers, presenters, and\npanelists. If you’d like to do that at an event, send us an email to\nhello@langchain.dev [hello@langchain.dev] with more information about yourself,\nwhat you want to talk about, and what city you’re based in and we’ll try to\nmatch you with an upcoming event! * Tell us about your LLM community: If you\nhost or participate in a community that would welcome support from LangChain\nand/or our team, send us an email at hello@langchain.dev [hello@langchain.dev]\nand let us know how we can help. 📣 HELP US AMPLIFY YOUR WORK If you’re","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":110,"to":123}}}}],["446",{"pageContent":"that would welcome support from LangChain and/or our team, send us an email at\nhello@langchain.dev [hello@langchain.dev] and let us know how we can help.\n📣 HELP US AMPLIFY YOUR WORK If you’re working on something you’re proud of, and\nthink the LangChain community would benefit from knowing about it, we want to\nhelp you show it off. * Post about your work and mention us: we love hanging out\non Twitter to see what people in the space are talking about and working on. If\nyou tag @langchainai [https://twitter.com/LangChainAI], we’ll almost certainly\nsee it and can show you some love. * Publish something on our blog: if you’re\nwriting about your experience building with LangChain, we’d love to post (or\ncrosspost) it on our blog! E-mail hello@langchain.dev [hello@langchain.dev] with\na draft of your post! Or even an idea for something you want to write about. *\nGet your product onto our integrations hub\n[https://integrations.langchain.com/]: Many developers take","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":123,"to":138}}}}],["447",{"pageContent":"with a draft of your post! Or even an idea for something you want to write\nabout. * Get your product onto our integrations hub\n[https://integrations.langchain.com/]: Many developers take advantage of our\nseamless integrations with other products, and come to our integrations hub to\nfind out who those are. If you want to get your product up there, tell us about\nit (and how it works with LangChain) at hello@langchain.dev\n[hello@langchain.dev]. ☀️ STAY IN THE LOOP Here’s where our team hangs out,\ntalks shop, spotlights cool work, and shares what we’re up to. We’d love to see\nyou there too. * Twitter [https://twitter.com/LangChainAI]: we post about what\nwe’re working on and what cool things we’re seeing in the space. If you tag\n@langchainai in your post, we’ll almost certainly see it, and can snow you some\nlove! * Discord [https://discord.gg/6adMQxSpJS]: connect with with >30k\ndevelopers who are building with LangChain * GitHub","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":138,"to":152}}}}],["448",{"pageContent":"in your post, we’ll almost certainly see it, and can snow you some love! *\nDiscord [https://discord.gg/6adMQxSpJS]: connect with with >30k developers who\nare building with LangChain * GitHub [https://github.com/hwchase17/langchainjs]:\nopen pull requests, contribute to a discussion, and/or contribute * Subscribe to\nour bi-weekly Release Notes [https://6w1pwbss0py.typeform.com/to/KjZB1auB]: a\ntwice/month email roundup of the coolest things going on in our orbit * Slack:\nif you’re building an application in production at your company, we’d love to\nget into a Slack channel together. Fill out this form\n[https://airtable.com/appwQzlErAS2qiP0L/shrGtGaVBVAz7NcV2] and we’ll get in\ntouch about setting one up. Previous Scrimba interactive guides\n[/docs/additional_resources/scrimba] Next langchain [/docs/api/] Community *\nDiscord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":152,"to":172}}}}],["449",{"pageContent":"* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/community","title":"Community navigator | 🦜️🔗 Langchain","description":"Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.","language":"en","loc":{"lines":{"from":172,"to":183}}}}],["450",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases USE CASES Walkthroughs\nof common end-to-end use cases 🗃️ QA AND CHAT OVER DOCUMENTS 2 items\n[/docs/use_cases/question_answering/] 📄️ TABULAR QUESTION ANSWERING Lots of\ndata and information is stored in tabular data, whether it be","metadata":{"source":"https://js.langchain.com/docs/use_cases/","title":"Use cases | 🦜️🔗 Langchain","description":"Walkthroughs of common end-to-end use cases","language":"en","loc":{"lines":{"from":1,"to":38}}}}],["451",{"pageContent":"use cases 🗃️ QA AND CHAT OVER DOCUMENTS 2 items\n[/docs/use_cases/question_answering/] 📄️ TABULAR QUESTION ANSWERING Lots of\ndata and information is stored in tabular data, whether it be csvs, excel\nsheets, or SQL tables. [/docs/use_cases/tabular] 📄️ INTERACTING WITH APIS Lots\nof data and information is stored behind APIs. [/docs/use_cases/api] 📄️\nSUMMARIZATION A common use case is wanting to summarize long documents.\n[/docs/use_cases/summarization] 🗃️ AGENT SIMULATIONS 1 items\n[/docs/use_cases/agent_simulations/] 🗃️ AUTONOMOUS AGENTS 3 items\n[/docs/use_cases/autonomous_agents/] 📄️ CHATBOTS Language models are good at\nproducing text, which makes them ideal for creating chatbots.\n[/docs/use_cases/chatbots] 📄️ EXTRACTION Most APIs and databases still deal\nwith structured information. Therefore, in order to better work with those, it\ncan be useful to extract structured information from text. Examples of this","metadata":{"source":"https://js.langchain.com/docs/use_cases/","title":"Use cases | 🦜️🔗 Langchain","description":"Walkthroughs of common end-to-end use cases","language":"en","loc":{"lines":{"from":38,"to":93}}}}],["452",{"pageContent":"APIs and databases still deal with structured information. Therefore, in order\nto better work with those, it can be useful to extract structured information\nfrom text. Examples of this include: [/docs/use_cases/extraction] Next QA and\nChat over Documents [/docs/use_cases/question_answering/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/","title":"Use cases | 🦜️🔗 Langchain","description":"Walkthroughs of common end-to-end use cases","language":"en","loc":{"lines":{"from":93,"to":112}}}}],["453",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] *\nChatbots CHATBOTS Language models are good at producing text, which makes them\nideal for creating chatbots. Aside from the base prompts/LLMs, an important\nconcept to know for Chatbots is memory. Most chat based","metadata":{"source":"https://js.langchain.com/docs/use_cases/chatbots/","title":"Chatbots | 🦜️🔗 Langchain","description":"Language models are good at producing text, which makes them ideal for creating chatbots.","language":"en","loc":{"lines":{"from":1,"to":28}}}}],["454",{"pageContent":"models are good at producing text, which makes them ideal for creating chatbots.\nAside from the base prompts/LLMs, an important concept to know for Chatbots is\nmemory. Most chat based applications rely on remembering what happened in\nprevious interactions, which memory is designed to help with. You might find the\nfollowing pages interesting: * Memory concepts and examples\n[/docs/modules/memory/]: Explanation of key concepts related to memory along\nwith how-to's and examples. * Conversation Agent\n[/docs/modules/agents/agent_types/chat_conversation_agent]: A notebook walking\nthrough how to create an agent optimized for conversation. Previous BabyAGI\n[/docs/use_cases/autonomous_agents/baby_agi] Next Extraction\n[/docs/use_cases/extraction] Community * Discord [https://discord.gg/cU2adEyC7w]\n* Twitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage","metadata":{"source":"https://js.langchain.com/docs/use_cases/chatbots/","title":"Chatbots | 🦜️🔗 Langchain","description":"Language models are good at producing text, which makes them ideal for creating chatbots.","language":"en","loc":{"lines":{"from":28,"to":54}}}}],["455",{"pageContent":"* Twitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/chatbots/","title":"Chatbots | 🦜️🔗 Langchain","description":"Language models are good at producing text, which makes them ideal for creating chatbots.","language":"en","loc":{"lines":{"from":54,"to":64}}}}],["456",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] *\nInteracting with APIs INTERACTING WITH APIS Lots of data and information is\nstored behind APIs. This page covers all resources available in LangChain for\nworking with APIs. CHAINS If you are just getting started and","metadata":{"source":"https://js.langchain.com/docs/use_cases/api","title":"Interacting with APIs | 🦜️🔗 Langchain","description":"Lots of data and information is stored behind APIs.","language":"en","loc":{"lines":{"from":1,"to":32}}}}],["457",{"pageContent":"WITH APIS Lots of data and information is stored behind APIs. This page covers\nall resources available in LangChain for working with APIs. CHAINS If you are\njust getting started and you have relatively simple APIs, you should get started\nwith chains. Chains are a sequence of predetermined steps, so they are good to\nget started with as they give you more control and let you understand what is\nhappening better. * OpenAPI Chain\n[/docs/modules/chains/additional/openai_functions/openapi] AGENTS Agents are\nmore complex, and involve multiple queries to the LLM to understand what to do.\nThe downside of agents are that you have less control. The upside is that they\nare more powerful, which allows you to use them on larger and more complex\nschemas. * OpenAPI Agent [/docs/modules/agents/toolkits/openapi] Previous\nTabular Question Answering [/docs/use_cases/tabular] Next Summarization\n[/docs/use_cases/summarization] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter","metadata":{"source":"https://js.langchain.com/docs/use_cases/api","title":"Interacting with APIs | 🦜️🔗 Langchain","description":"Lots of data and information is stored behind APIs.","language":"en","loc":{"lines":{"from":32,"to":61}}}}],["458",{"pageContent":"Question Answering [/docs/use_cases/tabular] Next Summarization\n[/docs/use_cases/summarization] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/api","title":"Interacting with APIs | 🦜️🔗 Langchain","description":"Lots of data and information is stored behind APIs.","language":"en","loc":{"lines":{"from":61,"to":78}}}}],["459",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] *\nSummarization SUMMARIZATION A common use case is wanting to summarize long\ndocuments. This naturally runs into the context window limitations. Unlike in\nquestion-answering, you can't just do some semantic search hacks to","metadata":{"source":"https://js.langchain.com/docs/use_cases/summarization","title":"Summarization | 🦜️🔗 Langchain","description":"A common use case is wanting to summarize long documents.","language":"en","loc":{"lines":{"from":1,"to":28}}}}],["460",{"pageContent":"common use case is wanting to summarize long documents. This naturally runs into\nthe context window limitations. Unlike in question-answering, you can't just do\nsome semantic search hacks to only select the chunks of text most relevant to\nthe question (because, in this case, there is no particular question - you want\nto summarize everything). So what do you do then? To get started, we would\nrecommend checking out the summarization chain, which attacks this problem in a\nrecursive manner. * Summarization Chain [/docs/modules/chains/popular/summarize]\nEXAMPLE Here's an example of how you can use the RefineDocumentsChain\n[/docs/modules/chains/document/refine] to summarize documents loaded from a\nYouTube video: import { loadSummarizationChain } from \"langchain/chains\"; import\n{ ChatAnthropic } from \"langchain/chat_models/anthropic\"; import {\nSearchApiLoader } from \"langchain/document_loaders/web/searchapi\"; import {\nPromptTemplate } from \"langchain/prompts\"; import {","metadata":{"source":"https://js.langchain.com/docs/use_cases/summarization","title":"Summarization | 🦜️🔗 Langchain","description":"A common use case is wanting to summarize long documents.","language":"en","loc":{"lines":{"from":28,"to":46}}}}],["461",{"pageContent":"{ ChatAnthropic } from \"langchain/chat_models/anthropic\"; import {\nSearchApiLoader } from \"langchain/document_loaders/web/searchapi\"; import {\nPromptTemplate } from \"langchain/prompts\"; import { TokenTextSplitter } from\n\"langchain/text_splitter\"; const loader = new SearchApiLoader({ engine:\n\"youtube_transcripts\", video_id: \"WTOm65IZneg\", }); const docs = await\nloader.load(); const splitter = new TokenTextSplitter({ chunkSize: 10000,\nchunkOverlap: 250, }); const docsSummary = await splitter.splitDocuments(docs);\nconst llmSummary = new ChatAnthropic({ modelName: \"claude-2\", temperature: 0.3,\n}); const summaryTemplate = ` You are an expert in summarizing YouTube videos.\nYour goal is to create a summary of a podcast. Below you find the transcript of\na podcast: -------- {text} -------- The transcript of the podcast will also be\nused as the basis for a question and answer bot. Provide some examples questions\nand answers that could be asked about the podcast. Make these","metadata":{"source":"https://js.langchain.com/docs/use_cases/summarization","title":"Summarization | 🦜️🔗 Langchain","description":"A common use case is wanting to summarize long documents.","language":"en","loc":{"lines":{"from":46,"to":79}}}}],["462",{"pageContent":"transcript of the podcast will also be used as the basis for a question and\nanswer bot. Provide some examples questions and answers that could be asked\nabout the podcast. Make these questions very specific. Total output will be a\nsummary of the video and a list of example questions the user could ask of the\nvideo. SUMMARY AND QUESTIONS: `; const SUMMARY_PROMPT =\nPromptTemplate.fromTemplate(summaryTemplate); const summaryRefineTemplate = `\nYou are an expert in summarizing YouTube videos. Your goal is to create a\nsummary of a podcast. We have provided an existing summary up to a certain\npoint: {existing_answer} Below you find the transcript of a podcast: --------\n{text} -------- Given the new context, refine the summary and example questions.\nThe transcript of the podcast will also be used as the basis for a question and\nanswer bot. Provide some examples questions and answers that could be asked\nabout the podcast. Make these questions very specific. If the context isn't\nuseful,","metadata":{"source":"https://js.langchain.com/docs/use_cases/summarization","title":"Summarization | 🦜️🔗 Langchain","description":"A common use case is wanting to summarize long documents.","language":"en","loc":{"lines":{"from":79,"to":103}}}}],["463",{"pageContent":"be used as the basis for a question and answer bot. Provide some examples\nquestions and answers that could be asked about the podcast. Make these\nquestions very specific. If the context isn't useful, return the original\nsummary and questions. Total output will be a summary of the video and a list of\nexample questions the user could ask of the video. SUMMARY AND QUESTIONS: `;\nconst SUMMARY_REFINE_PROMPT = PromptTemplate.fromTemplate( summaryRefineTemplate\n); const summarizeChain = loadSummarizationChain(llmSummary, { type: \"refine\",\nverbose: true, questionPrompt: SUMMARY_PROMPT, refinePrompt:\nSUMMARY_REFINE_PROMPT, }); const summary = await\nsummarizeChain.run(docsSummary); console.log(summary); /* Here is a summary of\nthe key points from the podcast transcript: - Jimmy helps provide hearing aids\nand cochlear implants to deaf and hard-of-hearing people who can't afford them.\nHe helps over 1,000 people hear again. - Jimmy surprises recipients with $10,000\ncash","metadata":{"source":"https://js.langchain.com/docs/use_cases/summarization","title":"Summarization | 🦜️🔗 Langchain","description":"A common use case is wanting to summarize long documents.","language":"en","loc":{"lines":{"from":103,"to":132}}}}],["464",{"pageContent":"helps provide hearing aids and cochlear implants to deaf and hard-of-hearing\npeople who can't afford them. He helps over 1,000 people hear again. - Jimmy\nsurprises recipients with $10,000 cash gifts in addition to the hearing aids. He\nalso gifts things like jet skis, basketball game tickets, and trips to concerts.\n- Jimmy travels internationally to provide hearing aids, visiting places like\nMexico, Guatemala, Brazil, South Africa, Malawi, and Indonesia. - Jimmy donates\n$100,000 to organizations around the world that teach sign language. - The\nrecipients are very emotional and grateful to be able to hear their loved ones\nagain. Here are some example questions and answers about the podcast: Q: How\nmany people did Jimmy help regain their hearing? A: Jimmy helped over 1,000\npeople regain their hearing. Q: What types of hearing devices did Jimmy provide\nto the recipients? A: Jimmy provided cutting-edge hearing aids and cochlear\nimplants. Q: In addition to the","metadata":{"source":"https://js.langchain.com/docs/use_cases/summarization","title":"Summarization | 🦜️🔗 Langchain","description":"A common use case is wanting to summarize long documents.","language":"en","loc":{"lines":{"from":132,"to":150}}}}],["465",{"pageContent":"people regain their hearing. Q: What types of hearing devices did Jimmy provide\nto the recipients? A: Jimmy provided cutting-edge hearing aids and cochlear\nimplants. Q: In addition to the hearing devices, what surprise gifts did Jimmy\ngive some recipients? A: In addition to hearing devices, Jimmy surprised some\nrecipients with $10,000 cash gifts, jet skis, basketball game tickets, and\nconcert tickets. Q: What countries did Jimmy travel to in order to help people?\nA: Jimmy traveled to places like Mexico, Guatemala, Brazil, South Africa,\nMalawi, and Indonesia. Q: How much money did Jimmy donate to organizations that\nteach sign language? A: Jimmy donated $100,000 to sign language organizations\naround the world. Q: How did the recipients react when they were able to hear\nagain? A: The recipients were very emotional and grateful, with many crying\ntears of joy at being able to hear their loved ones again. */ API REFERENCE: *\nloadSummarizationChain","metadata":{"source":"https://js.langchain.com/docs/use_cases/summarization","title":"Summarization | 🦜️🔗 Langchain","description":"A common use case is wanting to summarize long documents.","language":"en","loc":{"lines":{"from":150,"to":173}}}}],["466",{"pageContent":"able to hear again? A: The recipients were very emotional and grateful, with\nmany crying tears of joy at being able to hear their loved ones again. */ API\nREFERENCE: * loadSummarizationChain\n[/docs/api/chains/functions/loadSummarizationChain] from langchain/chains *\nChatAnthropic [/docs/api/chat_models_anthropic/classes/ChatAnthropic] from\nlangchain/chat_models/anthropic * SearchApiLoader\n[/docs/api/document_loaders_web_searchapi/classes/SearchApiLoader] from\nlangchain/document_loaders/web/searchapi * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nTokenTextSplitter [/docs/api/text_splitter/classes/TokenTextSplitter] from\nlangchain/text_splitter Previous Interacting with APIs [/docs/use_cases/api]\nNext Agent Simulations [/docs/use_cases/agent_simulations/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS","metadata":{"source":"https://js.langchain.com/docs/use_cases/summarization","title":"Summarization | 🦜️🔗 Langchain","description":"A common use case is wanting to summarize long documents.","language":"en","loc":{"lines":{"from":173,"to":201}}}}],["467",{"pageContent":"* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/summarization","title":"Summarization | 🦜️🔗 Langchain","description":"A common use case is wanting to summarize long documents.","language":"en","loc":{"lines":{"from":201,"to":212}}}}],["468",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Generative Agents\n[/docs/use_cases/agent_simulations/generative_agents] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] *\nAgent Simulations AGENT SIMULATIONS Agent simulations involve taking multiple\nagents and having them interact with each other. They tend to","metadata":{"source":"https://js.langchain.com/docs/use_cases/agent_simulations/","title":"Agent Simulations | 🦜️🔗 Langchain","description":"Agent simulations involve taking multiple agents and having them interact with each other.","language":"en","loc":{"lines":{"from":1,"to":30}}}}],["469",{"pageContent":"* / * Use cases [/docs/use_cases] * Agent Simulations AGENT SIMULATIONS Agent\nsimulations involve taking multiple agents and having them interact with each\nother. They tend to use a simulation environment with an LLM as their \"core\" and\nhelper classes to prompt them to ingest certain inputs such as prebuilt\n\"observations\", and react to new stimuli. They also benefit from long-term\nmemory so that they can preserve state between interactions. Like Autonomous\nAgents, Agent Simulations are still experimental and based on papers such as\nthis one [https://arxiv.org/abs/2304.03442]. 📄️ GENERATIVE AGENTS This script\nimplements a generative agent based on the paper Generative Agents: Interactive\nSimulacra of Human Behavior by Park, et. al.\n[/docs/use_cases/agent_simulations/generative_agents] Previous Summarization\n[/docs/use_cases/summarization] Next Generative Agents\n[/docs/use_cases/agent_simulations/generative_agents] Community * Discord\n[https://discord.gg/cU2adEyC7w] *","metadata":{"source":"https://js.langchain.com/docs/use_cases/agent_simulations/","title":"Agent Simulations | 🦜️🔗 Langchain","description":"Agent simulations involve taking multiple agents and having them interact with each other.","language":"en","loc":{"lines":{"from":30,"to":62}}}}],["470",{"pageContent":"Agents [/docs/use_cases/agent_simulations/generative_agents] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/agent_simulations/","title":"Agent Simulations | 🦜️🔗 Langchain","description":"Agent simulations involve taking multiple agents and having them interact with each other.","language":"en","loc":{"lines":{"from":62,"to":76}}}}],["471",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * SalesGPT\n[/docs/use_cases/autonomous_agents/sales_gpt] * AutoGPT\n[/docs/use_cases/autonomous_agents/auto_gpt] * BabyAGI\n[/docs/use_cases/autonomous_agents/baby_agi] * Chatbots\n[/docs/use_cases/chatbots] * Extraction [/docs/use_cases/extraction] * / * Use\ncases [/docs/use_cases] * Autonomous Agents AUTONOMOUS","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/","title":"Autonomous Agents | 🦜️🔗 Langchain","description":"Autonomous Agents are agents that designed to be more long running. You give them one or multiple long term goals, and they independently execute towards those goals. The applications combine tool usage and long term memory.","language":"en","loc":{"lines":{"from":1,"to":28}}}}],["472",{"pageContent":"* Chatbots [/docs/use_cases/chatbots] * Extraction [/docs/use_cases/extraction]\n* / * Use cases [/docs/use_cases] * Autonomous Agents AUTONOMOUS AGENTS\nAutonomous Agents are agents that designed to be more long running. You give\nthem one or multiple long term goals, and they independently execute towards\nthose goals. The applications combine tool usage and long term memory. At the\nmoment, Autonomous Agents are fairly experimental and based off of other\nopen-source projects. By implementing these open source projects in LangChain\nprimitives we can get the benefits of LangChain - easy switching and\nexperimenting with multiple LLMs, usage of different vectorstores as memory,\nusage of LangChain's collection of tools. 📄️ SALESGPT This notebook\ndemonstrates an implementation of a Context-Aware AI Sales agent with a Product\nKnowledge Base. [/docs/use_cases/autonomous_agents/sales_gpt] 📄️ AUTOGPT\nOriginal","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/","title":"Autonomous Agents | 🦜️🔗 Langchain","description":"Autonomous Agents are agents that designed to be more long running. You give them one or multiple long term goals, and they independently execute towards those goals. The applications combine tool usage and long term memory.","language":"en","loc":{"lines":{"from":28,"to":55}}}}],["473",{"pageContent":"SALESGPT This notebook demonstrates an implementation of a Context-Aware AI\nSales agent with a Product Knowledge Base.\n[/docs/use_cases/autonomous_agents/sales_gpt] 📄️ AUTOGPT Original\nRepo//github.com/Significant-Gravitas/Auto-GPT\n[/docs/use_cases/autonomous_agents/auto_gpt] 📄️ BABYAGI Original\nRepo//github.com/yoheinakajima/babyagi\n[/docs/use_cases/autonomous_agents/baby_agi] Previous Generative Agents\n[/docs/use_cases/agent_simulations/generative_agents] Next SalesGPT\n[/docs/use_cases/autonomous_agents/sales_gpt] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/","title":"Autonomous Agents | 🦜️🔗 Langchain","description":"Autonomous Agents are agents that designed to be more long running. You give them one or multiple long term goals, and they independently execute towards those goals. The applications combine tool usage and long term memory.","language":"en","loc":{"lines":{"from":55,"to":92}}}}],["474",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] *\nChatbots CHATBOTS Language models are good at producing text, which makes them\nideal for creating chatbots. Aside from the base prompts/LLMs, an important\nconcept to know for Chatbots is memory. Most chat based","metadata":{"source":"https://js.langchain.com/docs/use_cases/chatbots","title":"Chatbots | 🦜️🔗 Langchain","description":"Language models are good at producing text, which makes them ideal for creating chatbots.","language":"en","loc":{"lines":{"from":1,"to":28}}}}],["475",{"pageContent":"models are good at producing text, which makes them ideal for creating chatbots.\nAside from the base prompts/LLMs, an important concept to know for Chatbots is\nmemory. Most chat based applications rely on remembering what happened in\nprevious interactions, which memory is designed to help with. You might find the\nfollowing pages interesting: * Memory concepts and examples\n[/docs/modules/memory/]: Explanation of key concepts related to memory along\nwith how-to's and examples. * Conversation Agent\n[/docs/modules/agents/agent_types/chat_conversation_agent]: A notebook walking\nthrough how to create an agent optimized for conversation. Previous BabyAGI\n[/docs/use_cases/autonomous_agents/baby_agi] Next Extraction\n[/docs/use_cases/extraction] Community * Discord [https://discord.gg/cU2adEyC7w]\n* Twitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage","metadata":{"source":"https://js.langchain.com/docs/use_cases/chatbots","title":"Chatbots | 🦜️🔗 Langchain","description":"Language models are good at producing text, which makes them ideal for creating chatbots.","language":"en","loc":{"lines":{"from":28,"to":54}}}}],["476",{"pageContent":"* Twitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/chatbots","title":"Chatbots | 🦜️🔗 Langchain","description":"Language models are good at producing text, which makes them ideal for creating chatbots.","language":"en","loc":{"lines":{"from":54,"to":64}}}}],["477",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] *\nExtraction EXTRACTION Most APIs and databases still deal with structured\ninformation. Therefore, in order to better work with those, it can be useful to\nextract structured information from text. Examples of this","metadata":{"source":"https://js.langchain.com/docs/use_cases/extraction","title":"Extraction | 🦜️🔗 Langchain","description":"Most APIs and databases still deal with structured information. Therefore, in order to better work with those, it can be useful to extract structured information from text. Examples of this include:","language":"en","loc":{"lines":{"from":1,"to":28}}}}],["478",{"pageContent":"APIs and databases still deal with structured information. Therefore, in order\nto better work with those, it can be useful to extract structured information\nfrom text. Examples of this include: * Extracting a structured row to insert\ninto a database from a sentence * Extracting multiple rows to insert into a\ndatabase from a long document * Extracting the correct API parameters from a\nuser query This work is extremely related to output parsing. Output parsers are\nresponsible for instructing the LLM to respond in a specific format. In this\ncase, the output parsers specify the format of the data you would like to\nextract from the document. Then, in addition to the output format instructions,\nthe prompt should also contain the data you would like to extract information\nfrom. You can also try out the extraction chain\n[/docs/modules/chains/popular/structured_output], an LLMChain specialized to use\nOpenAI functions to generate output matching an input schema. While normal\noutput","metadata":{"source":"https://js.langchain.com/docs/use_cases/extraction","title":"Extraction | 🦜️🔗 Langchain","description":"Most APIs and databases still deal with structured information. Therefore, in order to better work with those, it can be useful to extract structured information from text. Examples of this include:","language":"en","loc":{"lines":{"from":28,"to":42}}}}],["479",{"pageContent":"also try out the extraction chain\n[/docs/modules/chains/popular/structured_output], an LLMChain specialized to use\nOpenAI functions to generate output matching an input schema. While normal\noutput parsers are good enough for basic structuring of response data, when\ndoing extraction you often want to extract more complicated or nested\nstructures. Previous Chatbots [/docs/use_cases/chatbots] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/extraction","title":"Extraction | 🦜️🔗 Langchain","description":"Most APIs and databases still deal with structured information. Therefore, in order to better work with those, it can be useful to extract structured information from text. Examples of this include:","language":"en","loc":{"lines":{"from":42,"to":63}}}}],["480",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Agent types","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent","title":"Conversational | 🦜️🔗 Langchain","description":"This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["481",{"pageContent":"* Retrieval [/docs/modules/data_connection/] * Chains [/docs/modules/chains/] *\nMemory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Agent types\n[/docs/modules/agents/agent_types/] * Conversational\n[/docs/modules/agents/agent_types/chat_conversation_agent] * OpenAI functions\n[/docs/modules/agents/agent_types/openai_functions_agent] * Plan and execute\n[/docs/modules/agents/agent_types/plan_and_execute] * ReAct\n[/docs/modules/agents/agent_types/react] * Structured tool chat\n[/docs/modules/agents/agent_types/structured_chat] * XML Agent\n[/docs/modules/agents/agent_types/xml] * How-to\n[/docs/modules/agents/how_to/callbacks] * Tools [/docs/modules/agents/tools/] *\nToolkits [/docs/modules/agents/toolkits/] * Callbacks [/docs/modules/callbacks/]\n* Modules [/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem]\n* Additional resources [/docs/additional_resources] * Community navigator","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent","title":"Conversational | 🦜️🔗 Langchain","description":"This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.","language":"en","loc":{"lines":{"from":25,"to":44}}}}],["482",{"pageContent":"Callbacks [/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides\n[/docs/guides] * Ecosystem [/docs/ecosystem] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Agents\n[/docs/modules/agents/] * Agent types [/docs/modules/agents/agent_types/] *\nConversational CONVERSATIONAL This walkthrough demonstrates how to use an agent\noptimized for conversation. Other agents are often optimized for using tools to\nfigure out the best response, which is not ideal in a conversational setting\nwhere you may want the agent to be able to chat with the user as well. This\nexample covers how to create a conversational agent for a chat model. It will\nutilize chat specific prompts. import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; import {","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent","title":"Conversational | 🦜️🔗 Langchain","description":"This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.","language":"en","loc":{"lines":{"from":44,"to":71}}}}],["483",{"pageContent":"user as well. This example covers how to create a conversational agent for a\nchat model. It will utilize chat specific prompts. import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; import { initializeAgentExecutorWithOptions }\nfrom \"langchain/agents\"; import { SerpAPI } from \"langchain/tools\"; import {\nCalculator } from \"langchain/tools/calculator\"; export const run = async () => {\nprocess.env.LANGCHAIN_HANDLER = \"langchain\"; const model = new ChatOpenAI({\ntemperature: 0 }); const tools = [ new SerpAPI(process.env.SERPAPI_API_KEY, {\nlocation: \"Austin,Texas,United States\", hl: \"en\", gl: \"us\", }), new\nCalculator(), ]; // Passing \"chat-conversational-react-description\" as the agent\ntype // automatically creates and uses BufferMemory with the executor. // If you\nwould like to override this, you can pass in a custom // memory option, but the\nmemoryKey set on it must be \"chat_history\". const executor = await","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent","title":"Conversational | 🦜️🔗 Langchain","description":"This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.","language":"en","loc":{"lines":{"from":71,"to":96}}}}],["484",{"pageContent":"uses BufferMemory with the executor. // If you would like to override this, you\ncan pass in a custom // memory option, but the memoryKey set on it must be\n\"chat_history\". const executor = await initializeAgentExecutorWithOptions(tools,\nmodel, { agentType: \"chat-conversational-react-description\", verbose: true, });\nconsole.log(\"Loaded agent.\"); const input0 = \"hi, i am bob\"; const result0 =\nawait executor.call({ input: input0 }); console.log(`Got output\n${result0.output}`); const input1 = \"whats my name?\"; const result1 = await\nexecutor.call({ input: input1 }); console.log(`Got output ${result1.output}`);\nconst input2 = \"whats the weather in pomfret?\"; const result2 = await\nexecutor.call({ input: input2 }); console.log(`Got output ${result2.output}`);\n}; API REFERENCE: * ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI]\nfrom langchain/chat_models/openai * initializeAgentExecutorWithOptions","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent","title":"Conversational | 🦜️🔗 Langchain","description":"This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.","language":"en","loc":{"lines":{"from":96,"to":130}}}}],["485",{"pageContent":"output ${result2.output}`); }; API REFERENCE: * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * initializeAgentExecutorWithOptions\n[/docs/api/agents/functions/initializeAgentExecutorWithOptions] from\nlangchain/agents * SerpAPI [/docs/api/tools/classes/SerpAPI] from\nlangchain/tools * Calculator [/docs/api/tools_calculator/classes/Calculator]\nfrom langchain/tools/calculator Loaded agent. Entering new agent_executor\nchain... { \"action\": \"Final Answer\", \"action_input\": \"Hello Bob! How can I\nassist you today?\" } Finished chain. Got output Hello Bob! How can I assist you\ntoday? Entering new agent_executor chain... { \"action\": \"Final Answer\",\n\"action_input\": \"Your name is Bob.\" } Finished chain. Got output Your name is\nBob. Entering new agent_executor chain... ```json { \"action\": \"search\",\n\"action_input\": \"weather in pomfret\" } ``` A steady rain early...then remaining\ncloudy with a few showers. High 48F. Winds","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent","title":"Conversational | 🦜️🔗 Langchain","description":"This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.","language":"en","loc":{"lines":{"from":130,"to":165}}}}],["486",{"pageContent":"new agent_executor chain... ```json { \"action\": \"search\", \"action_input\":\n\"weather in pomfret\" } ``` A steady rain early...then remaining cloudy with a\nfew showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%. ```json {\n\"action\": \"Final Answer\", \"action_input\": \"The weather in Pomfret is a steady\nrain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10\nto 15 mph. Chance of rain 80%.\" } ``` Finished chain. Got output The weather in\nPomfret is a steady rain early...then remaining cloudy with a few showers. High\n48F. Winds WNW at 10 to 15 mph. Chance of rain 80%. Previous Agent types\n[/docs/modules/agents/agent_types/] Next OpenAI functions\n[/docs/modules/agents/agent_types/openai_functions_agent] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent","title":"Conversational | 🦜️🔗 Langchain","description":"This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.","language":"en","loc":{"lines":{"from":165,"to":199}}}}],["487",{"pageContent":"* Twitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent","title":"Conversational | 🦜️🔗 Langchain","description":"This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.","language":"en","loc":{"lines":{"from":199,"to":209}}}}],["488",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * SalesGPT\n[/docs/use_cases/autonomous_agents/sales_gpt] * AutoGPT\n[/docs/use_cases/autonomous_agents/auto_gpt] * BabyAGI\n[/docs/use_cases/autonomous_agents/baby_agi] * Chatbots\n[/docs/use_cases/chatbots] * Extraction [/docs/use_cases/extraction] * / * Use\ncases [/docs/use_cases] * Autonomous Agents","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["489",{"pageContent":"* BabyAGI [/docs/use_cases/autonomous_agents/baby_agi] * Chatbots\n[/docs/use_cases/chatbots] * Extraction [/docs/use_cases/extraction] * / * Use\ncases [/docs/use_cases] * Autonomous Agents [/docs/use_cases/autonomous_agents/]\n* BabyAGI On this page BABYAGI info Original Repo:\nhttps://github.com/yoheinakajima/babyagi\n[https://github.com/yoheinakajima/babyagi] BabyAGI is made up of 3 components: *\nA chain responsible for creating tasks * A chain responsible for prioritising\ntasks * A chain responsible for executing tasks These chains are executed in\nsequence until the task list is empty or the maximum number of iterations is\nreached. SIMPLE EXAMPLE In this example we use BabyAGI directly without any\ntools. You'll see this results in successfully creating a list of tasks but when\nit comes to executing the tasks we do not get concrete results. This is because\nwe have not provided any tools to the BabyAGI. We'll see how to do that in the\nnext example. import {","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":25,"to":58}}}}],["490",{"pageContent":"tasks but when it comes to executing the tasks we do not get concrete results.\nThis is because we have not provided any tools to the BabyAGI. We'll see how to\ndo that in the next example. import { BabyAGI } from\n\"langchain/experimental/babyagi\"; import { MemoryVectorStore } from\n\"langchain/vectorstores/memory\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { OpenAI } from \"langchain/llms/openai\";\nconst vectorStore = new MemoryVectorStore(new OpenAIEmbeddings()); const babyAGI\n= BabyAGI.fromLLM({ llm: new OpenAI({ temperature: 0 }), vectorstore:\nvectorStore, maxIterations: 3, }); await babyAGI.call({ objective: \"Write a\nweather report for SF today\" }); /* *****TASK LIST***** 1: Make a todo list\n*****NEXT TASK***** 1: Make a todo list *****TASK RESULT***** 1. Check the\nweather forecast for San Francisco today 2. Make note of the temperature,\nhumidity, wind speed, and other relevant weather conditions 3. Write a weather\nreport summarizing the","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":58,"to":90}}}}],["491",{"pageContent":"Check the weather forecast for San Francisco today 2. Make note of the\ntemperature, humidity, wind speed, and other relevant weather conditions 3.\nWrite a weather report summarizing the forecast 4. Check for any weather alerts\nor warnings 5. Share the report with the relevant stakeholders *****TASK\nLIST***** 2: Check the current temperature in San Francisco 3: Check the current\nhumidity in San Francisco 4: Check the current wind speed in San Francisco 5:\nCheck for any weather alerts or warnings in San Francisco 6: Check the forecast\nfor the next 24 hours in San Francisco 7: Check the forecast for the next 48\nhours in San Francisco 8: Check the forecast for the next 72 hours in San\nFrancisco 9: Check the forecast for the next week in San Francisco 10: Check the\nforecast for the next month in San Francisco 11: Check the forecast for the next\n3 months in San Francisco 1: Write a weather report for SF today *****NEXT\nTASK***** 2: Check the current temperature in San","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":90,"to":112}}}}],["492",{"pageContent":"for the next month in San Francisco 11: Check the forecast for the next 3 months\nin San Francisco 1: Write a weather report for SF today *****NEXT TASK***** 2:\nCheck the current temperature in San Francisco *****TASK RESULT***** I will\ncheck the current temperature in San Francisco. I will use an online weather\nservice to get the most up-to-date information. *****TASK LIST***** 3: Check the\ncurrent UV index in San Francisco 4: Check the current air quality in San\nFrancisco 5: Check the current precipitation levels in San Francisco 6: Check\nthe current cloud cover in San Francisco 7: Check the current barometric\npressure in San Francisco 8: Check the current dew point in San Francisco 9:\nCheck the current wind direction in San Francisco 10: Check the current humidity\nlevels in San Francisco 1: Check the current temperature in San Francisco to the\naverage temperature for this time of year 2: Check the current visibility in San\nFrancisco 11: Write a weather report for SF","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":112,"to":136}}}}],["493",{"pageContent":"in San Francisco 1: Check the current temperature in San Francisco to the\naverage temperature for this time of year 2: Check the current visibility in San\nFrancisco 11: Write a weather report for SF today *****NEXT TASK***** 3: Check\nthe current UV index in San Francisco *****TASK RESULT***** The current UV index\nin San Francisco is moderate, with a value of 5. This means that it is safe to\nbe outside for short periods of time without sunscreen, but it is still\nrecommended to wear sunscreen and protective clothing when outside for extended\nperiods of time. */ API REFERENCE: * BabyAGI\n[/docs/api/experimental_babyagi/classes/BabyAGI] from\nlangchain/experimental/babyagi * MemoryVectorStore\n[/docs/api/vectorstores_memory/classes/MemoryVectorStore] from\nlangchain/vectorstores/memory * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai EXAMPLE WITH","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":136,"to":161}}}}],["494",{"pageContent":"* OpenAIEmbeddings [/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai EXAMPLE WITH TOOLS In this next example we replace the\nexecution chain with a custom agent with a Search tool. This gives BabyAGI the\nability to use real-world data when executing tasks, which makes it much more\npowerful. You can add additional tools to give it more capabilities. import {\nBabyAGI } from \"langchain/experimental/babyagi\"; import { MemoryVectorStore }\nfrom \"langchain/vectorstores/memory\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\"; import { LLMChain } from\n\"langchain/chains\"; import { ChainTool, SerpAPI, Tool } from \"langchain/tools\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\"; // First,\nwe create a custom agent which will serve as execution","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":161,"to":180}}}}],["495",{"pageContent":"{ ChainTool, SerpAPI, Tool } from \"langchain/tools\"; import {\ninitializeAgentExecutorWithOptions } from \"langchain/agents\"; // First, we\ncreate a custom agent which will serve as execution chain. const todoPrompt =\nPromptTemplate.fromTemplate( \"You are a planner who is an expert at coming up\nwith a todo list for a given objective. Come up with a todo list for this\nobjective: {objective}\" ); const tools: Tool[] = [ new\nSerpAPI(process.env.SERPAPI_API_KEY, { location: \"San\nFrancisco,California,United States\", hl: \"en\", gl: \"us\", }), new ChainTool({\nname: \"TODO\", chain: new LLMChain({ llm: new OpenAI({ temperature: 0 }), prompt:\ntodoPrompt, }), description: \"useful for when you need to come up with todo\nlists. Input: an objective to create a todo list for. Output: a todo list for\nthat objective. Please be very clear what the objective is!\", }), ]; const\nagentExecutor = await initializeAgentExecutorWithOptions( tools, new","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":180,"to":205}}}}],["496",{"pageContent":"create a todo list for. Output: a todo list for that objective. Please be very\nclear what the objective is!\", }), ]; const agentExecutor = await\ninitializeAgentExecutorWithOptions( tools, new OpenAI({ temperature: 0 }), {\nagentType: \"zero-shot-react-description\", agentArgs: { prefix: `You are an AI\nwho performs one task based on the following objective: {objective}. Take into\naccount these previously completed tasks: {context}.`, suffix: `Question: {task}\n{agent_scratchpad}`, inputVariables: [\"objective\", \"task\", \"context\",\n\"agent_scratchpad\"], }, } ); const vectorStore = new MemoryVectorStore(new\nOpenAIEmbeddings()); // Then, we create a BabyAGI instance. const babyAGI =\nBabyAGI.fromLLM({ llm: new OpenAI({ temperature: 0 }), executionChain:\nagentExecutor, // an agent executor is a chain vectorstore: vectorStore,\nmaxIterations: 10, }); await babyAGI.call({ objective: \"Write a short weather\nreport for SF today\" }); /* *****TASK","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":205,"to":235}}}}],["497",{"pageContent":"agentExecutor, // an agent executor is a chain vectorstore: vectorStore,\nmaxIterations: 10, }); await babyAGI.call({ objective: \"Write a short weather\nreport for SF today\" }); /* *****TASK LIST***** 1: Make a todo list *****NEXT\nTASK***** 1: Make a todo list *****TASK RESULT***** Today in San Francisco, the\nweather is sunny with a temperature of 70 degrees Fahrenheit, light winds, and\nlow humidity. The forecast for the next few days is expected to be similar.\n*****TASK LIST***** 2: Find the forecasted temperature for the next few days in\nSan Francisco 3: Find the forecasted wind speed for the next few days in San\nFrancisco 4: Find the forecasted humidity for the next few days in San Francisco\n5: Create a graph showing the forecasted temperature, wind speed, and humidity\nfor San Francisco over the next few days 6: Research the average temperature for\nSan Francisco in the past week 7: Research the average wind speed for San\nFrancisco in the past week 8: Research the average","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":235,"to":263}}}}],["498",{"pageContent":"Francisco over the next few days 6: Research the average temperature for San\nFrancisco in the past week 7: Research the average wind speed for San Francisco\nin the past week 8: Research the average humidity for San Francisco in the past\nweek 9: Create a graph showing the temperature, wind speed, and humidity for San\nFrancisco over the past week *****NEXT TASK***** 2: Find the forecasted\ntemperature for the next few days in San Francisco *****TASK RESULT***** The\nforecasted temperature for the next few days in San Francisco is 63°, 65°, 71°,\n73°, and 66°. *****TASK LIST***** 3: Find the forecasted wind speed for the next\nfew days in San Francisco 4: Find the forecasted humidity for the next few days\nin San Francisco 5: Create a graph showing the forecasted temperature, wind\nspeed, and humidity for San Francisco over the next few days 6: Research the\naverage temperature for San Francisco in the past week 7: Research the average\nwind speed for San Francisco in the past week 8:","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":263,"to":284}}}}],["499",{"pageContent":"and humidity for San Francisco over the next few days 6: Research the average\ntemperature for San Francisco in the past week 7: Research the average wind\nspeed for San Francisco in the past week 8: Research the average humidity for\nSan Francisco in the past week 9: Create a graph showing the temperature, wind\nspeed, and humidity for San Francisco over the past week 10: Compare the\nforecasted temperature, wind speed, and humidity for San Francisco over the next\nfew days to the average temperature, wind speed, and humidity for San Francisco\nover the past week 11: Find the forecasted precipitation for the next few days\nin San Francisco 12: Research the average wind direction for San Francisco in\nthe past week 13: Create a graph showing the forecasted temperature, wind speed,\nand humidity for San Francisco over the past week 14: Compare the forecasted\ntemperature, wind speed, and humidity for San Francisco over the next few days\nto *****NEXT TASK***** 3: Find the forecasted wind speed","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":284,"to":297}}}}],["500",{"pageContent":"San Francisco over the past week 14: Compare the forecasted temperature, wind\nspeed, and humidity for San Francisco over the next few days to *****NEXT\nTASK***** 3: Find the forecasted wind speed for the next few days in San\nFrancisco *****TASK RESULT***** West winds 10 to 20 mph. Gusts up to 35 mph in\nthe evening. Tuesday. Sunny. Highs in the 60s to upper 70s. West winds 5 to 15\nmph. *****TASK LIST***** 4: Research the average precipitation for San Francisco\nin the past week 5: Research the average temperature for San Francisco in the\npast week 6: Research the average wind speed for San Francisco in the past week\n7: Research the average humidity for San Francisco in the past week 8: Research\nthe average wind direction for San Francisco in the past week 9: Find the\nforecasted temperature, wind speed, and humidity for San Francisco over the next\nfew days 10: Find the forecasted precipitation for the next few days in San\nFrancisco 11: Create a graph showing the forecasted","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":297,"to":317}}}}],["501",{"pageContent":"temperature, wind speed, and humidity for San Francisco over the next few days\n10: Find the forecasted precipitation for the next few days in San Francisco 11:\nCreate a graph showing the forecasted temperature, wind speed, and humidity for\nSan Francisco over the next few days 12: Create a graph showing the temperature,\nwind speed, and humidity for San Francisco over the past week 13: Create a graph\nshowing the forecasted temperature, wind speed, and humidity for San Francisco\nover the past month 14: Compare the forecasted temperature, wind speed, and\nhumidity for San Francisco over the next few days to the average temperature,\nwind speed, and humidity for San Francisco over the past week 15: Compare the\nforecasted temperature, wind speed, and humidity for San Francisco over the next\nfew days to the *****NEXT TASK***** 4: Research the average precipitation for\nSan Francisco in the past week *****TASK RESULT***** According to Weather\nUnderground, the forecasted precipitation for San","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":317,"to":331}}}}],["502",{"pageContent":"to the *****NEXT TASK***** 4: Research the average precipitation for San\nFrancisco in the past week *****TASK RESULT***** According to Weather\nUnderground, the forecasted precipitation for San Francisco in the next few days\nis 7-hour rain and snow with 24-hour rain accumulation. *****TASK LIST***** 5:\nResearch the average wind speed for San Francisco over the past month 6: Create\na graph showing the forecasted temperature, wind speed, and humidity for San\nFrancisco over the past month 7: Compare the forecasted temperature, wind speed,\nand humidity for San Francisco over the next few days to the average\ntemperature, wind speed, and humidity for San Francisco over the past month 8:\nResearch the average temperature for San Francisco over the past month 9:\nResearch the average wind direction for San Francisco over the past month 10:\nCreate a graph showing the forecasted precipitation for San Francisco over the\nnext few days 11: Compare the forecasted precipitation for San Francisco","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":331,"to":349}}}}],["503",{"pageContent":"for San Francisco over the past month 10: Create a graph showing the forecasted\nprecipitation for San Francisco over the next few days 11: Compare the\nforecasted precipitation for San Francisco over the next few days to the average\nprecipitation for San Francisco over the past week 12: Find the forecasted\ntemperature, wind speed, and humidity for San Francisco over the next few days\n13: Find the forecasted precipitation for the next few days in San Francisco 14:\nCreate a graph showing the temperature, wind speed, and humidity for San\nFrancisco over the past week 15: Create a graph showing the forecasted\ntemperature, wind speed, and humidity for San Francisco over the next few days\n16: Compare the forecast *****NEXT TASK***** 5: Research the average wind speed\nfor San Francisco over the past month *****TASK RESULT***** The average wind\nspeed for San Francisco over the past month is 3.2 meters per second. *****TASK\nLIST***** 6: Find the forecasted temperature, wind speed, and","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":349,"to":368}}}}],["504",{"pageContent":"the past month *****TASK RESULT***** The average wind speed for San Francisco\nover the past month is 3.2 meters per second. *****TASK LIST***** 6: Find the\nforecasted temperature, wind speed, and humidity for San Francisco over the next\nfew days, 7: Find the forecasted precipitation for the next few days in San\nFrancisco, 8: Create a graph showing the temperature, wind speed, and humidity\nfor San Francisco over the past week, 9: Create a graph showing the forecasted\ntemperature, wind speed, and humidity for San Francisco over the next few days,\n10: Compare the forecasted temperature, wind speed, and humidity for San\nFrancisco over the next few days to the average wind speed for San Francisco\nover the past month, 11: Research the average wind speed for San Francisco over\nthe past week, 12: Create a graph showing the forecasted precipitation for San\nFrancisco over the next few days, 13: Compare the forecasted precipitation for\nSan Francisco over the next few days to the average","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":368,"to":383}}}}],["505",{"pageContent":"Create a graph showing the forecasted precipitation for San Francisco over the\nnext few days, 13: Compare the forecasted precipitation for San Francisco over\nthe next few days to the average precipitation for San Francisco over the past\nmonth, 14: Research the average temperature for San Francisco over the past\nmonth, 15: Research the average humidity for San Francisco over the past month,\n16: Compare the forecasted temperature, wind speed, and humidity for San\nFrancisco over the next few days to the average temperature, *****NEXT TASK*****\n6: Find the forecasted temperature, wind speed, and humidity for San Francisco\nover the next few days, *****TASK RESULT***** The forecast for San Francisco\nover the next few days is mostly sunny, with a high near 64. West wind 7 to 12\nmph increasing to 13 to 18 mph in the afternoon. Winds could gust as high as 22\nmph. Humidity will be around 50%. *****TASK LIST***** 7: Find the forecasted\nprecipitation for the next few days in San","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":383,"to":399}}}}],["506",{"pageContent":"increasing to 13 to 18 mph in the afternoon. Winds could gust as high as 22 mph.\nHumidity will be around 50%. *****TASK LIST***** 7: Find the forecasted\nprecipitation for the next few days in San Francisco, 8: Create a graph showing\nthe temperature, wind speed, and humidity for San Francisco over the past week,\n9: Create a graph showing the forecasted temperature, wind speed, and humidity\nfor San Francisco over the next few days, 10: Compare the forecasted\ntemperature, wind speed, and humidity for San Francisco over the next few days\nto the average wind speed for San Francisco over the past month, 11: Research\nthe average wind speed for San Francisco over the past week, 12: Create a graph\nshowing the forecasted precipitation for San Francisco over the next few days,\n13: Compare the forecasted precipitation for San Francisco over the next few\ndays to the average precipitation for San Francisco over the past month, 14:\nResearch the average temperature for San Francisco over the past","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":399,"to":410}}}}],["507",{"pageContent":"precipitation for San Francisco over the next few days to the average\nprecipitation for San Francisco over the past month, 14: Research the average\ntemperature for San Francisco over the past month, 15: Research the average\nhumidity for San Francisco over the past month, 16: Compare the forecasted\ntemperature, wind speed, and humidity for San Francisco over the next few days\nto the average temperature *****NEXT TASK***** 7: Find the forecasted\nprecipitation for the next few days in San Francisco, *****TASK RESULT*****\nAccording to Weather Underground, the forecasted precipitation for the next few\ndays in San Francisco is 7-hour rain and snow with 24-hour rain accumulation,\nradar and satellite maps of precipitation. *****TASK LIST***** 8: Create a graph\nshowing the temperature, wind speed, and humidity for San Francisco over the\npast week, 9: Create a graph showing the forecasted temperature, wind speed, and\nhumidity for San Francisco over the next few days, 10: Compare the","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":410,"to":427}}}}],["508",{"pageContent":"wind speed, and humidity for San Francisco over the past week, 9: Create a graph\nshowing the forecasted temperature, wind speed, and humidity for San Francisco\nover the next few days, 10: Compare the forecasted temperature, wind speed, and\nhumidity for San Francisco over the next few days to the average wind speed for\nSan Francisco over the past month, 11: Research the average wind speed for San\nFrancisco over the past week, 12: Create a graph showing the forecasted\nprecipitation for San Francisco over the next few days, 13: Compare the\nforecasted precipitation for San Francisco over the next few days to the average\nprecipitation for San Francisco over the past month, 14: Research the average\ntemperature for San Francisco over the past month, 15: Research the average\nhumidity for San Francisco over the past month, 16: Compare the forecasted\ntemperature, wind speed, and humidity for San Francisco over the next few days\nto the average temperature *****NEXT TASK***** 8: Create a graph","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":427,"to":439}}}}],["509",{"pageContent":"over the past month, 16: Compare the forecasted temperature, wind speed, and\nhumidity for San Francisco over the next few days to the average temperature\n*****NEXT TASK***** 8: Create a graph showing the temperature, wind speed, and\nhumidity for San Francisco over the past week, *****TASK RESULT***** A graph\nshowing the temperature, wind speed, and humidity for San Francisco over the\npast week. *****TASK LIST***** 9: Create a graph showing the forecasted\ntemperature, wind speed, and humidity for San Francisco over the next few days\n10: Compare the forecasted temperature, wind speed, and humidity for San\nFrancisco over the next few days to the average wind speed for San Francisco\nover the past month 11: Research the average wind speed for San Francisco over\nthe past week 12: Create a graph showing the forecasted precipitation for San\nFrancisco over the next few days 13: Compare the forecasted precipitation for\nSan Francisco over the next few days to the average precipitation for","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":439,"to":456}}}}],["510",{"pageContent":"graph showing the forecasted precipitation for San Francisco over the next few\ndays 13: Compare the forecasted precipitation for San Francisco over the next\nfew days to the average precipitation for San Francisco over the past month 14:\nResearch the average temperature for San Francisco over the past month 15:\nResearch the average humidity for San Francisco over the past month 16: Compare\nthe forecasted temperature, wind speed, and humidity for San Francisco over the\nnext few days to the average temperature *****NEXT TASK***** 9: Create a graph\nshowing the forecasted temperature, wind speed, and humidity for San Francisco\nover the next few days *****TASK RESULT***** The forecasted temperature, wind\nspeed, and humidity for San Francisco over the next few days can be seen in the\ngraph created. *****TASK LIST***** 10: Research the average wind speed for San\nFrancisco over the past month 11: Compare the forecasted temperature, wind\nspeed, and humidity for San Francisco over the next","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":456,"to":473}}}}],["511",{"pageContent":"LIST***** 10: Research the average wind speed for San Francisco over the past\nmonth 11: Compare the forecasted temperature, wind speed, and humidity for San\nFrancisco over the next few days to the average humidity for San Francisco over\nthe past month 12: Create a graph showing the forecasted precipitation for San\nFrancisco over the next few days 13: Compare the forecasted precipitation for\nSan Francisco over the next few days to the average precipitation for San\nFrancisco over the past month 14: Research the average temperature for San\nFrancisco over the past week 15: Compare the forecasted temperature, wind speed,\nand humidity for San Francisco over the next few days to the average wind speed\nfor San Francisco over the past week *****NEXT TASK***** 10: Research the\naverage wind speed for San Francisco over the past month *****TASK RESULT*****\nThe average wind speed for San Francisco over the past month is 2.7 meters per\nsecond. [...] */ API REFERENCE: * BabyAGI","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":473,"to":498}}}}],["512",{"pageContent":"speed for San Francisco over the past month *****TASK RESULT***** The average\nwind speed for San Francisco over the past month is 2.7 meters per second. [...]\n*/ API REFERENCE: * BabyAGI [/docs/api/experimental_babyagi/classes/BabyAGI]\nfrom langchain/experimental/babyagi * MemoryVectorStore\n[/docs/api/vectorstores_memory/classes/MemoryVectorStore] from\nlangchain/vectorstores/memory * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts * LLMChain\n[/docs/api/chains/classes/LLMChain] from langchain/chains * ChainTool\n[/docs/api/tools/classes/ChainTool] from langchain/tools * SerpAPI\n[/docs/api/tools/classes/SerpAPI] from langchain/tools * Tool\n[/docs/api/tools/classes/Tool] from langchain/tools *\ninitializeAgentExecutorWithOptions","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":498,"to":521}}}}],["513",{"pageContent":"from langchain/tools * SerpAPI [/docs/api/tools/classes/SerpAPI] from\nlangchain/tools * Tool [/docs/api/tools/classes/Tool] from langchain/tools *\ninitializeAgentExecutorWithOptions\n[/docs/api/agents/functions/initializeAgentExecutorWithOptions] from\nlangchain/agents Previous AutoGPT [/docs/use_cases/autonomous_agents/auto_gpt]\nNext Chatbots [/docs/use_cases/chatbots] * Simple Example * Example with Tools\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/autonomous_agents/baby_agi","title":"BabyAGI | 🦜️🔗 Langchain","description":"Original Repo//github.com/yoheinakajima/babyagi","language":"en","loc":{"lines":{"from":521,"to":547}}}}],["514",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Conversational Retrieval Agents\n[/docs/use_cases/question_answering/conversational_retrieval_agents] * Use local\nLLMs [/docs/use_cases/question_answering/local_retrieval_qa] * Tabular Question\nAnswering [/docs/use_cases/tabular] * Interacting with APIs\n[/docs/use_cases/api] * Summarization [/docs/use_cases/summarization] * Agent\nSimulations [/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] * QA\nand Chat over Documents QA AND","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":1,"to":27}}}}],["515",{"pageContent":"[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] * QA\nand Chat over Documents QA AND CHAT OVER DOCUMENTS Chat and Question-Answering\n(QA) over data are popular LLM use-cases. data can include many things,\nincluding: * Unstructured data (e.g., PDFs) * Structured data (e.g., SQL) * Code\n(e.g., Python) Below we will review Chat and QA on Unstructured data. intro.png\n[/assets/images/qa_intro-9b468dbffe1cbe7f0bd822b28648db9e.png] Unstructured data\ncan be loaded from many sources. Check out the document loader integrations here\n[/docs/modules/data_connection/document_loaders/] to browse the set of supported\nloaders. Each loader returns data as a LangChain Document. Documents are turned\ninto a Chat or QA app following the general steps below: * Splitting: Text\nsplitters [/docs/modules/data_connection/document_transformers/] break Documents\ninto splits of specified","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":27,"to":59}}}}],["516",{"pageContent":"are turned into a Chat or QA app following the general steps below: * Splitting:\nText splitters [/docs/modules/data_connection/document_transformers/] break\nDocuments into splits of specified size * Storage: Storage (e.g., often a\nvectorstore [/docs/modules/data_connection/vectorstores/]) will house and often\nembed [https://www.pinecone.io/learn/vector-embeddings/] the splits * Retrieval:\nThe app retrieves splits from storage (e.g., often with similar embeddings\n[https://www.pinecone.io/learn/k-nearest-neighbor/] to the input question) *\nOutput: An LLM [/docs/modules/model_io/models/llms/] produces an answer using a\nprompt that includes the question and the retrieved splits flow.jpeg\n[/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg] QUICKSTART Let's\nload this blog post [https://lilianweng.github.io/posts/2023-06-23-agent/] on\nagents as an example Document. We'll have a QA app in a few lines of code.\nFirst, set environment variables and install packages","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":59,"to":78}}}}],["517",{"pageContent":"blog post [https://lilianweng.github.io/posts/2023-06-23-agent/] on agents as an\nexample Document. We'll have a QA app in a few lines of code. First, set\nenvironment variables and install packages required for the guide: > yarn add\ncheerio # Or load env vars in your preferred way: > export OPENAI_API_KEY=\"...\"\n1. LOADING, SPLITTING, STORAGE 1.1 GETTING STARTED Specify a Document loader. //\nDocument loader import { CheerioWebBaseLoader } from\n\"langchain/document_loaders/web/cheerio\"; const loader = new\nCheerioWebBaseLoader( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" );\nconst data = await loader.load(); Split the Document into chunks for embedding\nand vector storage. import { RecursiveCharacterTextSplitter } from\n\"langchain/text_splitter\"; const textSplitter = new\nRecursiveCharacterTextSplitter({ chunkSize: 500, chunkOverlap: 0, }); const\nsplitDocs = await textSplitter.splitDocuments(data); Embed and store the splits\nin a vector database (for","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":78,"to":124}}}}],["518",{"pageContent":"= new RecursiveCharacterTextSplitter({ chunkSize: 500, chunkOverlap: 0, });\nconst splitDocs = await textSplitter.splitDocuments(data); Embed and store the\nsplits in a vector database (for demo purposes we use an unoptimized, in-memory\nexample but you can browse integrations here\n[/docs/modules/data_connection/vectorstores/integrations/]): import {\nOpenAIEmbeddings } from \"langchain/embeddings/openai\"; import {\nMemoryVectorStore } from \"langchain/vectorstores/memory\"; const embeddings = new\nOpenAIEmbeddings(); const vectorStore = await\nMemoryVectorStore.fromDocuments(splitDocs, embeddings); Here are the three\npieces together: lc.png\n[/assets/images/qa_data_load-70fac3ea6593b986613784dc056df21a.png] 1.2 GOING\nDEEPER 1.2.1 INTEGRATIONS Document Loaders * Browse document loader integrations\nhere [/docs/modules/data_connection/document_loaders/]. * See further\ndocumentation on loaders here [/docs/modules/data_connection/document_loaders/].\nDocument Transformers","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":124,"to":162}}}}],["519",{"pageContent":"loader integrations here [/docs/modules/data_connection/document_loaders/]. *\nSee further documentation on loaders here\n[/docs/modules/data_connection/document_loaders/]. Document Transformers * All\ncan ingest loaded Documents and process them (e.g., split). * See further\ndocumentation on transformers here\n[/docs/modules/data_connection/document_transformers/]. Vectorstores * Browse\nvectorstore integrations here\n[/docs/modules/data_connection/vectorstores/integrations/]. * See further\ndocumentation on vectorstores here\n[/docs/modules/data_connection/vectorstores/]. 2. RETRIEVAL 2.1 GETTING STARTED\nRetrieve relevant splits\n[https://www.pinecone.io/learn/what-is-similarity-search/] for any question\nusing similarity_search. const relevantDocs = await\nvectorStore.similaritySearch(\"What is task decomposition?\");\nconsole.log(relevantDocs.length); // 4 2.2 GOING DEEPER 2.2.1 RETRIEVAL\nVectorstores are commonly used for retrieval. But, they are not the only","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":162,"to":202}}}}],["520",{"pageContent":"is task decomposition?\"); console.log(relevantDocs.length); // 4 2.2 GOING\nDEEPER 2.2.1 RETRIEVAL Vectorstores are commonly used for retrieval. But, they\nare not the only option. For example, SVMs (see thread here\n[https://twitter.com/karpathy/status/1647025230546886658?s=20]) can also be\nused. LangChain has many retrievers and retrieval methods\n[/docs/modules/data_connection/retrievers/] including, but not limited to,\nvectorstores. All retrievers implement some common methods, such as\ngetRelevantDocuments(). 3. QA 3.1 GETTING STARTED Distill the retrieved\ndocuments into an answer using an LLM (e.g., gpt-3.5-turbo) with RetrievalQA\nchain. import { RetrievalQAChain } from \"langchain/chains\"; import { ChatOpenAI\n} from \"langchain/chat_models/openai\"; const model = new ChatOpenAI({ modelName:\n\"gpt-3.5-turbo\" }); const chain = RetrievalQAChain.fromLLM(model,\nvectorStore.asRetriever()); const response = await chain.call({ query: \"What is\ntask","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":202,"to":242}}}}],["521",{"pageContent":"model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" }); const chain =\nRetrievalQAChain.fromLLM(model, vectorStore.asRetriever()); const response =\nawait chain.call({ query: \"What is task decomposition?\" });\nconsole.log(response); /* { text: 'Task decomposition refers to the process of\nbreaking down a larger task into smaller, more manageable subgoals. By\ndecomposing a task, it becomes easier for an agent or system to handle complex\ntasks efficiently. Task decomposition can be done through various methods such\nas using prompting or task-specific instructions, or through human inputs. It\nhelps in planning and organizing the steps required to complete a task\neffectively.' } */ 3.2 GOING DEEPER 3.2.1 INTEGRATIONS LLMs * Browse LLM\nintegrations and further documentation here [/docs/modules/model_io/models/].\n3.2.2 CUSTOMIZING THE PROMPT The prompt in RetrievalQA chain can be customized\nas follows. import { RetrievalQAChain } from \"langchain/chains\"; import {","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":242,"to":273}}}}],["522",{"pageContent":"here [/docs/modules/model_io/models/]. 3.2.2 CUSTOMIZING THE PROMPT The prompt\nin RetrievalQA chain can be customized as follows. import { RetrievalQAChain }\nfrom \"langchain/chains\"; import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; import { PromptTemplate } from\n\"langchain/prompts\"; const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\"\n}); const template = `Use the following pieces of context to answer the question\nat the end. If you don't know the answer, just say that you don't know, don't\ntry to make up an answer. Use three sentences maximum and keep the answer as\nconcise as possible. Always say \"thanks for asking!\" at the end of the answer.\n{context} Question: {question} Helpful Answer:`; const chain =\nRetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), { prompt:\nPromptTemplate.fromTemplate(template), }); const response = await chain.call({\nquery: \"What is task decomposition?\" }); console.log(response); /* { text: 'Task\ndecomposition is the","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":273,"to":305}}}}],["523",{"pageContent":"prompt: PromptTemplate.fromTemplate(template), }); const response = await\nchain.call({ query: \"What is task decomposition?\" }); console.log(response); /*\n{ text: 'Task decomposition is the process of breaking down a large task into\nsmaller, more manageable subgoals. This allows for efficient handling of complex\ntasks and aids in planning and organizing the steps needed to achieve the\noverall goal. Thanks for asking!' } */ 3.2.3 RETURNING SOURCE DOCUMENTS The full\nset of retrieved documents used for answer distillation can be returned using\nreturn_source_documents=True. import { RetrievalQAChain } from\n\"langchain/chains\"; import { ChatOpenAI } from \"langchain/chat_models/openai\";\nconst model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" }); const chain =\nRetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {\nreturnSourceDocuments: true }); const response = await chain.call({ query: \"What\nis task","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":305,"to":337}}}}],["524",{"pageContent":"\"gpt-3.5-turbo\" }); const chain = RetrievalQAChain.fromLLM(model,\nvectorStore.asRetriever(), { returnSourceDocuments: true }); const response =\nawait chain.call({ query: \"What is task decomposition?\" });\nconsole.log(response.sourceDocuments[0]); /* Document { pageContent: 'Task\ndecomposition can be done (1) by LLM with simple prompting like \"Steps for\nXYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using\ntask-specific instructions; e.g. \"Write a story outline.\" for writing a novel,\nor (3) with human inputs.', metadata: [Object] } */ 3.2.4 CUSTOMIZING RETRIEVED\nDOCS IN THE LLM PROMPT Retrieved documents can be fed to an LLM for answer\ndistillation in a few different ways. stuff, refine, and map-reduce chains for\npassing documents to an LLM prompt are well summarized here\n[/docs/modules/chains/document/]. stuff is commonly used because it simply\n\"stuffs\" all retrieved documents into the prompt. The loadQAChain\n[/docs/modules/chains/document/] methods","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":337,"to":368}}}}],["525",{"pageContent":"summarized here [/docs/modules/chains/document/]. stuff is commonly used because\nit simply \"stuffs\" all retrieved documents into the prompt. The loadQAChain\n[/docs/modules/chains/document/] methods are easy ways to pass documents to an\nLLM using these various approaches. import { loadQAStuffChain } from\n\"langchain/chains\"; const stuffChain = loadQAStuffChain(model); const\nstuffResult = await stuffChain.call({ input_documents: relevantDocs, question:\n\"What is task decomposition?\", }); console.log(stuffResult); /* { text: 'Task\ndecomposition is the process of breaking down a large task into smaller, more\nmanageable subgoals or steps. This allows for efficient handling of complex\ntasks by focusing on one subgoal at a time. Task decomposition can be done\nthrough various methods such as using simple prompting, task-specific\ninstructions, or human inputs.' } */ 4. CHAT 4.1 GETTING STARTED To keep chat\nhistory, we use a variant of the previous chain called a","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":368,"to":400}}}}],["526",{"pageContent":"methods such as using simple prompting, task-specific instructions, or human\ninputs.' } */ 4. CHAT 4.1 GETTING STARTED To keep chat history, we use a variant\nof the previous chain called a ConversationalRetrievalQAChain. First, specify a\nMemory buffer to track the conversation inputs / outputs. import {\nConversationalRetrievalQAChain } from \"langchain/chains\"; import { BufferMemory\n} from \"langchain/memory\"; import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; const memory = new BufferMemory({ memoryKey:\n\"chat_history\", returnMessages: true, }); Next, we initialize and call the\nchain: const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" }); const chain\n= ConversationalRetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {\nmemory }); const result = await chain.call({ question: \"What are some of the\nmain ideas in self-reflection?\" }); console.log(result); /* { text: 'Some main\nideas in self-reflection include:\\n' + '\\n' + '1. Iterative","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":400,"to":444}}}}],["527",{"pageContent":"chain.call({ question: \"What are some of the main ideas in self-reflection?\" });\nconsole.log(result); /* { text: 'Some main ideas in self-reflection include:\\n'\n+ '\\n' + '1. Iterative Improvement: Self-reflection allows autonomous agents to\nimprove by continuously refining past action decisions and correcting\nmistakes.\\n' + '\\n' + '2. Trial and Error: Self-reflection plays a crucial role\nin real-world tasks where trial and error are inevitable. It helps agents learn\nfrom failed trajectories and make adjustments for future actions.\\n' + '\\n' +\n'3. Constructive Criticism: Agents engage in constructive self-criticism of\ntheir big-picture behavior to identify areas for improvement.\\n' + '\\n' + '4.\nDecision and Strategy Refinement: Reflection on past decisions and strategies\nenables agents to refine their approach and make more informed choices.\\n' +\n'\\n' + '5. Efficiency and Optimization: Self-reflection encourages agents to be\nsmart and","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":444,"to":461}}}}],["528",{"pageContent":"decisions and strategies enables agents to refine their approach and make more\ninformed choices.\\n' + '\\n' + '5. Efficiency and Optimization: Self-reflection\nencourages agents to be smart and efficient in their actions, aiming to complete\ntasks in the least number of steps.\\n' + '\\n' + 'These ideas highlight the\nimportance of self-reflection in enhancing performance and guiding future\nactions.' } */ The Memory buffer has context to resolve \"it\" (\"self-reflection\")\nin the below question. const followupResult = await chain.call({ question: \"How\ndoes the Reflexion paper handle it?\" }); console.log(followupResult); /* { text:\n\"The Reflexion paper introduces a framework that equips agents with dynamic\nmemory and self-reflection capabilities to improve their reasoning skills. The\napproach involves showing the agent two-shot examples, where each example\nconsists of a failed trajectory and an ideal reflection on how to guide future\nchanges in the agent's plan. These","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":461,"to":481}}}}],["529",{"pageContent":"skills. The approach involves showing the agent two-shot examples, where each\nexample consists of a failed trajectory and an ideal reflection on how to guide\nfuture changes in the agent's plan. These reflections are then added to the\nagent's working memory as context for querying a language model. The agent uses\nthis self-reflection information to make decisions on whether to start a new\ntrial or continue with the current plan.\" } */ 4.2 GOING DEEPER The\ndocumentation [/docs/modules/chains/popular/chat_vector_db] on\nConversationalRetrievalQAChain offers a few extensions, such as streaming and\nsource documents. Previous Use cases [/docs/use_cases] Next Conversational\nRetrieval Agents\n[/docs/use_cases/question_answering/conversational_retrieval_agents] Community *\nDiscord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":481,"to":509}}}}],["530",{"pageContent":"* Twitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/","title":"QA and Chat over Documents | 🦜️🔗 Langchain","description":"Chat and Question-Answering (QA) over data are popular LLM use-cases.","language":"en","loc":{"lines":{"from":509,"to":519}}}}],["531",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Conversational Retrieval Agents\n[/docs/use_cases/question_answering/conversational_retrieval_agents] * Use local\nLLMs [/docs/use_cases/question_answering/local_retrieval_qa] * Tabular Question\nAnswering [/docs/use_cases/tabular] * Interacting with APIs\n[/docs/use_cases/api] * Summarization [/docs/use_cases/summarization] * Agent\nSimulations [/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] * QA\nand Chat over Documents","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents","title":"Conversational Retrieval Agents | 🦜️🔗 Langchain","description":"This is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["532",{"pageContent":"Agents [/docs/use_cases/autonomous_agents/] * Chatbots\n[/docs/use_cases/chatbots] * Extraction [/docs/use_cases/extraction] * / * Use\ncases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Conversational Retrieval Agents On this\npage CONVERSATIONAL RETRIEVAL AGENTS This is an agent specifically optimized for\ndoing retrieval when necessary while holding a conversation and being able to\nanswer questions based on previous dialogue in the conversation. To start, we\nwill set up the retriever we want to use, then turn it into a retriever tool.\nNext, we will use the high-level constructor for this type of agent. Finally, we\nwill walk through how to construct a conversational retrieval agent from\ncomponents. THE RETRIEVER To start, we need a retriever to use! The code here is\nmostly just example code. Feel free to use your own retriever and skip to the\nnext section on creating a retriever tool. import { FaissStore } from","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents","title":"Conversational Retrieval Agents | 🦜️🔗 Langchain","description":"This is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able","language":"en","loc":{"lines":{"from":24,"to":51}}}}],["533",{"pageContent":"we need a retriever to use! The code here is mostly just example code. Feel free\nto use your own retriever and skip to the next section on creating a retriever\ntool. import { FaissStore } from \"langchain/vectorstores/faiss\"; import {\nOpenAIEmbeddings } from \"langchain/embeddings/openai\"; import { TextLoader }\nfrom \"langchain/document_loaders/fs/text\"; import {\nRecursiveCharacterTextSplitter } from \"langchain/text_splitter\"; const loader =\nnew TextLoader(\"state_of_the_union.txt\"); const docs = await loader.load();\nconst splitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000,\nchunkOverlap: 0 }); const texts = await splitter.splitDocuments(docs); const\nvectorStore = await FaissStore.fromDocuments(texts, new OpenAIEmbeddings());\nconst retriever = vectorStore.asRetriever(); RETRIEVER TOOL Now we need to\ncreate a tool for our retriever. The main things we need to pass in are a name\nfor the retriever as well as a description. These will both be used by the\nlanguage","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents","title":"Conversational Retrieval Agents | 🦜️🔗 Langchain","description":"This is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able","language":"en","loc":{"lines":{"from":51,"to":79}}}}],["534",{"pageContent":"TOOL Now we need to create a tool for our retriever. The main things we need to\npass in are a name for the retriever as well as a description. These will both\nbe used by the language model, so they should be informative. import {\ncreateRetrieverTool } from \"langchain/agents/toolkits\"; const tool =\ncreateRetrieverTool(retriever, { name: \"search_state_of_union\", description:\n\"Searches and returns documents regarding the state-of-the-union.\", }); AGENT\nCONSTRUCTOR Here, we will use the high level\ncreate_conversational_retrieval_agent API to construct the agent. Notice that\nbeside the list of tools, the only thing we need to pass in is a language model\nto use. Under the hood, this agent is using the OpenAIFunctionsAgent, so we need\nto use an ChatOpenAI model. import { createConversationalRetrievalAgent } from\n\"langchain/agents/toolkits\"; import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; const model = new ChatOpenAI({ temperature: 0,\n}); const executor = await","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents","title":"Conversational Retrieval Agents | 🦜️🔗 Langchain","description":"This is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able","language":"en","loc":{"lines":{"from":79,"to":109}}}}],["535",{"pageContent":"} from \"langchain/agents/toolkits\"; import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; const model = new ChatOpenAI({ temperature: 0,\n}); const executor = await createConversationalRetrievalAgent(model, [tool], {\nverbose: true, }); We can now try it out! const result = await executor.call({\ninput: \"Hi, I'm Bob!\" }); console.log(result); /* { output: 'Hello Bob! How can\nI assist you today?', intermediateSteps: [] } */ const result2 = await\nexecutor.call({ input: \"What's my name?\" }); console.log(result2); /* { output:\n'Your name is Bob.', intermediateSteps: [] } */ const result3 = await\nexecutor.call({ input: \"What did the president say about Ketanji Brown Jackson\nin the most recent state of the union?\" }); console.log(result3); /* { output:\n\"In the most recent state of the union, President Biden mentioned Ketanji Brown\nJackson. He nominated her as a Circuit Court of Appeals judge and described her\nas one of the nation's top legal","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents","title":"Conversational Retrieval Agents | 🦜️🔗 Langchain","description":"This is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able","language":"en","loc":{"lines":{"from":109,"to":156}}}}],["536",{"pageContent":"\"In the most recent state of the union, President Biden mentioned Ketanji Brown\nJackson. He nominated her as a Circuit Court of Appeals judge and described her\nas one of the nation's top legal minds who will continue Justice Breyer's legacy\nof excellence. He mentioned that she has received a broad range of support,\nincluding from the Fraternal Order of Police and former judges appointed by\nDemocrats and Republicans.\", intermediateSteps: [ {...} ] } */ const result4 =\nawait executor.call({ input: \"How long ago did he nominate her?\" });\nconsole.log(result4); /* { output: 'President Biden nominated Ketanji Brown\nJackson four days before the most recent state of the union address.',\nintermediateSteps: [] } */ Note that for the final call, the agent used\npreviously retrieved information to answer the query and did not need to call\nthe tool again! Here's a trace showing how the agent fetches documents to answer\nthe question with the retrieval","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents","title":"Conversational Retrieval Agents | 🦜️🔗 Langchain","description":"This is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able","language":"en","loc":{"lines":{"from":156,"to":182}}}}],["537",{"pageContent":"used previously retrieved information to answer the query and did not need to\ncall the tool again! Here's a trace showing how the agent fetches documents to\nanswer the question with the retrieval tool:\nhttps://smith.langchain.com/public/1e2b1887-ca44-4210-913b-a69c1b8a8e7e/r\n[https://smith.langchain.com/public/1e2b1887-ca44-4210-913b-a69c1b8a8e7e/r]\nCREATING FROM COMPONENTS What actually is going on underneath the hood? Let's\ntake a look so we can understand how to modify things going forward. MEMORY In\nthis example, we want the agent to remember not only previous conversations, but\nalso previous intermediate steps. For that, we can use\nOpenAIAgentTokenBufferMemory. Note that if you want to change whether the agent\nremembers intermediate steps, how the long the retained buffer is, or anything\nlike that you should change this part. import { OpenAIAgentTokenBufferMemory }\nfrom \"langchain/agents/toolkits\"; const memory = new\nOpenAIAgentTokenBufferMemory({ llm: model,","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents","title":"Conversational Retrieval Agents | 🦜️🔗 Langchain","description":"This is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able","language":"en","loc":{"lines":{"from":182,"to":205}}}}],["538",{"pageContent":"is, or anything like that you should change this part. import {\nOpenAIAgentTokenBufferMemory } from \"langchain/agents/toolkits\"; const memory =\nnew OpenAIAgentTokenBufferMemory({ llm: model, memoryKey: \"chat_history\",\noutputKey: \"output\" }); You should make sure memoryKey is set to \"chat_history\"\nand outputKey is set to \"output\" for the OpenAI functions agent. This memory\nalso has returnMessages set to true by default. You can also load messages from\nprior conversations into this memory by initializing it with a pre-loaded chat\nhistory: import { ChatOpenAI } from \"langchain/chat_models/openai\"; import {\nOpenAIAgentTokenBufferMemory } from \"langchain/agents/toolkits\"; import {\nHumanMessage, AIMessage } from \"langchain/schema\"; import { ChatMessageHistory }\nfrom \"langchain/memory\"; const previousMessages = [ new HumanMessage(\"My name is\nBob\"), new AIMessage(\"Nice to meet you, Bob!\"), ]; const chatHistory = new\nChatMessageHistory(previousMessages); const memory = new","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents","title":"Conversational Retrieval Agents | 🦜️🔗 Langchain","description":"This is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able","language":"en","loc":{"lines":{"from":205,"to":235}}}}],["539",{"pageContent":"previousMessages = [ new HumanMessage(\"My name is Bob\"), new AIMessage(\"Nice to\nmeet you, Bob!\"), ]; const chatHistory = new\nChatMessageHistory(previousMessages); const memory = new\nOpenAIAgentTokenBufferMemory({ llm: new ChatOpenAI({}), memoryKey:\n\"chat_history\", outputKey: \"output\", chatHistory, }); AGENT EXECUTOR We can\nrecreate the agent executor directly with the initializeAgentExecutorWithOptions\nmethod. This allows us to customize the agent's system message by passing in a\nprefix into agentArgs. Importantly, we must pass in return_intermediate_steps:\ntrue since we are recording that with our memory object. import {\ninitializeAgentExecutorWithOptions } from \"langchain/agents\"; const executor =\nawait initializeAgentExecutorWithOptions(tools, llm, { agentType:\n\"openai-functions\", memory, returnIntermediateSteps: true, agentArgs: { prefix:\nprefix ?? `Do your best to answer the questions. Feel free to use any tools\navailable to look up","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents","title":"Conversational Retrieval Agents | 🦜️🔗 Langchain","description":"This is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able","language":"en","loc":{"lines":{"from":235,"to":268}}}}],["540",{"pageContent":"\"openai-functions\", memory, returnIntermediateSteps: true, agentArgs: { prefix:\nprefix ?? `Do your best to answer the questions. Feel free to use any tools\navailable to look up relevant information, only if necessary.`, }, }); Previous\nQA and Chat over Documents [/docs/use_cases/question_answering/] Next Use local\nLLMs [/docs/use_cases/question_answering/local_retrieval_qa] * The Retriever *\nRetriever Tool * Agent Constructor * Creating from components * Memory * Agent\nexecutor Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents","title":"Conversational Retrieval Agents | 🦜️🔗 Langchain","description":"This is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able","language":"en","loc":{"lines":{"from":268,"to":305}}}}],["541",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Conversational Retrieval Agents\n[/docs/use_cases/question_answering/conversational_retrieval_agents] * Use local\nLLMs [/docs/use_cases/question_answering/local_retrieval_qa] * Tabular Question\nAnswering [/docs/use_cases/tabular] * Interacting with APIs\n[/docs/use_cases/api] * Summarization [/docs/use_cases/summarization] * Agent\nSimulations [/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] * QA\nand Chat over Documents","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["542",{"pageContent":"Agents [/docs/use_cases/autonomous_agents/] * Chatbots\n[/docs/use_cases/chatbots] * Extraction [/docs/use_cases/extraction] * / * Use\ncases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Use local LLMs On this page USE LOCAL\nLLMS The popularity of projects like PrivateGPT\n[https://github.com/imartinez/privateGPT], llama.cpp\n[https://github.com/ggerganov/llama.cpp], and GPT4All\n[https://github.com/nomic-ai/gpt4all] underscore the importance of running LLMs\nlocally. LangChain integrates with Ollama [https://ollama.ai/] to run several\nopen source LLMs locally with GPU support. For example, here we show how to run\nLlama 2 locally (e.g., on your laptop) using local embeddings, a local vector\nstore, and a local LLM. You can check out other open-source models supported by\nOllama here [https://github.com/jmorganca/ollama#model-library]. This tutorial\nis designed for Node.js running on Mac OSX with at least 16 GB of RAM. SETUP\nFirst,","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":24,"to":53}}}}],["543",{"pageContent":"open-source models supported by Ollama here\n[https://github.com/jmorganca/ollama#model-library]. This tutorial is designed\nfor Node.js running on Mac OSX with at least 16 GB of RAM. SETUP First, install\npackages needed for local embeddings and vector storage. For this demo, we'll\nuse Llama 2 through Ollama as our LLM, Transformers.js\n[/docs/modules/data_connection/text_embedding/integrations/transformers/] for\nembeddings, and HNWSLib\n[/docs/modules/data_connection/vectorstores/integrations/hnswlib] as a vector\nstore for retrieval. We'll also install cheerio for scraping, though you can use\nany loader. * npm * Yarn * pnpm npm install @xenova/transformers npm install\nhnswlib-node npm install cheerio yarn add @xenova/transformers yarn add\nhnswlib-node yarn add cheerio pnpm add @xenova/transformers pnpm add\nhnswlib-node pnpm add cheerio You'll also need to set up Ollama and run a local\ninstance using these instructions","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":53,"to":91}}}}],["544",{"pageContent":"add hnswlib-node yarn add cheerio pnpm add @xenova/transformers pnpm add\nhnswlib-node pnpm add cheerio You'll also need to set up Ollama and run a local\ninstance using these instructions [https://github.com/jmorganca/ollama#ollama].\nDOCUMENT LOADING Next, we need to load some documents. We'll use a blog post on\nagents as an example. import { CheerioWebBaseLoader } from\n\"langchain/document_loaders/web/cheerio\"; import {\nRecursiveCharacterTextSplitter } from \"langchain/text_splitter\"; import {\nHNSWLib } from \"langchain/vectorstores/hnswlib\"; import {\nHuggingFaceTransformersEmbeddings } from \"langchain/embeddings/hf_transformers\";\nconst loader = new CheerioWebBaseLoader(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\" ); const docs = await\nloader.load(); const splitter = new RecursiveCharacterTextSplitter({\nchunkOverlap: 0, chunkSize: 500, }); const splitDocuments = await\nsplitter.splitDocuments(docs); const vectorstore = await HNSWLib.fromDocuments(","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":91,"to":128}}}}],["545",{"pageContent":"= new RecursiveCharacterTextSplitter({ chunkOverlap: 0, chunkSize: 500, });\nconst splitDocuments = await splitter.splitDocuments(docs); const vectorstore =\nawait HNSWLib.fromDocuments( splitDocuments, new\nHuggingFaceTransformersEmbeddings() ); const retrievedDocs = await\nvectorstore.similaritySearch( \"What are the approaches to Task Decomposition?\"\n); console.log(retrievedDocs[0]); /* Document { pageContent: 'Task decomposition\ncan be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What\nare the subgoals for achieving XYZ?\", (2) by using task-specific instructions;\ne.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.',\nmetadata: { source: 'https://lilianweng.github.io/posts/2023-06-23-agent/', loc:\n{ lines: [Object] } } } */ API REFERENCE: * CheerioWebBaseLoader\n[/docs/api/document_loaders_web_cheerio/classes/CheerioWebBaseLoader] from\nlangchain/document_loaders/web/cheerio *","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":128,"to":163}}}}],["546",{"pageContent":"{ lines: [Object] } } } */ API REFERENCE: * CheerioWebBaseLoader\n[/docs/api/document_loaders_web_cheerio/classes/CheerioWebBaseLoader] from\nlangchain/document_loaders/web/cheerio * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter * HNSWLib\n[/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * HuggingFaceTransformersEmbeddings\n[/docs/api/embeddings_hf_transformers/classes/HuggingFaceTransformersEmbeddings]\nfrom langchain/embeddings/hf_transformers COMPOSABLE CHAIN We can use a chain\nfor retrieval by passing in the retrieved docs and a prompt. It formats the\nprompt template using the input key values provided and passes the formatted\nstring to Llama 2, or another specified LLM. In this case, the documents\nretrieved by the vector-store powered retriever are converted to strings and\npassed into the {context} variable in the prompt: import {","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":163,"to":191}}}}],["547",{"pageContent":"2, or another specified LLM. In this case, the documents retrieved by the\nvector-store powered retriever are converted to strings and passed into the\n{context} variable in the prompt: import { CheerioWebBaseLoader } from\n\"langchain/document_loaders/web/cheerio\"; import {\nRecursiveCharacterTextSplitter } from \"langchain/text_splitter\"; import {\nHNSWLib } from \"langchain/vectorstores/hnswlib\"; import { Ollama } from\n\"langchain/llms/ollama\"; import { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableSequence, RunnablePassthrough, } from\n\"langchain/schema/runnable\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; import { Document } from \"langchain/document\";\nimport { HuggingFaceTransformersEmbeddings } from\n\"langchain/embeddings/hf_transformers\"; const loader = new CheerioWebBaseLoader(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\" ); const docs = await\nloader.load(); const splitter = new RecursiveCharacterTextSplitter({\nchunkOverlap:","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":191,"to":216}}}}],["548",{"pageContent":"loader = new CheerioWebBaseLoader(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\" ); const docs = await\nloader.load(); const splitter = new RecursiveCharacterTextSplitter({\nchunkOverlap: 0, chunkSize: 500, }); const splitDocuments = await\nsplitter.splitDocuments(docs); const vectorstore = await HNSWLib.fromDocuments(\nsplitDocuments, new HuggingFaceTransformersEmbeddings() ); const retriever =\nvectorstore.asRetriever(); // Prompt const prompt =\nPromptTemplate.fromTemplate(`Answer the question based only on the following\ncontext: {context} Question: {question}`); // Llama 2 7b wrapped by Ollama const\nmodel = new Ollama({ baseUrl: \"http://localhost:11434\", model: \"llama2\", });\nconst serializeDocs = (docs: Document[]) => docs.map((doc) =>\ndoc.pageContent).join(\"\\n\"); const chain = RunnableSequence.from([ { context:\nretriever.pipe(serializeDocs), question: new RunnablePassthrough(), }, prompt,\nmodel, new StringOutputParser(), ]); const","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":216,"to":261}}}}],["549",{"pageContent":"chain = RunnableSequence.from([ { context: retriever.pipe(serializeDocs),\nquestion: new RunnablePassthrough(), }, prompt, model, new StringOutputParser(),\n]); const result = await chain.invoke( \"What are the approaches to Task\nDecomposition?\" ); console.log(result); /* Based on the provided context, there\nare three approaches to task decomposition: 1. Using simple prompts like \"Steps\nfor XYZ\" or \"What are the subgoals for achieving XYZ?\" to elicit a list of tasks\nfrom a language model (LLM). 2. Providing task-specific instructions, such as\n\"Write a story outline\" for writing a novel, to guide the LLM in decomposing the\ntask into smaller subtasks. 3. Incorporating human inputs to help the LLM learn\nand improve its decomposition abilities over time. */ API REFERENCE: *\nCheerioWebBaseLoader\n[/docs/api/document_loaders_web_cheerio/classes/CheerioWebBaseLoader] from\nlangchain/document_loaders/web/cheerio * RecursiveCharacterTextSplitter","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":261,"to":292}}}}],["550",{"pageContent":"REFERENCE: * CheerioWebBaseLoader\n[/docs/api/document_loaders_web_cheerio/classes/CheerioWebBaseLoader] from\nlangchain/document_loaders/web/cheerio * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter * HNSWLib\n[/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * Ollama [/docs/api/llms_ollama/classes/Ollama]\nfrom langchain/llms/ollama * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nRunnableSequence [/docs/api/schema_runnable/classes/RunnableSequence] from\nlangchain/schema/runnable * RunnablePassthrough\n[/docs/api/schema_runnable/classes/RunnablePassthrough] from\nlangchain/schema/runnable * StringOutputParser\n[/docs/api/schema_output_parser/classes/StringOutputParser] from\nlangchain/schema/output_parser * Document [/docs/api/document/classes/Document]\nfrom langchain/document * HuggingFaceTransformersEmbeddings","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":292,"to":304}}}}],["551",{"pageContent":"from langchain/schema/output_parser * Document\n[/docs/api/document/classes/Document] from langchain/document *\nHuggingFaceTransformersEmbeddings\n[/docs/api/embeddings_hf_transformers/classes/HuggingFaceTransformersEmbeddings]\nfrom langchain/embeddings/hf_transformers RETRIEVALQA For an even simpler flow,\nuse the preconfigured RetrievalQAChain. This will use a default QA prompt and\nwill retrieve from the vector store. You can still pass in a custom prompt if\ndesired. type: \"stuff\" (see here [/docs/modules/chains/document/stuff]) means\nthat all the docs will be added (stuffed) into a prompt. import {\nRetrievalQAChain, loadQAStuffChain } from \"langchain/chains\"; import {\nCheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\"; import {\nRecursiveCharacterTextSplitter } from \"langchain/text_splitter\"; import {\nHNSWLib } from \"langchain/vectorstores/hnswlib\"; import { Ollama } from\n\"langchain/llms/ollama\"; import { PromptTemplate } from \"langchain/prompts\";\nimport {","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":304,"to":326}}}}],["552",{"pageContent":"from \"langchain/text_splitter\"; import { HNSWLib } from\n\"langchain/vectorstores/hnswlib\"; import { Ollama } from\n\"langchain/llms/ollama\"; import { PromptTemplate } from \"langchain/prompts\";\nimport { HuggingFaceTransformersEmbeddings } from\n\"langchain/embeddings/hf_transformers\"; const loader = new CheerioWebBaseLoader(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\" ); const docs = await\nloader.load(); const splitter = new RecursiveCharacterTextSplitter({\nchunkOverlap: 0, chunkSize: 500, }); const splitDocuments = await\nsplitter.splitDocuments(docs); const vectorstore = await HNSWLib.fromDocuments(\nsplitDocuments, new HuggingFaceTransformersEmbeddings() ); const retriever =\nvectorstore.asRetriever(); // Llama 2 7b wrapped by Ollama const model = new\nOllama({ baseUrl: \"http://localhost:11434\", model: \"llama2\", }); const template\n= `Use the following pieces of context to answer the question at the end. If you\ndon't know the answer, just say that you don't","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":326,"to":358}}}}],["553",{"pageContent":"\"http://localhost:11434\", model: \"llama2\", }); const template = `Use the\nfollowing pieces of context to answer the question at the end. If you don't know\nthe answer, just say that you don't know, don't try to make up an answer. Use\nthree sentences maximum and keep the answer as concise as possible. Always say\n\"thanks for asking!\" at the end of the answer. {context} Question: {question}\nHelpful Answer:`; const QA_CHAIN_PROMPT = new PromptTemplate({ inputVariables:\n[\"context\", \"question\"], template, }); // Create a retrieval QA chain that uses\na Llama 2-powered QA stuff chain with a custom prompt. const chain = new\nRetrievalQAChain({ combineDocumentsChain: loadQAStuffChain(model, { prompt:\nQA_CHAIN_PROMPT }), retriever, returnSourceDocuments: true, inputKey:\n\"question\", }); const response = await chain.call({ question: \"What are the\napproaches to Task Decomposition?\", }); console.log(response); /* { text:\n'Thanks for asking! There are several approaches to","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":358,"to":391}}}}],["554",{"pageContent":"response = await chain.call({ question: \"What are the approaches to Task\nDecomposition?\", }); console.log(response); /* { text: 'Thanks for asking! There\nare several approaches to task decomposition, which can be categorized into\nthree main types:\\n' + '\\n' + '1. Using language models with simple prompting\n(e.g., \"Steps for XYZ.\"), or asking for subgoals for achieving XYZ.\\n' + '2.\nProviding task-specific instructions, such as writing a story outline for\nwriting a novel.\\n' + '3. Incorporating human inputs to decompose tasks.\\n' +\n'\\n' + 'Each approach has its advantages and limitations, and the choice of\nwhich one to use depends on the specific task and the desired level of\ncomplexity and adaptability. Thanks for asking!', sourceDocuments: [ Document {\npageContent: 'Task decomposition can be done (1) by LLM with simple prompting\nlike \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by\nusing","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":391,"to":408}}}}],["555",{"pageContent":"[ Document { pageContent: 'Task decomposition can be done (1) by LLM with simple\nprompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving\nXYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\"\nfor writing a novel, or (3) with human inputs.', metadata: [Object] }, Document\n{ pageContent: 'Fig. 1. Overview of a LLM-powered autonomous agent system.\\n' +\n'Component One: Planning#\\n' + 'A complicated task usually involves many steps.\nAn agent needs to know what they are and plan ahead.\\n' + 'Task Decomposition#',\nmetadata: [Object] }, Document { pageContent: 'Challenges in long-term planning\nand task decomposition: Planning over a lengthy history and effectively\nexploring the solution space remain challenging. LLMs struggle to adjust plans\nwhen faced with unexpected errors, making them less robust compared to humans\nwho learn from trial and error.',","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":408,"to":421}}}}],["556",{"pageContent":"exploring the solution space remain challenging. LLMs struggle to adjust plans\nwhen faced with unexpected errors, making them less robust compared to humans\nwho learn from trial and error.', metadata: [Object] }, Document { pageContent:\n'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning\npossibilities at each step. It first decomposes the problem into multiple\nthought steps and generates multiple thoughts per step, creating a tree\nstructure. The search process can be BFS (breadth-first search) or DFS\n(depth-first search) with each state evaluated by a classifier (via a prompt) or\nmajority vote.', metadata: [Object] } ] } */ API REFERENCE: * RetrievalQAChain\n[/docs/api/chains/classes/RetrievalQAChain] from langchain/chains *\nloadQAStuffChain [/docs/api/chains/functions/loadQAStuffChain] from\nlangchain/chains * CheerioWebBaseLoader\n[/docs/api/document_loaders_web_cheerio/classes/CheerioWebBaseLoader]","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":421,"to":439}}}}],["557",{"pageContent":"langchain/chains * loadQAStuffChain\n[/docs/api/chains/functions/loadQAStuffChain] from langchain/chains *\nCheerioWebBaseLoader\n[/docs/api/document_loaders_web_cheerio/classes/CheerioWebBaseLoader] from\nlangchain/document_loaders/web/cheerio * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter * HNSWLib\n[/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * Ollama [/docs/api/llms_ollama/classes/Ollama]\nfrom langchain/llms/ollama * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nHuggingFaceTransformersEmbeddings\n[/docs/api/embeddings_hf_transformers/classes/HuggingFaceTransformersEmbeddings]\nfrom langchain/embeddings/hf_transformers Previous Conversational Retrieval\nAgents [/docs/use_cases/question_answering/conversational_retrieval_agents] Next\nTabular Question Answering [/docs/use_cases/tabular] * Setup * Document loading\n*","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":439,"to":458}}}}],["558",{"pageContent":"Retrieval Agents\n[/docs/use_cases/question_answering/conversational_retrieval_agents] Next\nTabular Question Answering [/docs/use_cases/tabular] * Setup * Document loading\n* Composable chain * RetrievalQA Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa","title":"Use local LLMs | 🦜️🔗 Langchain","description":"The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.","language":"en","loc":{"lines":{"from":458,"to":480}}}}],["559",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Prompts [/docs/modules/model_io/prompts/] * Language\nmodels [/docs/modules/model_io/models/] * LLMs\n[/docs/modules/model_io/models/llms/] * How-to","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/llms/","title":"LLMs | 🦜️🔗 Langchain","description":"Large Language Models (LLMs) are a core component of LangChain.","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["560",{"pageContent":"* Prompts [/docs/modules/model_io/prompts/] * Language models\n[/docs/modules/model_io/models/] * LLMs [/docs/modules/model_io/models/llms/] *\nHow-to [/docs/modules/model_io/models/llms/how_to/cancelling_requests] *\nIntegrations [/docs/modules/model_io/models/llms/integrations/ai21] * Chat\nmodels [/docs/modules/model_io/models/chat/] * Output parsers\n[/docs/modules/model_io/output_parsers/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * Memory\n[/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/llms/","title":"LLMs | 🦜️🔗 Langchain","description":"Large Language Models (LLMs) are a core component of LangChain.","language":"en","loc":{"lines":{"from":24,"to":47}}}}],["561",{"pageContent":"*\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Language models [/docs/modules/model_io/models/] *\nLLMs On this page LLMS Large Language Models (LLMs) are a core component of\nLangChain. LangChain does not serve its own LLMs, but rather provides a standard\ninterface for interacting with many different LLMs. For more detailed\ndocumentation check out our: * How-to guides: Walkthroughs of core\nfunctionality, like streaming, async, etc. * Integrations: How to use different\nLLM providers (OpenAI, Anthropic, etc.) GET STARTED There are lots of LLM\nproviders (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to\nprovide a standard interface for all of them. In this walkthrough we'll work\nwith an OpenAI LLM wrapper, although the functionalities highlighted are generic\nfor all","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/llms/","title":"LLMs | 🦜️🔗 Langchain","description":"Large Language Models (LLMs) are a core component of LangChain.","language":"en","loc":{"lines":{"from":47,"to":77}}}}],["562",{"pageContent":"- the LLM class is designed to provide a standard interface for all of them. In\nthis walkthrough we'll work with an OpenAI LLM wrapper, although the\nfunctionalities highlighted are generic for all LLM types. SETUP To start we'll\nneed to install the official OpenAI package: * npm * Yarn * pnpm npm install -S\nopenai yarn add openai pnpm add openai Accessing the API requires an API key,\nwhich you can get by creating an account and heading here\n[https://platform.openai.com/account/api-keys]. Once we have a key we'll want to\nset it as an environment variable by running: export OPENAI_API_KEY=\"...\" If\nyou'd prefer not to set an environment variable you can pass the key in directly\nvia the openAIApiKey parameter when initializing the OpenAI LLM class: import {\nOpenAI } from \"langchain/llms/openai\"; const llm = new OpenAI({ openAIApiKey:\n\"YOUR_KEY_HERE\", }); otherwise you can initialize with an empty object: import {\nOpenAI } from","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/llms/","title":"LLMs | 🦜️🔗 Langchain","description":"Large Language Models (LLMs) are a core component of LangChain.","language":"en","loc":{"lines":{"from":77,"to":128}}}}],["563",{"pageContent":"class: import { OpenAI } from \"langchain/llms/openai\"; const llm = new OpenAI({\nopenAIApiKey: \"YOUR_KEY_HERE\", }); otherwise you can initialize with an empty\nobject: import { OpenAI } from \"langchain/llms/openai\"; const llm = new\nOpenAI({}); CALL: STRING IN -> STRING OUT The simplest way to use an LLM is the\n.call method: pass in a string, get a string completion. const res = await\nllm.call(\"Tell me a joke\"); console.log(res); // \"Why did the chicken cross the\nroad?\\n\\nTo get to the other side.\" GENERATE: BATCH CALLS, RICHER OUTPUTS\ngenerate lets you can call the model with a list of strings, getting back a more\ncomplete response than just the text. This complete response can include things\nlike multiple top responses and other LLM provider-specific information: const\nllmResult = await llm.generate([\"Tell me a joke\", \"Tell me a poem\"], [\"Tell me a\njoke\", \"Tell me a poem\"]); console.log(llmResult.generations.length) //","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/llms/","title":"LLMs | 🦜️🔗 Langchain","description":"Large Language Models (LLMs) are a core component of LangChain.","language":"en","loc":{"lines":{"from":128,"to":172}}}}],["564",{"pageContent":"LLM provider-specific information: const llmResult = await llm.generate([\"Tell\nme a joke\", \"Tell me a poem\"], [\"Tell me a joke\", \"Tell me a poem\"]);\nconsole.log(llmResult.generations.length) // 30\nconsole.log(llmResult.generations[0]); /* [ { text: \"\\n\\nQ: What did the fish\nsay when it hit the wall?\\nA: Dam!\", generationInfo: { finishReason: \"stop\",\nlogprobs: null } } ] */ console.log(llmResult.generations[1]); /* [ { text:\n\"\\n\\nRoses are red,\\nViolets are blue,\\nSugar is sweet,\\nAnd so are you.\",\ngenerationInfo: { finishReason: \"stop\", logprobs: null } } ] */ You can also\naccess provider specific information that is returned. This information is NOT\nstandardized across providers. console.log(llmResult.llmOutput); /* {\ntokenUsage: { completionTokens: 46, promptTokens: 8, totalTokens: 54 } } */\nHere's an example with additional parameters, which sets -1 for max_tokens to\nturn on token size calculations: import {","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/llms/","title":"LLMs | 🦜️🔗 Langchain","description":"Large Language Models (LLMs) are a core component of LangChain.","language":"en","loc":{"lines":{"from":172,"to":221}}}}],["565",{"pageContent":"{ completionTokens: 46, promptTokens: 8, totalTokens: 54 } } */ Here's an\nexample with additional parameters, which sets -1 for max_tokens to turn on\ntoken size calculations: import { OpenAI } from \"langchain/llms/openai\"; export\nconst run = async () => { const model = new OpenAI({ // customize openai model\nthat's used, `text-davinci-003` is the default modelName: \"text-ada-001\", //\n`max_tokens` supports a magic -1 param where the max token length for the\nspecified modelName // is calculated and included in the request to OpenAI as\nthe `max_tokens` param maxTokens: -1, // use `modelKwargs` to pass params\ndirectly to the openai call // note that they use snake_case instead of\ncamelCase modelKwargs: { user: \"me\", }, // for additional logging for debugging\npurposes verbose: true, }); const resA = await model.call( \"What would be a good\ncompany name a company that makes colorful socks?\" ); console.log({ resA","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/llms/","title":"LLMs | 🦜️🔗 Langchain","description":"Large Language Models (LLMs) are a core component of LangChain.","language":"en","loc":{"lines":{"from":221,"to":255}}}}],["566",{"pageContent":"logging for debugging purposes verbose: true, }); const resA = await model.call(\n\"What would be a good company name a company that makes colorful socks?\" );\nconsole.log({ resA }); // { resA: '\\n\\nSocktastic Colors' } }; API REFERENCE: *\nOpenAI [/docs/api/llms_openai/classes/OpenAI] from langchain/llms/openai\nADVANCED This section is for users who want a deeper technical understanding of\nhow LangChain works. If you are just getting started, you can skip this section.\nBoth LLMs and Chat Models are built on top of the BaseLanguageModel class. This\nclass provides a common interface for all models, and allows us to easily swap\nout models in chains without changing the rest of the code. The\nBaseLanguageModel class has two abstract methods: generatePrompt and\ngetNumTokens, which are implemented by BaseChatModel and BaseLLM respectively.\nBaseLLM is a subclass of BaseLanguageModel that provides a common interface for\nLLMs while BaseChatModel is a subclass","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/llms/","title":"LLMs | 🦜️🔗 Langchain","description":"Large Language Models (LLMs) are a core component of LangChain.","language":"en","loc":{"lines":{"from":255,"to":285}}}}],["567",{"pageContent":"getNumTokens, which are implemented by BaseChatModel and BaseLLM respectively.\nBaseLLM is a subclass of BaseLanguageModel that provides a common interface for\nLLMs while BaseChatModel is a subclass of BaseLanguageModel that provides a\ncommon interface for chat models. Previous Language models\n[/docs/modules/model_io/models/] Next Cancelling requests\n[/docs/modules/model_io/models/llms/how_to/cancelling_requests] * Get started\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/model_io/models/llms/","title":"LLMs | 🦜️🔗 Langchain","description":"Large Language Models (LLMs) are a core component of LangChain.","language":"en","loc":{"lines":{"from":285,"to":311}}}}],["568",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nDocument loaders [/docs/modules/data_connection/document_loaders/] * Document\ntransformers","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":23}}}}],["569",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Document loaders\n[/docs/modules/data_connection/document_loaders/] * Document transformers\n[/docs/modules/data_connection/document_transformers/] * Text embedding models\n[/docs/modules/data_connection/text_embedding/] * Vector stores\n[/docs/modules/data_connection/vectorstores/] * Integrations\n[/docs/modules/data_connection/vectorstores/integrations/] * Memory\n[/docs/modules/data_connection/vectorstores/integrations/memory] * AnalyticDB\n[/docs/modules/data_connection/vectorstores/integrations/analyticdb] * Chroma\n[/docs/modules/data_connection/vectorstores/integrations/chroma] * Cloudflare\nVectorize\n[/docs/modules/data_connection/vectorstores/integrations/cloudflare_vectorize] *\nElasticsearch\n[/docs/modules/data_connection/vectorstores/integrations/elasticsearch] * Faiss","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":23,"to":35}}}}],["570",{"pageContent":"[/docs/modules/data_connection/vectorstores/integrations/cloudflare_vectorize] *\nElasticsearch\n[/docs/modules/data_connection/vectorstores/integrations/elasticsearch] * Faiss\n[/docs/modules/data_connection/vectorstores/integrations/faiss] * Google Vertex\nAI Matching Engine\n[/docs/modules/data_connection/vectorstores/integrations/googlevertexai] *\nHNSWLib [/docs/modules/data_connection/vectorstores/integrations/hnswlib] *\nLanceDB [/docs/modules/data_connection/vectorstores/integrations/lancedb] *\nMilvus [/docs/modules/data_connection/vectorstores/integrations/milvus] *\nMongoDB Atlas\n[/docs/modules/data_connection/vectorstores/integrations/mongodb_atlas] *\nMyScale [/docs/modules/data_connection/vectorstores/integrations/myscale] *\nOpenSearch [/docs/modules/data_connection/vectorstores/integrations/opensearch]\n* PGVector [/docs/modules/data_connection/vectorstores/integrations/pgvector] *","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":35,"to":46}}}}],["571",{"pageContent":"* OpenSearch\n[/docs/modules/data_connection/vectorstores/integrations/opensearch] * PGVector\n[/docs/modules/data_connection/vectorstores/integrations/pgvector] * Pinecone\n[/docs/modules/data_connection/vectorstores/integrations/pinecone] * Prisma\n[/docs/modules/data_connection/vectorstores/integrations/prisma] * Qdrant\n[/docs/modules/data_connection/vectorstores/integrations/qdrant] * Redis\n[/docs/modules/data_connection/vectorstores/integrations/redis] * SingleStore\n[/docs/modules/data_connection/vectorstores/integrations/singlestore] * Supabase\n[/docs/modules/data_connection/vectorstores/integrations/supabase] * Tigris\n[/docs/modules/data_connection/vectorstores/integrations/tigris] * TypeORM\n[/docs/modules/data_connection/vectorstores/integrations/typeorm] * Typesense\n[/docs/modules/data_connection/vectorstores/integrations/typesense] * USearch","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":46,"to":57}}}}],["572",{"pageContent":"* TypeORM [/docs/modules/data_connection/vectorstores/integrations/typeorm] *\nTypesense [/docs/modules/data_connection/vectorstores/integrations/typesense] *\nUSearch [/docs/modules/data_connection/vectorstores/integrations/usearch] *\nVectara [/docs/modules/data_connection/vectorstores/integrations/vectara] *\nVercel Postgres\n[/docs/modules/data_connection/vectorstores/integrations/vercel_postgres] * Voy\n[/docs/modules/data_connection/vectorstores/integrations/voy] * Weaviate\n[/docs/modules/data_connection/vectorstores/integrations/weaviate] * Xata\n[/docs/modules/data_connection/vectorstores/integrations/xata] * Zep\n[/docs/modules/data_connection/vectorstores/integrations/zep] * Retrievers\n[/docs/modules/data_connection/retrievers/] * Experimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Chains [/docs/modules/chains/] * Memory","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":57,"to":69}}}}],["573",{"pageContent":"[/docs/modules/data_connection/retrievers/] * Experimental\n[/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai]\n* Chains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Retrieval\n[/docs/modules/data_connection/] * Vector stores\n[/docs/modules/data_connection/vectorstores/] * Integrations VECTOR STORES:\nINTEGRATIONS 📄️ MEMORY MemoryVectorStore is an in-memory, ephemeral vectorstore\nthat stores embeddings in-memory and does an exact, linear search for the most\nsimilar","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":69,"to":98}}}}],["574",{"pageContent":"STORES: INTEGRATIONS 📄️ MEMORY MemoryVectorStore is an in-memory, ephemeral\nvectorstore that stores embeddings in-memory and does an exact, linear search\nfor the most similar embeddings. The default similarity metric is cosine\nsimilarity, but can be changed to any of the similarity metrics supported by\nml-distance. [/docs/modules/data_connection/vectorstores/integrations/memory]\n📄️ ANALYTICDB AnalyticDB for PostgreSQL is a massively parallel processing\n(MPP) data warehousing service that is designed to analyze large volumes of data\nonline. [/docs/modules/data_connection/vectorstores/integrations/analyticdb] 📄️\nCHROMA Chroma is a AI-native open-source vector database focused on developer\nproductivity and happiness. Chroma is licensed under Apache 2.0.\n[/docs/modules/data_connection/vectorstores/integrations/chroma] 📄️ CLOUDFLARE\nVECTORIZE If you're deploying your project in a Cloudflare worker, you can use\nCloudflare Vectorize with","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":98,"to":128}}}}],["575",{"pageContent":"CLOUDFLARE VECTORIZE If you're deploying your project in a Cloudflare worker,\nyou can use Cloudflare Vectorize with LangChain.js.\n[/docs/modules/data_connection/vectorstores/integrations/cloudflare_vectorize]\n📄️ ELASTICSEARCH Only available on Node.js.\n[/docs/modules/data_connection/vectorstores/integrations/elasticsearch] 📄️\nFAISS Only available on Node.js.\n[/docs/modules/data_connection/vectorstores/integrations/faiss] 📄️ GOOGLE\nVERTEX AI MATCHING ENGINE Only available on Node.js.\n[/docs/modules/data_connection/vectorstores/integrations/googlevertexai] 📄️\nHNSWLIB Only available on Node.js.\n[/docs/modules/data_connection/vectorstores/integrations/hnswlib] 📄️ LANCEDB\nLanceDB is an embedded vector database for AI applications. It is open source\nand distributed with an Apache-2.0 license.\n[/docs/modules/data_connection/vectorstores/integrations/lancedb] 📄️ MILVUS\nMilvus is a vector database built for embeddings similarity search and AI","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":128,"to":172}}}}],["576",{"pageContent":"and distributed with an Apache-2.0 license.\n[/docs/modules/data_connection/vectorstores/integrations/lancedb] 📄️ MILVUS\nMilvus is a vector database built for embeddings similarity search and AI\napplications. [/docs/modules/data_connection/vectorstores/integrations/milvus]\n📄️ MONGODB ATLAS Only available on Node.js.\n[/docs/modules/data_connection/vectorstores/integrations/mongodb_atlas] 📄️\nMYSCALE Only available on Node.js.\n[/docs/modules/data_connection/vectorstores/integrations/myscale] 📄️ OPENSEARCH\nOnly available on Node.js.\n[/docs/modules/data_connection/vectorstores/integrations/opensearch] 📄️\nPGVECTOR To enable vector search in a generic PostgreSQL database, LangChain.js\nsupports using the pgvector Postgres extension.\n[/docs/modules/data_connection/vectorstores/integrations/pgvector] 📄️ PINECONE\nOnly available on Node.js.\n[/docs/modules/data_connection/vectorstores/integrations/pinecone] 📄️ PRISMA\nFor augmenting existing models in PostgreSQL","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":172,"to":221}}}}],["577",{"pageContent":"PINECONE Only available on Node.js.\n[/docs/modules/data_connection/vectorstores/integrations/pinecone] 📄️ PRISMA\nFor augmenting existing models in PostgreSQL database with vector search,\nLangchain supports using Prisma together with PostgreSQL and pgvector Postgres\nextension. [/docs/modules/data_connection/vectorstores/integrations/prisma] 📄️\nQDRANT Qdrant is a vector similarity search engine. It provides a\nproduction-ready service with a convenient API to store, search, and manage\npoints - vectors with an additional payload.\n[/docs/modules/data_connection/vectorstores/integrations/qdrant] 📄️ REDIS Redis\nis a fast open source, in-memory data store.\n[/docs/modules/data_connection/vectorstores/integrations/redis] 📄️ SINGLESTORE\nSingleStoreDB is a high-performance distributed SQL database that supports\ndeployment both in the cloud and on-premise. It provides vector storage, as well\nas vector functions like dotproduct and euclideandistance, thereby supporting AI","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":221,"to":254}}}}],["578",{"pageContent":"SQL database that supports deployment both in the cloud and on-premise. It\nprovides vector storage, as well as vector functions like dotproduct and\neuclideandistance, thereby supporting AI applications that require text\nsimilarity matching.\n[/docs/modules/data_connection/vectorstores/integrations/singlestore] 📄️\nSUPABASE Langchain supports using Supabase Postgres database as a vector store,\nusing the pgvector postgres extension. Refer to the Supabase blog post for more\ninformation. [/docs/modules/data_connection/vectorstores/integrations/supabase]\n📄️ TIGRIS Tigris makes it easy to build AI applications with vector embeddings.\n[/docs/modules/data_connection/vectorstores/integrations/tigris] 📄️ TYPEORM To\nenable vector search in a generic PostgreSQL database, LangChainJS supports\nusing TypeORM with the pgvector Postgres extension.\n[/docs/modules/data_connection/vectorstores/integrations/typeorm] 📄️ TYPESENSE\nVector store that utilizes the Typesense search","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":254,"to":285}}}}],["579",{"pageContent":"supports using TypeORM with the pgvector Postgres extension.\n[/docs/modules/data_connection/vectorstores/integrations/typeorm] 📄️ TYPESENSE\nVector store that utilizes the Typesense search engine.\n[/docs/modules/data_connection/vectorstores/integrations/typesense] 📄️ USEARCH\nOnly available on Node.js.\n[/docs/modules/data_connection/vectorstores/integrations/usearch] 📄️ VECTARA\nVectara is a platform for building GenAI applications. It provides an\neasy-to-use API for document indexing and querying that is managed by Vectara\nand is optimized for performance and accuracy.\n[/docs/modules/data_connection/vectorstores/integrations/vectara] 📄️ VERCEL\nPOSTGRES LangChain.js supports using the @vercel/postgres package to use generic\nPostgres databases\n[/docs/modules/data_connection/vectorstores/integrations/vercel_postgres] 📄️\nVOY Voy is a WASM vector similarity search engine written in Rust.\n[/docs/modules/data_connection/vectorstores/integrations/voy] 📄️","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":285,"to":326}}}}],["580",{"pageContent":"VOY Voy is a WASM vector similarity search engine written in Rust.\n[/docs/modules/data_connection/vectorstores/integrations/voy] 📄️ WEAVIATE\nWeaviate is an open source vector database that stores both objects and vectors,\nallowing for combining vector search with structured filtering. LangChain\nconnects to Weaviate via the weaviate-ts-client package, the official Typescript\nclient for Weaviate.\n[/docs/modules/data_connection/vectorstores/integrations/weaviate] 📄️ XATA Xata\nis a serverless data platform, based on PostgreSQL. It provides a type-safe\nTypeScript/JavaScript SDK for interacting with your database, and a UI for\nmanaging your data.\n[/docs/modules/data_connection/vectorstores/integrations/xata] 📄️ ZEP Zep is an\nopen source long-term memory store for LLM applications. Zep makes it easy to\nadd relevant documents,\n[/docs/modules/data_connection/vectorstores/integrations/zep] Previous Vector","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":326,"to":356}}}}],["581",{"pageContent":"ZEP Zep is an open source long-term memory store for LLM applications. Zep makes\nit easy to add relevant documents,\n[/docs/modules/data_connection/vectorstores/integrations/zep] Previous Vector\nstores [/docs/modules/data_connection/vectorstores/] Next Memory\n[/docs/modules/data_connection/vectorstores/integrations/memory] Community *\nDiscord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/","title":"Vector Stores: Integrations | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":356,"to":379}}}}],["582",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * How to [/docs/modules/chains/how_to/] * Foundational","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":1,"to":24}}}}],["583",{"pageContent":"* Model I/ O [/docs/modules/model_io/] * Retrieval\n[/docs/modules/data_connection/] * Chains [/docs/modules/chains/] * How to\n[/docs/modules/chains/how_to/] * Foundational\n[/docs/modules/chains/foundational/] * Documents\n[/docs/modules/chains/document/] * Popular [/docs/modules/chains/popular/] * API\nchains [/docs/modules/chains/popular/api] * Retrieval QA\n[/docs/modules/chains/popular/vector_db_qa] * Conversational Retrieval QA\n[/docs/modules/chains/popular/chat_vector_db] * SQL\n[/docs/modules/chains/popular/sqlite] * Structured Output with OpenAI functions\n[/docs/modules/chains/popular/structured_output] * Summarization\n[/docs/modules/chains/popular/summarize] * Additional\n[/docs/modules/chains/additional/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":24,"to":44}}}}],["584",{"pageContent":"* Memory [/docs/modules/memory/] * Agents [/docs/modules/agents/] * Callbacks\n[/docs/modules/callbacks/] * Modules [/docs/modules/] * Guides [/docs/guides] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Modules [/docs/modules/] * Chains\n[/docs/modules/chains/] * Popular [/docs/modules/chains/popular/] *\nConversational Retrieval QA CONVERSATIONAL RETRIEVAL QA The\nConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat\nhistory component. It first combines the chat history (either explicitly passed\nin or retrieved from the provided memory) and the question into a standalone\nquestion, then looks up relevant documents from the retriever, and finally\npasses those documents and the question to a question","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":44,"to":70}}}}],["585",{"pageContent":"retrieved from the provided memory) and the question into a standalone question,\nthen looks up relevant documents from the retriever, and finally passes those\ndocuments and the question to a question answering chain to return a response.\nTo create one, you will need a retriever. In the below example, we will create\none from a vector store, which can be created from embeddings. import {\nChatOpenAI } from \"langchain/chat_models/openai\"; import {\nConversationalRetrievalQAChain } from \"langchain/chains\"; import { HNSWLib }\nfrom \"langchain/vectorstores/hnswlib\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { RecursiveCharacterTextSplitter } from\n\"langchain/text_splitter\"; import { BufferMemory } from \"langchain/memory\";\nimport * as fs from \"fs\"; export const run = async () => { /* Initialize the LLM\nto use to answer the question */ const model = new ChatOpenAI({}); /* Load in\nthe file we want to do question answering over */ const text =","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":70,"to":89}}}}],["586",{"pageContent":"const run = async () => { /* Initialize the LLM to use to answer the question */\nconst model = new ChatOpenAI({}); /* Load in the file we want to do question\nanswering over */ const text = fs.readFileSync(\"state_of_the_union.txt\",\n\"utf8\"); /* Split the text into chunks */ const textSplitter = new\nRecursiveCharacterTextSplitter({ chunkSize: 1000 }); const docs = await\ntextSplitter.createDocuments([text]); /* Create the vectorstore */ const\nvectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings()); /*\nCreate the chain */ const chain = ConversationalRetrievalQAChain.fromLLM( model,\nvectorStore.asRetriever(), { memory: new BufferMemory({ memoryKey:\n\"chat_history\", // Must be set to \"chat_history\" }), } ); /* Ask it a question\n*/ const question = \"What did the president say about Justice Breyer?\"; const\nres = await chain.call({ question }); console.log(res); /* Ask it a follow up\nquestion */ const","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":89,"to":114}}}}],["587",{"pageContent":"it a question */ const question = \"What did the president say about Justice\nBreyer?\"; const res = await chain.call({ question }); console.log(res); /* Ask\nit a follow up question */ const followUpRes = await chain.call({ question: \"Was\nthat nice?\", }); console.log(followUpRes); }; API REFERENCE: * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * ConversationalRetrievalQAChain\n[/docs/api/chains/classes/ConversationalRetrievalQAChain] from langchain/chains\n* HNSWLib [/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter * BufferMemory [/docs/api/memory/classes/BufferMemory]\nfrom langchain/memory In the above code snippet, the fromLLM method of the","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":114,"to":137}}}}],["588",{"pageContent":"from langchain/text_splitter * BufferMemory\n[/docs/api/memory/classes/BufferMemory] from langchain/memory In the above code\nsnippet, the fromLLM method of the ConversationalRetrievalQAChain class has the\nfollowing signature: static fromLLM( llm: BaseLanguageModel, retriever:\nBaseRetriever, options?: { questionGeneratorChainOptions?: { llm?:\nBaseLanguageModel; template?: string; }; qaChainOptions?: QAChainParams;\nreturnSourceDocuments?: boolean; } ): ConversationalRetrievalQAChain Here's an\nexplanation of each of the attributes of the options object: *\nquestionGeneratorChainOptions: An object that allows you to pass a custom\ntemplate and LLM to the underlying question generation chain. * If the template\nis provided, the ConversationalRetrievalQAChain will use this template to\ngenerate a question from the conversation context instead of using the question\nprovided in the question parameter. * Passing in a separate LLM (llm) here","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":137,"to":164}}}}],["589",{"pageContent":"will use this template to generate a question from the conversation context\ninstead of using the question provided in the question parameter. * Passing in a\nseparate LLM (llm) here allows you to use a cheaper/faster model to create the\ncondensed question while using a more powerful model for the final response, and\ncan reduce unnecessary latency. * qaChainOptions: Options that allow you to\ncustomize the specific QA chain used in the final step. The default is the\nStuffDocumentsChain [/docs/modules/chains/document/stuff], but you can customize\nwhich chain is used by passing in a type parameter. Passing specific options\nhere is completely optional, but can be useful if you want to customize the way\nthe response is presented to the end user, or if you have too many documents for\nthe default StuffDocumentsChain. You can see the API reference of the usable\nfields here [/docs/api/chains/types/QAChainParams]. In case you want to make\nchat_history available to the","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":164,"to":173}}}}],["590",{"pageContent":"for the default StuffDocumentsChain. You can see the API reference of the usable\nfields here [/docs/api/chains/types/QAChainParams]. In case you want to make\nchat_history available to the final answering qaChain, which ultimately answers\nthe user question, you HAVE to pass a custom qaTemplate with chat_history as\ninput, as it is not present in the default Template, which only gets passed\ncontext documents and generated question. * returnSourceDocuments: A boolean\nvalue that indicates whether the ConversationalRetrievalQAChain should return\nthe source documents that were used to retrieve the answer. If set to true, the\ndocuments will be included in the result returned by the call() method. This can\nbe useful if you want to allow the user to see the sources used to generate the\nanswer. If not set, the default value will be false. * If you are using this\noption and passing in a memory instance, set inputKey and outputKey on the\nmemory instance to the same","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":173,"to":182}}}}],["591",{"pageContent":"the answer. If not set, the default value will be false. * If you are using this\noption and passing in a memory instance, set inputKey and outputKey on the\nmemory instance to the same values as the chain input and final conversational\nchain output. These default to \"question\" and \"text\" respectively, and specify\nthe values that the memory should store. BUILT-IN MEMORY Here's a customization\nexample using a faster LLM to generate questions and a slower, more\ncomprehensive LLM for the final answer. It uses a built-in memory object and\nreturns the referenced source documents. Because we have returnSourceDocuments\nset and are thus returning multiple values from the chain, we must set inputKey\nand outputKey on the memory instance to let it know which values to store.\nimport { ChatOpenAI } from \"langchain/chat_models/openai\"; import {\nConversationalRetrievalQAChain } from \"langchain/chains\"; import { HNSWLib }\nfrom \"langchain/vectorstores/hnswlib\"; import {","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":182,"to":199}}}}],["592",{"pageContent":"{ ChatOpenAI } from \"langchain/chat_models/openai\"; import {\nConversationalRetrievalQAChain } from \"langchain/chains\"; import { HNSWLib }\nfrom \"langchain/vectorstores/hnswlib\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { RecursiveCharacterTextSplitter } from\n\"langchain/text_splitter\"; import { BufferMemory } from \"langchain/memory\";\nimport * as fs from \"fs\"; export const run = async () => { const text =\nfs.readFileSync(\"state_of_the_union.txt\", \"utf8\"); const textSplitter = new\nRecursiveCharacterTextSplitter({ chunkSize: 1000 }); const docs = await\ntextSplitter.createDocuments([text]); const vectorStore = await\nHNSWLib.fromDocuments(docs, new OpenAIEmbeddings()); const fasterModel = new\nChatOpenAI({ modelName: \"gpt-3.5-turbo\", }); const slowerModel = new\nChatOpenAI({ modelName: \"gpt-4\", }); const chain =\nConversationalRetrievalQAChain.fromLLM( slowerModel, vectorStore.asRetriever(),\n{ returnSourceDocuments:","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":199,"to":223}}}}],["593",{"pageContent":"slowerModel = new ChatOpenAI({ modelName: \"gpt-4\", }); const chain =\nConversationalRetrievalQAChain.fromLLM( slowerModel, vectorStore.asRetriever(),\n{ returnSourceDocuments: true, memory: new BufferMemory({ memoryKey:\n\"chat_history\", inputKey: \"question\", // The key for the input to the chain\noutputKey: \"text\", // The key for the final conversational output of the chain\nreturnMessages: true, // If using with a chat model (e.g. gpt-3.5 or gpt-4) }),\nquestionGeneratorChainOptions: { llm: fasterModel, }, } ); /* Ask it a question\n*/ const question = \"What did the president say about Justice Breyer?\"; const\nres = await chain.call({ question }); console.log(res); const followUpRes =\nawait chain.call({ question: \"Was that nice?\" }); console.log(followUpRes); };\nAPI REFERENCE: * ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI]\nfrom langchain/chat_models/openai *","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":223,"to":257}}}}],["594",{"pageContent":"chain.call({ question: \"Was that nice?\" }); console.log(followUpRes); }; API\nREFERENCE: * ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * ConversationalRetrievalQAChain\n[/docs/api/chains/classes/ConversationalRetrievalQAChain] from langchain/chains\n* HNSWLib [/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter * BufferMemory [/docs/api/memory/classes/BufferMemory]\nfrom langchain/memory STREAMING You can also use the above concept of using two\ndifferent LLMs to stream only the final response from the chain, and not output\nfrom the intermediate standalone question generation step. Here's an example:\nimport { ChatOpenAI } from","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":257,"to":279}}}}],["595",{"pageContent":"of using two different LLMs to stream only the final response from the chain,\nand not output from the intermediate standalone question generation step. Here's\nan example: import { ChatOpenAI } from \"langchain/chat_models/openai\"; import {\nConversationalRetrievalQAChain } from \"langchain/chains\"; import { HNSWLib }\nfrom \"langchain/vectorstores/hnswlib\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { RecursiveCharacterTextSplitter } from\n\"langchain/text_splitter\"; import { BufferMemory } from \"langchain/memory\";\nimport * as fs from \"fs\"; export const run = async () => { const text =\nfs.readFileSync(\"state_of_the_union.txt\", \"utf8\"); const textSplitter = new\nRecursiveCharacterTextSplitter({ chunkSize: 1000 }); const docs = await\ntextSplitter.createDocuments([text]); const vectorStore = await\nHNSWLib.fromDocuments(docs, new OpenAIEmbeddings()); let streamedResponse = \"\";\nconst streamingModel = new ChatOpenAI({ streaming: true, callbacks: [","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":279,"to":299}}}}],["596",{"pageContent":"const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\nlet streamedResponse = \"\"; const streamingModel = new ChatOpenAI({ streaming:\ntrue, callbacks: [ { handleLLMNewToken(token) { streamedResponse += token; }, },\n], }); const nonStreamingModel = new ChatOpenAI({}); const chain =\nConversationalRetrievalQAChain.fromLLM( streamingModel,\nvectorStore.asRetriever(), { returnSourceDocuments: true, memory: new\nBufferMemory({ memoryKey: \"chat_history\", inputKey: \"question\", // The key for\nthe input to the chain outputKey: \"text\", // The key for the final\nconversational output of the chain returnMessages: true, // If using with a chat\nmodel }), questionGeneratorChainOptions: { llm: nonStreamingModel, }, } ); /*\nAsk it a question */ const question = \"What did the president say about Justice\nBreyer?\"; const res = await","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":299,"to":330}}}}],["597",{"pageContent":"{ llm: nonStreamingModel, }, } ); /* Ask it a question */ const question = \"What\ndid the president say about Justice Breyer?\"; const res = await chain.call({\nquestion }); console.log({ streamedResponse }); /* { streamedResponse:\n'President Biden thanked Justice Breyer for his service, and honored him as an\nArmy veteran, Constitutional scholar and retiring Justice of the United States\nSupreme Court.' } */ }; API REFERENCE: * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * ConversationalRetrievalQAChain\n[/docs/api/chains/classes/ConversationalRetrievalQAChain] from langchain/chains\n* HNSWLib [/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":330,"to":355}}}}],["598",{"pageContent":"[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter * BufferMemory [/docs/api/memory/classes/BufferMemory]\nfrom langchain/memory EXTERNALLY-MANAGED MEMORY For this chain, if you'd like to\nformat the chat history in a custom way (or pass in chat messages directly for\nconvenience), you can also pass the chat history in explicitly by omitting the\nmemory option and supplying a chat_history string or array of HumanMessages\n[/docs/api/schema/classes/HumanMessage] and AIMessages\n[/docs/api/schema/classes/AIMessage] directly into the chain.call method: import\n{ OpenAI } from \"langchain/llms/openai\"; import { ConversationalRetrievalQAChain\n} from \"langchain/chains\"; import { HNSWLib } from\n\"langchain/vectorstores/hnswlib\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { RecursiveCharacterTextSplitter }","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":355,"to":371}}}}],["599",{"pageContent":"} from \"langchain/chains\"; import { HNSWLib } from\n\"langchain/vectorstores/hnswlib\"; import { OpenAIEmbeddings } from\n\"langchain/embeddings/openai\"; import { RecursiveCharacterTextSplitter } from\n\"langchain/text_splitter\"; import * as fs from \"fs\"; /* Initialize the LLM to\nuse to answer the question */ const model = new OpenAI({}); /* Load in the file\nwe want to do question answering over */ const text =\nfs.readFileSync(\"state_of_the_union.txt\", \"utf8\"); /* Split the text into chunks\n*/ const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]); /* Create the\nvectorstore */ const vectorStore = await HNSWLib.fromDocuments(docs, new\nOpenAIEmbeddings()); /* Create the chain */ const chain =\nConversationalRetrievalQAChain.fromLLM( model, vectorStore.asRetriever() ); /*\nAsk it a question */ const question = \"What did the president say about Justice\nBreyer?\"; /* Can be a string or an array of chat messages","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":371,"to":393}}}}],["600",{"pageContent":"model, vectorStore.asRetriever() ); /* Ask it a question */ const question =\n\"What did the president say about Justice Breyer?\"; /* Can be a string or an\narray of chat messages */ const res = await chain.call({ question, chat_history:\n\"\" }); console.log(res); /* Ask it a follow up question */ const chatHistory =\n`${question}\\n${res.text}`; const followUpRes = await chain.call({ question:\n\"Was that nice?\", chat_history: chatHistory, }); console.log(followUpRes); API\nREFERENCE: * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai * ConversationalRetrievalQAChain\n[/docs/api/chains/classes/ConversationalRetrievalQAChain] from langchain/chains\n* HNSWLib [/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":393,"to":418}}}}],["601",{"pageContent":"[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * RecursiveCharacterTextSplitter\n[/docs/api/text_splitter/classes/RecursiveCharacterTextSplitter] from\nlangchain/text_splitter PROMPT CUSTOMIZATION If you want to further change the\nchain's behavior, you can change the prompts for both the underlying question\ngeneration chain and the QA chain. One case where you might want to do this is\nto improve the chain's ability to answer meta questions about the chat history.\nBy default, the only input to the QA chain is the standalone question generated\nfrom the question generation chain. This poses a challenge when asking meta\nquestions about information in previous interactions from the chat history. For\nexample, if you introduce a friend Bob and mention his age as 28, the chain is\nunable to provide his age upon asking a question like \"How old is Bob?\". This\nlimitation occurs because the bot searches for Bob in the vector store, rather","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":418,"to":432}}}}],["602",{"pageContent":"and mention his age as 28, the chain is unable to provide his age upon asking a\nquestion like \"How old is Bob?\". This limitation occurs because the bot searches\nfor Bob in the vector store, rather than considering the message history. You\ncan pass an alternative prompt for the question generation chain that also\nreturns parts of the chat history relevant to the answer, allowing the QA chain\nto answer meta questions with the additional context: import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; import { ConversationalRetrievalQAChain } from\n\"langchain/chains\"; import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\"; import {\nBufferMemory } from \"langchain/memory\"; const\nCUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT = `Given the following conversation and a\nfollow up question, return the conversation history excerpt that includes any\nrelevant context to the question if it exists and rephrase the follow up\nquestion to","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":432,"to":445}}}}],["603",{"pageContent":"the following conversation and a follow up question, return the conversation\nhistory excerpt that includes any relevant context to the question if it exists\nand rephrase the follow up question to be a standalone question. Chat History:\n{chat_history} Follow Up Input: {question} Your answer should follow the\nfollowing format: \\`\\`\\` Use the following pieces of context to answer the users\nquestion. If you don't know the answer, just say that you don't know, don't try\nto make up an answer. ---------------- Standalone question: \\`\\`\\` Your\nanswer:`; const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\",\ntemperature: 0, }); const vectorStore = await HNSWLib.fromTexts( [ \"Mitochondria\nare the powerhouse of the cell\", \"Foo is red\", \"Bar is red\", \"Buildings are made\nout of brick\", \"Mitochondria are made of lipids\", ], [{ id: 2 }, { id: 1 }, {\nid: 3 }, { id: 4 }, { id: 5 }], new","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":445,"to":473}}}}],["604",{"pageContent":"of the cell\", \"Foo is red\", \"Bar is red\", \"Buildings are made out of brick\",\n\"Mitochondria are made of lipids\", ], [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4\n}, { id: 5 }], new OpenAIEmbeddings() ); const chain =\nConversationalRetrievalQAChain.fromLLM( model, vectorStore.asRetriever(), {\nmemory: new BufferMemory({ memoryKey: \"chat_history\", returnMessages: true, }),\nquestionGeneratorChainOptions: { template:\nCUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT, }, } ); const res = await chain.call({\nquestion: \"I have a friend called Bob. He's 28 years old. He'd like to know what\nthe powerhouse of the cell is?\", }); console.log(res); /* { text: \"The\npowerhouse of the cell is the mitochondria.\" } */ const res2 = await\nchain.call({ question: \"How old is Bob?\", }); console.log(res2); // Bob is 28\nyears old. /* { text: \"Bob is 28 years old.\" } */ API REFERENCE: * ChatOpenAI","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":473,"to":526}}}}],["605",{"pageContent":"} */ const res2 = await chain.call({ question: \"How old is Bob?\", });\nconsole.log(res2); // Bob is 28 years old. /* { text: \"Bob is 28 years old.\" }\n*/ API REFERENCE: * ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI]\nfrom langchain/chat_models/openai * ConversationalRetrievalQAChain\n[/docs/api/chains/classes/ConversationalRetrievalQAChain] from langchain/chains\n* HNSWLib [/docs/api/vectorstores_hnswlib/classes/HNSWLib] from\nlangchain/vectorstores/hnswlib * OpenAIEmbeddings\n[/docs/api/embeddings_openai/classes/OpenAIEmbeddings] from\nlangchain/embeddings/openai * BufferMemory\n[/docs/api/memory/classes/BufferMemory] from langchain/memory Keep in mind that\nadding more context to the prompt in this way may distract the LLM from other\nrelevant retrieved information. Previous Retrieval QA\n[/docs/modules/chains/popular/vector_db_qa] Next SQL\n[/docs/modules/chains/popular/sqlite] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":526,"to":562}}}}],["606",{"pageContent":"information. Previous Retrieval QA [/docs/modules/chains/popular/vector_db_qa]\nNext SQL [/docs/modules/chains/popular/sqlite] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/modules/chains/popular/chat_vector_db","title":"Conversational Retrieval QA | 🦜️🔗 Langchain","description":"The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.","language":"en","loc":{"lines":{"from":562,"to":582}}}}],["607",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Use cases [/docs/use_cases] * QA and Chat over Documents\n[/docs/use_cases/question_answering/] * Tabular Question Answering\n[/docs/use_cases/tabular] * Interacting with APIs [/docs/use_cases/api] *\nSummarization [/docs/use_cases/summarization] * Agent Simulations\n[/docs/use_cases/agent_simulations/] * Autonomous Agents\n[/docs/use_cases/autonomous_agents/] * Chatbots [/docs/use_cases/chatbots] *\nExtraction [/docs/use_cases/extraction] * / * Use cases [/docs/use_cases] *\nTabular Question Answering TABULAR QUESTION ANSWERING Lots of data and\ninformation is stored in tabular data, whether it be csvs, excel sheets, or SQL\ntables. This page covers all resources available in LangChain for","metadata":{"source":"https://js.langchain.com/docs/use_cases/tabular","title":"Tabular Question Answering | 🦜️🔗 Langchain","description":"Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables.","language":"en","loc":{"lines":{"from":1,"to":28}}}}],["608",{"pageContent":"QUESTION ANSWERING Lots of data and information is stored in tabular data,\nwhether it be csvs, excel sheets, or SQL tables. This page covers all resources\navailable in LangChain for working with data in this format. CHAINS If you are\njust getting started, and you have relatively small/simple tabular data, you\nshould get started with chains. Chains are a sequence of predetermined steps, so\nthey are good to get started with as they give you more control and let you\nunderstand what is happening better. * SQL Database Chain\n[/docs/modules/chains/popular/sqlite] AGENTS Agents are more complex, and\ninvolve multiple queries to the LLM to understand what to do. The downside of\nagents are that you have less control. The upside is that they are more\npowerful, which allows you to use them on larger databases and more complex\nschemas. * SQL Agent [/docs/modules/agents/toolkits/sql] Previous Use local LLMs\n[/docs/use_cases/question_answering/local_retrieval_qa] Next Interacting with","metadata":{"source":"https://js.langchain.com/docs/use_cases/tabular","title":"Tabular Question Answering | 🦜️🔗 Langchain","description":"Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables.","language":"en","loc":{"lines":{"from":28,"to":55}}}}],["609",{"pageContent":"on larger databases and more complex schemas. * SQL Agent\n[/docs/modules/agents/toolkits/sql] Previous Use local LLMs\n[/docs/use_cases/question_answering/local_retrieval_qa] Next Interacting with\nAPIs [/docs/use_cases/api] Community * Discord [https://discord.gg/cU2adEyC7w] *\nTwitter [https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/use_cases/tabular","title":"Tabular Question Answering | 🦜️🔗 Langchain","description":"Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables.","language":"en","loc":{"lines":{"from":55,"to":78}}}}],["610",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/guides/","title":"Guides | 🦜️🔗 Langchain","description":"Design guides for key parts of the development process","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["611",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Deployment [/docs/guides/deployment/]\n* Evaluation [/docs/guides/evaluation/] * Fallbacks [/docs/guides/fallbacks] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Guides GUIDES Design guides for key parts of\nthe development process 🗃️ DEPLOYMENT 1 items [/docs/guides/deployment/] 🗃️\nEVALUATION 4 items [/docs/guides/evaluation/] 📄️ FALLBACKS When working with\nlanguage models, you may often encounter issues from the underlying APIs,","metadata":{"source":"https://js.langchain.com/docs/guides/","title":"Guides | 🦜️🔗 Langchain","description":"Design guides for key parts of the development process","language":"en","loc":{"lines":{"from":25,"to":69}}}}],["612",{"pageContent":"items [/docs/guides/deployment/] 🗃️ EVALUATION 4 items\n[/docs/guides/evaluation/] 📄️ FALLBACKS When working with language models, you\nmay often encounter issues from the underlying APIs, e.g. rate limits or\ndowntime. [/docs/guides/fallbacks] Previous Modules [/docs/modules/] Next\nDeployment [/docs/guides/deployment/] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/guides/","title":"Guides | 🦜️🔗 Langchain","description":"Design guides for key parts of the development process","language":"en","loc":{"lines":{"from":69,"to":104}}}}],["613",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/guides/deployment/","title":"Deployment | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["614",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Deployment [/docs/guides/deployment/]\n* Next.js [/docs/guides/deployment/nextjs] * Evaluation\n[/docs/guides/evaluation/] * Fallbacks [/docs/guides/fallbacks] * Ecosystem\n[/docs/ecosystem] * Additional resources [/docs/additional_resources] *\nCommunity navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Guides [/docs/guides] * Deployment DEPLOYMENT\n📄️ NEXT.JS Open in GitHub Codespaces [/docs/guides/deployment/nextjs] Previous\nGuides [/docs/guides] Next Next.js [/docs/guides/deployment/nextjs] Community *\nDiscord","metadata":{"source":"https://js.langchain.com/docs/guides/deployment/","title":"Deployment | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":25,"to":65}}}}],["615",{"pageContent":"* Deployment DEPLOYMENT 📄️ NEXT.JS Open in GitHub Codespaces\n[/docs/guides/deployment/nextjs] Previous Guides [/docs/guides] Next Next.js\n[/docs/guides/deployment/nextjs] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/guides/deployment/","title":"Deployment | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":65,"to":94}}}}],["616",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/guides/evaluation/","title":"Evaluation | 🦜️🔗 Langchain","description":"Building applications with language models involves many moving parts. One of the most critical components is ensuring that the outcomes produced by your models are reliable and useful across a broad array of inputs, and that they work well with your application's other software components. Ensuring reliability usually boils down to some combination of application design, testing & evaluation, and runtime checks.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["617",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Deployment [/docs/guides/deployment/]\n* Evaluation [/docs/guides/evaluation/] * String Evaluators\n[/docs/guides/evaluation/string/] * Comparison Evaluators\n[/docs/guides/evaluation/comparison/] * Trajectory Evaluators\n[/docs/guides/evaluation/trajectory/] * Examples\n[/docs/guides/evaluation/examples/] * Fallbacks [/docs/guides/fallbacks] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Guides [/docs/guides] * Evaluation On this","metadata":{"source":"https://js.langchain.com/docs/guides/evaluation/","title":"Evaluation | 🦜️🔗 Langchain","description":"Building applications with language models involves many moving parts. One of the most critical components is ensuring that the outcomes produced by your models are reliable and useful across a broad array of inputs, and that they work well with your application's other software components. Ensuring reliability usually boils down to some combination of application design, testing & evaluation, and runtime checks.","language":"en","loc":{"lines":{"from":25,"to":52}}}}],["618",{"pageContent":"* API reference [/docs/api/] * / * Guides [/docs/guides] * Evaluation On this\npage EVALUATION Building applications with language models involves many moving\nparts. One of the most critical components is ensuring that the outcomes\nproduced by your models are reliable and useful across a broad array of inputs,\nand that they work well with your application's other software components.\nEnsuring reliability usually boils down to some combination of application\ndesign, testing & evaluation, and runtime checks. The guides in this section\nreview the APIs and functionality LangChain provides to help you better evaluate\nyour applications. Evaluation and testing are both critical when thinking about\ndeploying LLM applications, since production environments require repeatable and\nuseful outcomes. LangChain offers various types of evaluators to help you\nmeasure performance and integrity on diverse data, and we hope to encourage the\ncommunity to create and share other useful evaluators","metadata":{"source":"https://js.langchain.com/docs/guides/evaluation/","title":"Evaluation | 🦜️🔗 Langchain","description":"Building applications with language models involves many moving parts. One of the most critical components is ensuring that the outcomes produced by your models are reliable and useful across a broad array of inputs, and that they work well with your application's other software components. Ensuring reliability usually boils down to some combination of application design, testing & evaluation, and runtime checks.","language":"en","loc":{"lines":{"from":52,"to":73}}}}],["619",{"pageContent":"offers various types of evaluators to help you measure performance and integrity\non diverse data, and we hope to encourage the community to create and share\nother useful evaluators so everyone can improve. These docs will introduce the\nevaluator types, how to use them, and provide some examples of their use in\nreal-world scenarios. Each evaluator type in LangChain comes with ready-to-use\nimplementations and an extensible API that allows for customization according to\nyour unique requirements. Here are some of the types of evaluators we offer:\nThese evaluators can be used across various scenarios and can be applied to\ndifferent chain and LLM implementations in the LangChain library. REFERENCE DOCS\n🗃️ STRING EVALUATORS 2 items [/docs/guides/evaluation/string/] 🗃️ COMPARISON\nEVALUATORS 2 items [/docs/guides/evaluation/comparison/] 🗃️ TRAJECTORY\nEVALUATORS 1 items [/docs/guides/evaluation/trajectory/] 🗃️ EXAMPLES 1","metadata":{"source":"https://js.langchain.com/docs/guides/evaluation/","title":"Evaluation | 🦜️🔗 Langchain","description":"Building applications with language models involves many moving parts. One of the most critical components is ensuring that the outcomes produced by your models are reliable and useful across a broad array of inputs, and that they work well with your application's other software components. Ensuring reliability usually boils down to some combination of application design, testing & evaluation, and runtime checks.","language":"en","loc":{"lines":{"from":73,"to":110}}}}],["620",{"pageContent":"COMPARISON EVALUATORS 2 items [/docs/guides/evaluation/comparison/] 🗃️\nTRAJECTORY EVALUATORS 1 items [/docs/guides/evaluation/trajectory/] 🗃️ EXAMPLES\n1 items [/docs/guides/evaluation/examples/] Previous Next.js\n[/docs/guides/deployment/nextjs] Next String Evaluators\n[/docs/guides/evaluation/string/] * Reference Docs Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/guides/evaluation/","title":"Evaluation | 🦜️🔗 Langchain","description":"Building applications with language models involves many moving parts. One of the most critical components is ensuring that the outcomes produced by your models are reliable and useful across a broad array of inputs, and that they work well with your application's other software components. Ensuring reliability usually boils down to some combination of application design, testing & evaluation, and runtime checks.","language":"en","loc":{"lines":{"from":110,"to":149}}}}],["621",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["622",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Deployment [/docs/guides/deployment/]\n* Evaluation [/docs/guides/evaluation/] * Fallbacks [/docs/guides/fallbacks] *\nEcosystem [/docs/ecosystem] * Additional resources [/docs/additional_resources]\n* Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Guides [/docs/guides] * Fallbacks On this\npage FALLBACKS When working with language models, you may often encounter issues\nfrom the underlying APIs, e.g. rate limits or downtime. Therefore, as you move\nyour LLM applications into production it becomes more and more important to have","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":25,"to":54}}}}],["623",{"pageContent":"models, you may often encounter issues from the underlying APIs, e.g. rate\nlimits or downtime. Therefore, as you move your LLM applications into production\nit becomes more and more important to have contingencies for errors. That's why\nwe've introduced the concept of fallbacks. Crucially, fallbacks can be applied\nnot only on the LLM level but on the whole runnable level. This is important\nbecause often times different models require different prompts. So if your call\nto OpenAI fails, you don't just want to send the same prompt to Anthropic - you\nprobably want want to use e.g. a different prompt template. HANDLING LLM API\nERRORS This is maybe the most common use case for fallbacks. A request to an LLM\nAPI can fail for a variety of reasons - the API could be down, you could have\nhit a rate limit, or any number of things. IMPORTANT: By default, many of\nLangChain's LLM wrappers catch errors and retry. You will most likely want to\nturn those off when working with fallbacks. Otherwise","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":54,"to":69}}}}],["624",{"pageContent":"limit, or any number of things. IMPORTANT: By default, many of LangChain's LLM\nwrappers catch errors and retry. You will most likely want to turn those off\nwhen working with fallbacks. Otherwise the first wrapper will keep on retrying\nrather than failing. import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\"; // Use a fake\nmodel name that will always throw an error const fakeOpenAIModel = new\nChatOpenAI({ modelName: \"potato!\", maxRetries: 0, }); const anthropicModel = new\nChatAnthropic({}); const modelWithFallback = fakeOpenAIModel.withFallbacks({\nfallbacks: [anthropicModel], }); const result = await\nmodelWithFallback.invoke(\"What is your name?\"); console.log(result); /*\nAIMessage { content: ' My name is Claude. I was created by Anthropic.',\nadditional_kwargs: {} } */ API REFERENCE: * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai *","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":69,"to":106}}}}],["625",{"pageContent":"name is Claude. I was created by Anthropic.', additional_kwargs: {} } */ API\nREFERENCE: * ChatOpenAI [/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * ChatAnthropic\n[/docs/api/chat_models_anthropic/classes/ChatAnthropic] from\nlangchain/chat_models/anthropic FALLBACKS FOR RUNNABLESEQUENCES We can also\ncreate fallbacks for sequences, that are sequences themselves. Here we do that\nwith two different models: ChatOpenAI and then normal OpenAI (which does not use\na chat model). Because OpenAI is NOT a chat model, you likely want a different\nprompt. import { ChatOpenAI } from \"langchain/chat_models/openai\"; import {\nOpenAI } from \"langchain/llms/openai\"; import { StringOutputParser } from\n\"langchain/schema/output_parser\"; import { ChatPromptTemplate, PromptTemplate }\nfrom \"langchain/prompts\"; const chatPrompt = ChatPromptTemplate.fromMessages<{\nanimal: string }>([ [ \"system\", \"You're a nice assistant who always includes a","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":106,"to":133}}}}],["626",{"pageContent":"PromptTemplate } from \"langchain/prompts\"; const chatPrompt =\nChatPromptTemplate.fromMessages<{ animal: string }>([ [ \"system\", \"You're a nice\nassistant who always includes a compliment in your response\", ], [\"human\", \"Why\ndid the {animal} cross the road?\"], ]); // Use a fake model name that will\nalways throw an error const fakeOpenAIChatModel = new ChatOpenAI({ modelName:\n\"potato!\", maxRetries: 0, }); const prompt =\nPromptTemplate.fromTemplate(`Instructions: You should always include a\ncompliment in your response. Question: Why did the {animal} cross the road?\nAnswer:`); const openAILLM = new OpenAI({}); const outputParser = new\nStringOutputParser(); const badChain =\nchatPrompt.pipe(fakeOpenAIChatModel).pipe(outputParser); const goodChain =\nprompt.pipe(openAILLM).pipe(outputParser); const chain =\nbadChain.withFallbacks({ fallbacks: [goodChain], }); const result = await\nchain.invoke({ animal: \"dragon\", }); console.log(result); /* I don't know,","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":133,"to":175}}}}],["627",{"pageContent":"chain = badChain.withFallbacks({ fallbacks: [goodChain], }); const result =\nawait chain.invoke({ animal: \"dragon\", }); console.log(result); /* I don't know,\nbut I'm sure it was an impressive sight. You must have a great imagination to\ncome up with such an interesting question! */ API REFERENCE: * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * OpenAI [/docs/api/llms_openai/classes/OpenAI]\nfrom langchain/llms/openai * StringOutputParser\n[/docs/api/schema_output_parser/classes/StringOutputParser] from\nlangchain/schema/output_parser * ChatPromptTemplate\n[/docs/api/prompts/classes/ChatPromptTemplate] from langchain/prompts *\nPromptTemplate [/docs/api/prompts/classes/PromptTemplate] from langchain/prompts\nHANDLING LONG INPUTS One of the big limiting factors of LLMs in their context\nwindow. Sometimes you can count and track the length of prompts before sending\nthem to an LLM, but in situations where that is","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":175,"to":204}}}}],["628",{"pageContent":"LONG INPUTS One of the big limiting factors of LLMs in their context window.\nSometimes you can count and track the length of prompts before sending them to\nan LLM, but in situations where that is hard/complicated you can fallback to a\nmodel with longer context length. import { ChatOpenAI } from\n\"langchain/chat_models/openai\"; // Use a model with a shorter context window\nconst shorterLlm = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\", maxRetries: 0,\n}); const longerLlm = new ChatOpenAI({ modelName: \"gpt-3.5-turbo-16k\", }); const\nmodelWithFallback = shorterLlm.withFallbacks({ fallbacks: [longerLlm], }); const\ninput = `What is the next number: ${\"one, two, \".repeat(3000)}`; try { await\nshorterLlm.invoke(input); } catch (e) { // Length error console.log(e); } const\nresult = await modelWithFallback.invoke(input); console.log(result); /*\nAIMessage { content: 'The next number is one.', name: undefined,\nadditional_kwargs: { function_call: undefined }","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":204,"to":242}}}}],["629",{"pageContent":"= await modelWithFallback.invoke(input); console.log(result); /* AIMessage {\ncontent: 'The next number is one.', name: undefined, additional_kwargs: {\nfunction_call: undefined } } */ API REFERENCE: * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai FALLBACK TO A BETTER MODEL Often times we ask\nmodels to output format in a specific format (like JSON). Models like GPT-3.5\ncan do this okay, but sometimes struggle. This naturally points to fallbacks -\nwe can try with a faster and cheaper model, but then if parsing fails we can use\nGPT-4. import { z } from \"zod\"; import { OpenAI } from \"langchain/llms/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\"; import {\nPromptTemplate } from \"langchain/prompts\"; import { StructuredOutputParser }\nfrom \"langchain/output_parsers\"; const prompt = PromptTemplate.fromTemplate(\n`Return a JSON object containing the following value wrapped in an \"input\" key.\nDo not","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":242,"to":275}}}}],["630",{"pageContent":"{ StructuredOutputParser } from \"langchain/output_parsers\"; const prompt =\nPromptTemplate.fromTemplate( `Return a JSON object containing the following\nvalue wrapped in an \"input\" key. Do not return anything else:\\n{input}` ); const\nbadModel = new OpenAI({ maxRetries: 0, modelName: \"text-ada-001\", }); const\nnormalModel = new ChatOpenAI({ modelName: \"gpt-4\", }); const outputParser =\nStructuredOutputParser.fromZodSchema( z.object({ input: z.string(), }) ); const\nbadChain = prompt.pipe(badModel).pipe(outputParser); const goodChain =\nprompt.pipe(normalModel).pipe(outputParser); try { const result = await\nbadChain.invoke({ input: \"testing0\", }); } catch (e) { console.log(e); /*\nOutputParserException [Error]: Failed to parse. Text: \" { \"name\" : \" Testing0 \",\n\"lastname\" : \" testing \", \"fullname\" : \" testing \", \"role\" : \" test \",\n\"telephone\" : \"+1-555-555-555 \", \"email\" : \" testing@gmail.com \", \"role\" : \"\ntest \", \"text\" : \" testing0 is different than","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":275,"to":309}}}}],["631",{"pageContent":"\", \"lastname\" : \" testing \", \"fullname\" : \" testing \", \"role\" : \" test \",\n\"telephone\" : \"+1-555-555-555 \", \"email\" : \" testing@gmail.com \", \"role\" : \"\ntest \", \"text\" : \" testing0 is different than testing \", \"role\" : \" test \",\n\"immediate_affected_version\" : \" 0.0.1 \", \"immediate_version\" : \" 1.0.0 \",\n\"leading_version\" : \" 1.0.0 \", \"version\" : \" 1.0.0 \", \"finger prick\" : \" no \",\n\"finger prick\" : \" s \", \"text\" : \" testing0 is different than testing \", \"role\"\n: \" test \", \"immediate_affected_version\" : \" 0.0.1 \", \"immediate_version\" : \"\n1.0.0 \", \"leading_version\" : \" 1.0.0 \", \"version\" : \" 1.0.0 \", \"finger prick\"\n:\". Error: SyntaxError: Unexpected end of JSON input */ } const chain =\nbadChain.withFallbacks({ fallbacks: [goodChain], }); const result = await\nchain.invoke({ input: \"testing\", }); console.log(result); /* { input: 'testing'\n} */ API REFERENCE: * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai * ChatOpenAI","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":309,"to":333}}}}],["632",{"pageContent":"input: \"testing\", }); console.log(result); /* { input: 'testing' } */ API\nREFERENCE: * OpenAI [/docs/api/llms_openai/classes/OpenAI] from\nlangchain/llms/openai * ChatOpenAI\n[/docs/api/chat_models_openai/classes/ChatOpenAI] from\nlangchain/chat_models/openai * PromptTemplate\n[/docs/api/prompts/classes/PromptTemplate] from langchain/prompts *\nStructuredOutputParser [/docs/api/output_parsers/classes/StructuredOutputParser]\nfrom langchain/output_parsers Previous Comparing Chain Outputs\n[/docs/guides/evaluation/examples/comparisons] Next Ecosystem [/docs/ecosystem]\n* Handling LLM API errors * Fallbacks for RunnableSequences * Handling long\ninputs * Fallback to a better model Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright ©","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":333,"to":375}}}}],["633",{"pageContent":"* Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/guides/fallbacks","title":"Fallbacks | 🦜️🔗 Langchain","description":"When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.","language":"en","loc":{"lines":{"from":375,"to":382}}}}],["634",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/additional_resources/","title":"Additional resources | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["635",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * LangChain Expression\nLanguage Cheatsheet [/docs/additional_resources/expression_language_cheatsheet]\n* Scrimba interactive guides [/docs/additional_resources/scrimba] * Gallery\n[https://github.com/kyrolabs/awesome-langchain] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Additional resources ADDITIONAL RESOURCES 📄️\nLANGCHAIN EXPRESSION LANGUAGE CHEATSHEET For a quick reference for LangChain\nExpression Language, check out the below","metadata":{"source":"https://js.langchain.com/docs/additional_resources/","title":"Additional resources | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":25,"to":53}}}}],["636",{"pageContent":"[/docs/api/] * / * Additional resources ADDITIONAL RESOURCES 📄️ LANGCHAIN\nEXPRESSION LANGUAGE CHEATSHEET For a quick reference for LangChain Expression\nLanguage, check out the below overview/cheatsheet made by @zhanghaili0610:\n[/docs/additional_resources/expression_language_cheatsheet] 📄️ SCRIMBA\nINTERACTIVE GUIDES Scrimba is a code-learning platform that allows you to\ninteractively edit and run [/docs/additional_resources/scrimba] 🔗 GALLERY\n[https://github.com/kyrolabs/awesome-langchain] Previous Unstructured\n[/docs/ecosystem/integrations/unstructured] Next LangChain Expression Language\nCheatsheet [/docs/additional_resources/expression_language_cheatsheet] Community\n* Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023","metadata":{"source":"https://js.langchain.com/docs/additional_resources/","title":"Additional resources | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":53,"to":97}}}}],["637",{"pageContent":"* Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/additional_resources/","title":"Additional resources | 🦜️🔗 Langchain","language":"en","loc":{"lines":{"from":97,"to":104}}}}],["638",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/additional_resources/expression_language_cheatsheet","title":"LangChain Expression Language Cheatsheet | 🦜️🔗 Langchain","description":"For a quick reference for LangChain Expression Language, check out the below overview/cheatsheet made by @zhanghaili0610:","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["639",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * LangChain Expression\nLanguage Cheatsheet [/docs/additional_resources/expression_language_cheatsheet]\n* Scrimba interactive guides [/docs/additional_resources/scrimba] * Gallery\n[https://github.com/kyrolabs/awesome-langchain] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Additional resources\n[/docs/additional_resources] * LangChain Expression Language Cheatsheet\nLANGCHAIN EXPRESSION LANGUAGE CHEATSHEET For a quick reference for LangChain","metadata":{"source":"https://js.langchain.com/docs/additional_resources/expression_language_cheatsheet","title":"LangChain Expression Language Cheatsheet | 🦜️🔗 Langchain","description":"For a quick reference for LangChain Expression Language, check out the below overview/cheatsheet made by @zhanghaili0610:","language":"en","loc":{"lines":{"from":25,"to":51}}}}],["640",{"pageContent":"[/docs/api/] * / * Additional resources [/docs/additional_resources] * LangChain\nExpression Language Cheatsheet LANGCHAIN EXPRESSION LANGUAGE CHEATSHEET For a\nquick reference for LangChain Expression Language, check out the below\noverview/cheatsheet made by @zhanghaili0610\n[https://twitter.com/zhanghaili0610]:\n[/assets/images/langchain-js-runnable-cheatsheet-17e8dcc53c6636dd6f3fad0fbdb65115.png]\nPrevious Additional resources [/docs/additional_resources] Next Scrimba\ninteractive guides [/docs/additional_resources/scrimba] Community * Discord\n[https://discord.gg/cU2adEyC7w] * Twitter [https://twitter.com/LangChainAI]\nGitHub * Python [https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/additional_resources/expression_language_cheatsheet","title":"LangChain Expression Language Cheatsheet | 🦜️🔗 Langchain","description":"For a quick reference for LangChain Expression Language, check out the below overview/cheatsheet made by @zhanghaili0610:","language":"en","loc":{"lines":{"from":51,"to":83}}}}],["641",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/ecosystem/integrations/unstructured","title":"Unstructured | 🦜️🔗 Langchain","description":"This page covers how to use Unstructured within LangChain.","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["642",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nIntegrations [/docs/ecosystem/integrations/] * Databerry\n[/docs/ecosystem/integrations/databerry] * Helicone\n[/docs/ecosystem/integrations/helicone] * LLMonitor\n[/docs/ecosystem/integrations/llmonitor] * Google MakerSuite\n[/docs/ecosystem/integrations/makersuite] * Unstructured\n[/docs/ecosystem/integrations/unstructured] * LangSmith\n[https://docs.smith.langchain.com] * Additional resources\n[/docs/additional_resources] * Community navigator [/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Ecosystem","metadata":{"source":"https://js.langchain.com/docs/ecosystem/integrations/unstructured","title":"Unstructured | 🦜️🔗 Langchain","description":"This page covers how to use Unstructured within LangChain.","language":"en","loc":{"lines":{"from":25,"to":49}}}}],["643",{"pageContent":"*\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Ecosystem [/docs/ecosystem] * Integrations\n[/docs/ecosystem/integrations/] * Unstructured On this page UNSTRUCTURED This\npage covers how to use Unstructured [https://unstructured.io] within LangChain.\nWHAT IS UNSTRUCTURED? Unstructured is an open source\n[https://github.com/Unstructured-IO/unstructured] Python package for extracting\ntext from raw documents for use in machine learning applications. Currently,\nUnstructured supports partitioning Word documents (in .doc or .docx format),\nPowerPoints (in .ppt or .pptx format), PDFs, HTML files, images, emails (in .eml\nor .msg format), epubs, markdown, and plain text files. unstructured is a Python\npackage and cannot be used directly with TS/JS, however Unstructured also\nmaintains a REST API [https://github.com/Unstructured-IO/unstructured-api] to\nsupport","metadata":{"source":"https://js.langchain.com/docs/ecosystem/integrations/unstructured","title":"Unstructured | 🦜️🔗 Langchain","description":"This page covers how to use Unstructured within LangChain.","language":"en","loc":{"lines":{"from":49,"to":74}}}}],["644",{"pageContent":"text files. unstructured is a Python package and cannot be used directly with\nTS/JS, however Unstructured also maintains a REST API\n[https://github.com/Unstructured-IO/unstructured-api] to support pre-processing\npipelines written in other programming languages. The endpoint for the hosted\nUnstructured API is https://api.unstructured.io/general/v0/general, or you can\nrun the service locally using the instructions found here\n[https://github.com/Unstructured-IO/unstructured-api#dizzy-instructions-for-using-the-docker-image].\nCheck out the Unstructured documentation page\n[https://unstructured-io.github.io/unstructured/] for instructions on how to\nobtain an API key. QUICK START You can use Unstructured in langchain with the\nfollowing code. Replace the filename with the file you would like to process. If\nyou are running the container locally, switch the url to\nhttp://127.0.0.1:8000/general/v0/general. Check out the API documentation page\n[https://api.unstructured.io/general/docs] for","metadata":{"source":"https://js.langchain.com/docs/ecosystem/integrations/unstructured","title":"Unstructured | 🦜️🔗 Langchain","description":"This page covers how to use Unstructured within LangChain.","language":"en","loc":{"lines":{"from":74,"to":90}}}}],["645",{"pageContent":"to process. If you are running the container locally, switch the url to\nhttp://127.0.0.1:8000/general/v0/general. Check out the API documentation page\n[https://api.unstructured.io/general/docs] for additional details. import {\nUnstructuredLoader } from \"langchain/document_loaders/fs/unstructured\"; const\noptions = { apiKey: \"MY_API_KEY\", }; const loader = new UnstructuredLoader(\n\"src/document_loaders/example_data/notion.md\", options ); const docs = await\nloader.load(); API REFERENCE: * UnstructuredLoader\n[/docs/api/document_loaders_fs_unstructured/classes/UnstructuredLoader] from\nlangchain/document_loaders/fs/unstructured DIRECTORIES You can also load all of\nthe files in the directory using UnstructuredDirectoryLoader, which inherits\nfrom DirectoryLoader\n[/docs/modules/data_connection/document_loaders/integrations/file_loaders/directory]:\nimport { UnstructuredDirectoryLoader } from\n\"langchain/document_loaders/fs/unstructured\"; const options = { apiKey:","metadata":{"source":"https://js.langchain.com/docs/ecosystem/integrations/unstructured","title":"Unstructured | 🦜️🔗 Langchain","description":"This page covers how to use Unstructured within LangChain.","language":"en","loc":{"lines":{"from":90,"to":123}}}}],["646",{"pageContent":"{ UnstructuredDirectoryLoader } from\n\"langchain/document_loaders/fs/unstructured\"; const options = { apiKey:\n\"MY_API_KEY\", }; const loader = new UnstructuredDirectoryLoader(\n\"langchain/src/document_loaders/tests/example_data\", options ); const docs =\nawait loader.load(); API REFERENCE: * UnstructuredDirectoryLoader\n[/docs/api/document_loaders_fs_unstructured/classes/UnstructuredDirectoryLoader]\nfrom langchain/document_loaders/fs/unstructured Currently, the\nUnstructuredLoader supports the following document types: * Plain text files\n(.txt/.text) * PDFs (.pdf) * Word Documents (.doc/.docx) * PowerPoints\n(.ppt/.pptx) * Images (.jpg/.jpeg) * Emails (.eml/.msg) * HTML (.html) *\nMarkdown Files (.md) The output from the UnstructuredLoader will be an array of\nDocument objects that looks like the following: [ Document { pageContent:\n`Decoder: The decoder is also composed of a stack of N = 6 identical layers. In\naddition to the two sub-layers in each encoder","metadata":{"source":"https://js.langchain.com/docs/ecosystem/integrations/unstructured","title":"Unstructured | 🦜️🔗 Langchain","description":"This page covers how to use Unstructured within LangChain.","language":"en","loc":{"lines":{"from":123,"to":159}}}}],["647",{"pageContent":"that looks like the following: [ Document { pageContent: `Decoder: The decoder\nis also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, wh ich\nperforms multi-head attention over the output of the encoder stack. Similar to\nthe encoder, we employ residual connections around each of the sub-layers,\nfollowed by layer normalization. We also modify the self -attention sub-layer in\nthe decoder stack to prevent positions from attending to subsequent positions.\nThis masking, combined with fact that the output embeddings are offset by one\nposition, ensures that the predic tions for position i can depend only on the\nknown outputs at positions less than i.`, metadata: { page_number: 3, filename:\n'1706.03762.pdf', category: 'NarrativeText' } }, Document { pageContent: '3.2\nAttention', metadata: { page_number: 3, filename: '1706.03762.pdf',","metadata":{"source":"https://js.langchain.com/docs/ecosystem/integrations/unstructured","title":"Unstructured | 🦜️🔗 Langchain","description":"This page covers how to use Unstructured within LangChain.","language":"en","loc":{"lines":{"from":159,"to":181}}}}],["648",{"pageContent":"page_number: 3, filename: '1706.03762.pdf', category: 'NarrativeText' } },\nDocument { pageContent: '3.2 Attention', metadata: { page_number: 3, filename:\n'1706.03762.pdf', category: 'Title' } ] Previous Google MakerSuite\n[/docs/ecosystem/integrations/makersuite] Next Additional resources\n[/docs/additional_resources] * What is Unstructured? * Quick start * Directories\nCommunity * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/ecosystem/integrations/unstructured","title":"Unstructured | 🦜️🔗 Langchain","description":"This page covers how to use Unstructured within LangChain.","language":"en","loc":{"lines":{"from":181,"to":216}}}}],["649",{"pageContent":"Skip to main content 🦜️🔗 LangChain [/]Docs [/docs/get_started/introduction]Use\ncases [/docs/use_cases]API [/docs/api/] LangSmith\n[https://smith.langchain.com/]Python Docs\n[https://python.langchain.com/en/latest/]https://github.com/hwchase17/langchainjs\nSearch CTRLK * Get started [/docs/get_started] * Introduction\n[/docs/get_started/introduction] * Installation [/docs/get_started/installation]\n* Quickstart [/docs/get_started/quickstart] * LangChain Expression Language\n[/docs/expression_language/] * How to [/docs/expression_language/how_to/routing]\n* Cookbook [/docs/expression_language/cookbook/] * LangChain Expression Language\n(LCEL) [/docs/expression_language/] * Interface\n[/docs/expression_language/interface] * Modules [/docs/modules/] * Model I/ O\n[/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] * Chains\n[/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks","metadata":{"source":"https://js.langchain.com/docs/additional_resources/scrimba","title":"Scrimba interactive guides | 🦜️🔗 Langchain","description":"Scrimba is a code-learning platform that allows you to interactively edit and run","language":"en","loc":{"lines":{"from":1,"to":25}}}}],["650",{"pageContent":"O [/docs/modules/model_io/] * Retrieval [/docs/modules/data_connection/] *\nChains [/docs/modules/chains/] * Memory [/docs/modules/memory/] * Agents\n[/docs/modules/agents/] * Callbacks [/docs/modules/callbacks/] * Modules\n[/docs/modules/] * Guides [/docs/guides] * Ecosystem [/docs/ecosystem] *\nAdditional resources [/docs/additional_resources] * LangChain Expression\nLanguage Cheatsheet [/docs/additional_resources/expression_language_cheatsheet]\n* Scrimba interactive guides [/docs/additional_resources/scrimba] * Gallery\n[https://github.com/kyrolabs/awesome-langchain] * Community navigator\n[/docs/community] *\n----------------------------------------------------------------------------------------------------------------------------------\n* API reference [/docs/api/] * / * Additional resources\n[/docs/additional_resources] * Scrimba interactive guides On this page SCRIMBA\nINTERACTIVE GUIDES Scrimba [https://scrimba.com] is a code-learning platform","metadata":{"source":"https://js.langchain.com/docs/additional_resources/scrimba","title":"Scrimba interactive guides | 🦜️🔗 Langchain","description":"Scrimba is a code-learning platform that allows you to interactively edit and run","language":"en","loc":{"lines":{"from":25,"to":53}}}}],["651",{"pageContent":"* / * Additional resources [/docs/additional_resources] * Scrimba interactive\nguides On this page SCRIMBA INTERACTIVE GUIDES Scrimba [https://scrimba.com] is\na code-learning platform that allows you to interactively edit and run code\nwhile watching a video walkthrough. We've partnered with Scrimba on course\nmaterials (called \"scrims\") that teach the fundamentals of building with\nLangChain.js - check them out below, and check back for more as they become\navailable! LANGCHAIN EXPRESSION LANGUAGE (LCEL) * The basics (PromptTemplate +\nLLM) [https://scrimba.com/scrim/c6rD6Nt9] * Adding an output parser\n[https://scrimba.com/scrim/co6ae44248eacc1abd87ae3dc] * Attaching function calls\nto a model [https://scrimba.com/scrim/cof5449f5bc972f8c90be6a82] DEEPER DIVES *\nSetting up a new PromptTemplate [https://scrimba.com/scrim/cbGwRwuV] * Setting\nup ChatOpenAI parameters [https://scrimba.com/scrim/cEgbBBUw] * Attaching stop\nsequences","metadata":{"source":"https://js.langchain.com/docs/additional_resources/scrimba","title":"Scrimba interactive guides | 🦜️🔗 Langchain","description":"Scrimba is a code-learning platform that allows you to interactively edit and run","language":"en","loc":{"lines":{"from":53,"to":80}}}}],["652",{"pageContent":"DIVES * Setting up a new PromptTemplate [https://scrimba.com/scrim/cbGwRwuV] *\nSetting up ChatOpenAI parameters [https://scrimba.com/scrim/cEgbBBUw] *\nAttaching stop sequences [https://scrimba.com/scrim/co9704e389428fe2193eb955c]\nPrevious LangChain Expression Language Cheatsheet\n[/docs/additional_resources/expression_language_cheatsheet] Next Community\nnavigator [/docs/community] * LangChain Expression Language (LCEL) * Deeper\ndives Community * Discord [https://discord.gg/cU2adEyC7w] * Twitter\n[https://twitter.com/LangChainAI] GitHub * Python\n[https://github.com/hwchase17/langchain] * JS/TS\n[https://github.com/hwchase17/langchainjs] More * Homepage\n[https://langchain.com] * Blog [https://blog.langchain.dev] Copyright © 2023\nLangChain, Inc.","metadata":{"source":"https://js.langchain.com/docs/additional_resources/scrimba","title":"Scrimba interactive guides | 🦜️🔗 Langchain","description":"Scrimba is a code-learning platform that allows you to interactively edit and run","language":"en","loc":{"lines":{"from":80,"to":107}}}}]]